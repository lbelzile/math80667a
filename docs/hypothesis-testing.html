<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 Hypothesis testing | Experimental Design and Statistical Methods</title>
<meta name="author" content="Léo Belzile">
<meta name="description" content="In most applied domains, empirical evidences drive the advancement of the field and data from well designed experiments contribute to the built up of science. In order to draw conclusions in...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="2 Hypothesis testing | Experimental Design and Statistical Methods">
<meta property="og:type" content="book">
<meta property="og:description" content="In most applied domains, empirical evidences drive the advancement of the field and data from well designed experiments contribute to the built up of science. In order to draw conclusions in...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2 Hypothesis testing | Experimental Design and Statistical Methods">
<meta name="twitter:description" content="In most applied domains, empirical evidences drive the advancement of the field and data from well designed experiments contribute to the built up of science. In order to draw conclusions in...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Experimental Design and Statistical Methods</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Experimental Design and Statistical Methods</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="active" href="hypothesis-testing.html"><span class="header-section-number">2</span> Hypothesis testing</a></li>
<li><a class="" href="CRT.html"><span class="header-section-number">3</span> Completely randomized designs</a></li>
<li><a class="" href="replication-crisis.html"><span class="header-section-number">4</span> Replication crisis</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/lbelzile/math80667a">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="hypothesis-testing" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Hypothesis testing<a class="anchor" aria-label="anchor" href="#hypothesis-testing"><i class="fas fa-link"></i></a>
</h1>
<p>In most applied domains, empirical evidences drive the advancement of the field and data from well designed experiments contribute to the built up of science. In order to draw conclusions in favour or against a theory, researchers turn (often unwillingly) to statistics to back up their claims. This has led to the prevalence of the use of the null hypothesis statistical testing (NHST) framework. One important aspect of the reproducibility crisis is the misuse of <span class="math inline">\(p\)</span>-values in journal articles: falsification of a null hypothesis is not enough to provide substantive findings for a theory.</p>
<p>Because introductory statistics course typically present hypothesis tests without giving much thoughts to the underlying construction principles of such procedures, users often have a reductive view of statistics as a catalogue of pre-determined procedures. To make a culinary analogy, users focus on learning recipes rather than trying to understand the basics of cookery. This chapter focuses on understanding of key ideas related to testing.</p>
<div class="keyidea">
<p><strong>Learning objectives</strong>:</p>
<ul>
<li>Understanding the role of uncertainty in decision making and</li>
<li>Understanding the importance of signal-to-noise ratio as a measure of evidence.</li>
<li>Knowing the basic ingredients of hypothesis testing and being capable of correctly formulating and identifying these components in a paper.</li>
<li>Correctly interpreting <span class="math inline">\(p\)</span>-values and confidence intervals for a parameter.</li>
</ul>
</div>
<div id="hypothesis" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Hypothesis<a class="anchor" aria-label="anchor" href="#hypothesis"><i class="fas fa-link"></i></a>
</h2>
<p>The first step of a design is formulating a research question. Generally, this hypothesis will specify potential differences between population characteristics due to some intervention (a treatment) that the researcher wants to quantify. This is the step during which researchers decide on sample size, choice of response variable and metric for the measurement, write down the study plan, etc.</p>
<p>It is important to note that most research questions cannot be answered by simple tools. Researchers wishing to perform innovative methodological research should contact experts and consult with statisticians <strong>before</strong> they collect their data to get information on how best to proceed for what they have in mind so as to avoid the risk of making misleading and false claims based on incorrect analysis or data collection.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:xkcd2569"></span>
<img src="figures/xkcd2569_hypothesis_generation.png" alt="xkcd comic [2569 (Hypothesis generation) by Randall Munroe](https://xkcd.com/605/). Alt text: Frazzled scientists are requesting that everyone please stop generating hypotheses for a little bit while they work through the backlog. Cartoon reprinted under the [CC BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/)." width="60%"><p class="caption">
Figure 2.1: xkcd comic <a href="https://xkcd.com/605/">2569 (Hypothesis generation) by Randall Munroe</a>. Alt text: Frazzled scientists are requesting that everyone please stop generating hypotheses for a little bit while they work through the backlog. Cartoon reprinted under the <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC BY-NC 2.5 license</a>.
</p>
</div>
</div>
<div id="sampling-variability" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Sampling variability<a class="anchor" aria-label="anchor" href="#sampling-variability"><i class="fas fa-link"></i></a>
</h2>
<p>Given data, a researcher will be interested in estimating particular characteristics of the population. We can characterize the set of all potential values their measurements can take, together with their frequency, via a distribution.</p>
<p>An hypothesis test will focus on one or multiple of these characteristics. Suppose we have two groups, control and treatment, whose population averages are <span class="math inline">\(\mu_C\)</span> and <span class="math inline">\(\mu_T\)</span> which we wish to compare.</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 2.1  (A/B testing) </strong></span>Consider two webpage design: one is the current version (<em>status quo</em>) and the other implementation contains a clickable banner in a location where eyetracker suggest that viewers eyes spend more time or attention. The number of clicks on those headlines are what generate longer viewing, and thus higher revenues from advertisement. The characteristic of interest here would be the average click conversation rate for each of the webpage design.</p>
<p>It is fairly simple to redirect traffic so that a random fraction gets assigned to the new design for study. After a suitable period of time, the data can be analyzed to see if the new webpage generates more clicks.</p>
</div>
<p>People commonly look at the difference in average, say <span class="math inline">\(\delta=\mu_T - \mu_C\)</span> as a measure of the effectiveness of the treatment.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We could look at the ratio &lt;span class="math inline"&gt;\(\mu_T/\mu_C\)&lt;/span&gt; instead.&lt;/p&gt;'><sup>5</sup></a> If we properly randomized observations in each subgroup and nothing else changes, then this measures the impact of the treatment. Because we only have a sample at hand and not the whole population, we don’t know for sure the values of <span class="math inline">\(\mu_C\)</span> and <span class="math inline">\(\mu_T\)</span>. These quantities exist, but are unknown to us so the best we can do is estimate them using random samples that are drawn from the population.</p>
<p>We call numerical summaries of the data <strong>statistics</strong>. Its important to distinguish between procedures/formulas and their numerical values. An <strong>estimator</strong> is a rule or formula used to calculate an estimate of some parameter or quantity of interest based on observed data (like a recipe for cake). Once we have observed data we can actually compute the sample mean, that is, we have an estimate — an actual value (the cake), which is a single realization and not random. In other words,</p>
<ul>
<li>an estimand is our conceptual target, like the population characteristic of interest.</li>
<li>an estimator is the procedure or formula telling us how to transform the sample data into a numerical summary that is a proxy of our target.</li>
<li>an estimate is a number, the numerical value obtained once we apply the formula to observed data.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:cake"></span>
<img src="figures/estimand.jpg" alt="[Estimand](https://www.flickr.com/photos/darkdwarf/16563489881) (left), estimator (middle) and [estimate](https://www.flickr.com/photos/bensutherland/14685548773) (right) illustrated with cakes and based on an original idea of Simon Grund. Cake photos shared under [CC BY-NC 2.0 license](https://creativecommons.org/licenses/by-nc/2.0/)." width="33%"><img src="figures/estimator.jpg" alt="[Estimand](https://www.flickr.com/photos/darkdwarf/16563489881) (left), estimator (middle) and [estimate](https://www.flickr.com/photos/bensutherland/14685548773) (right) illustrated with cakes and based on an original idea of Simon Grund. Cake photos shared under [CC BY-NC 2.0 license](https://creativecommons.org/licenses/by-nc/2.0/)." width="33%"><img src="figures/estimate.jpg" alt="[Estimand](https://www.flickr.com/photos/darkdwarf/16563489881) (left), estimator (middle) and [estimate](https://www.flickr.com/photos/bensutherland/14685548773) (right) illustrated with cakes and based on an original idea of Simon Grund. Cake photos shared under [CC BY-NC 2.0 license](https://creativecommons.org/licenses/by-nc/2.0/)." width="33%"><p class="caption">
Figure 2.2: <a href="https://www.flickr.com/photos/darkdwarf/16563489881">Estimand</a> (left), estimator (middle) and <a href="https://www.flickr.com/photos/bensutherland/14685548773">estimate</a> (right) illustrated with cakes and based on an original idea of Simon Grund. Cake photos shared under <a href="https://creativecommons.org/licenses/by-nc/2.0/">CC BY-NC 2.0 license</a>.
</p>
</div>
<p>For example, we may use as estimand <span class="math inline">\(\mu\)</span>, the population average of <span class="math inline">\(Y_1, \ldots\)</span>. The estimator will be sample mean of a sample of size <span class="math inline">\(n\)</span> is the sum of its elements divided by the sample size, <span class="math inline">\(\overline{Y}=(Y_1 + \cdots + Y_n)/n\)</span>. The estimate for a given sample will be a numerical value, say 4.3.</p>
<p>Because the inputs of the estimator are random, the output is also random and change from one sample to the next: even if you repeat a recipe, you won’t get the exact same result every time.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:xkcd605"></span>
<img src="figures/xkcd2581_health_stats.png" alt="xkcd comic [2581 (Health Stats) by Randall Munroe](https://xkcd.com/2581/). Alt text: You will live on forever in our hearts, pushing a little extra blood toward our left hands now and then to give them a squeeze. Cartoon reprinted under the [CC BY-NC 2.5 license](https://creativecommons.org/licenses/by-nc/2.5/)." width="70%"><p class="caption">
Figure 1.8: xkcd comic <a href="https://xkcd.com/2581/">2581 (Health Stats) by Randall Munroe</a>. Alt text: You will live on forever in our hearts, pushing a little extra blood toward our left hands now and then to give them a squeeze. Cartoon reprinted under the <a href="https://creativecommons.org/licenses/by-nc/2.5/">CC BY-NC 2.5 license</a>.
</p>
</div>
<p>To illustrate this point, Figure <a href="hypothesis-testing.html#fig:samplevar">2.3</a> shows five simple random samples of size <span class="math inline">\(n=10\)</span> drawn from an hypothetical population with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, along with their sample mean <span class="math inline">\(\overline{y}\)</span>. Thus, sampling variability implies that the sample means of the subgroups will always differ even if the share the same characteristics. You can view sampling variability as noise: our goal is to extract the signal (typically differences in means) but accounting for spurious results due to the background noise.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:samplevar"></span>
<img src="02-hypothesis_testing_files/figure-html/samplevar-1.png" alt="Five samples of size $n=10$ drawn from a common population with mean $\mu$ (horizontal line). The colored segments show the sample means of each sample." width="85%"><p class="caption">
Figure 2.3: Five samples of size <span class="math inline">\(n=10\)</span> drawn from a common population with mean <span class="math inline">\(\mu\)</span> (horizontal line). The colored segments show the sample means of each sample.
</p>
</div>
<p>We can clearly see from Figure <a href="hypothesis-testing.html#fig:samplevar">2.3</a> that, even if each sample is drawn from the same population, the sample mean varies from one sample to the next as a result of the sampling variability. The astute eye might even notice that the sample means are less dispersed around the full black horizontal line representing the overall average <span class="math inline">\(\mu\)</span> than the individual measurements. This is a fundamental principle of statistics: information accumulates as you get more data and the sample mean <span class="math inline">\(\overline{Y}\)</span> is based on multiple observations.</p>
<p>Values of the sample mean don’t tell the whole picture and studying differences in mean (between groups, or relative to a postulated reference value) is not enough to draw conclusions. In most settings, there is no guarantee that the sample mean will be equal to it’s true value because it changes from one sample to the next: the only guarantee we have is that it will be on average equal to the population average. Depending on the choice of measurement and variability in the population, there may be considerable differences from one observation to the next and this means the observed difference could be a fluke.</p>
<p>To get an idea of how certain something is, we have to consider the variability of an observation <span class="math inline">\(Y_i\)</span>. This variance is typically denoted <span class="math inline">\(\sigma^2\)</span> and the standard deviation, <span class="math inline">\(\sigma\)</span>, is expressed in the same unit as the measurements.</p>
<p>The sample variance <span class="math inline">\(S_n\)</span> is an estimator of the standard deviation <span class="math inline">\(\sigma\)</span>, where
<span class="math display">\[\begin{align*}
S^2_n &amp;= \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})^2
\end{align*}\]</span>
is the sum of squared difference between observations and the sample average, scaled by a factor proportional to the sample size.</p>
<p>The standard deviation <em>of a statistic</em> is termed <strong>standard error</strong>; it should not be confused with the standard deviation <span class="math inline">\(\sigma\)</span> of the population from which the sample observations <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are drawn. Both standard deviation and standard error are expressed in the same units as the measurements, so are easier to interpret than variance. Since the standard error is a function of the sample size, it is however good practice to report the estimated standard deviation in reports.</p>
<div class="example">
<span id="exm:unlabeled-div-8" class="example"><strong>Example 2.2  (Sample proportion and uniform draws) </strong></span>To illustrate the concept of sampling variability, we follow the lead of <a href="https://www.crumplab.com/statistics/foundations-for-inference.html">Matthew Crump</a> and consider samples from a uniform distribution on <span class="math inline">\(\{1, 2, \ldots, 10\}\)</span> each number in this interval is equally likely to be sampled.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unifsamp1"></span>
<img src="02-hypothesis_testing_files/figure-html/unifsamp1-1.png" alt="Histograms for 10 random samples of size $n=20$ from a discrete uniform distribution." width="85%"><p class="caption">
Figure 2.4: Histograms for 10 random samples of size <span class="math inline">\(n=20\)</span> from a discrete uniform distribution.
</p>
</div>
<p>Even if they are drawn from the same population, the 10 samples in Figure <a href="hypothesis-testing.html#fig:unifsamp1">2.4</a> look quite different. The only thing at play here is the sample variability: since there are <span class="math inline">\(n=20\)</span> observations in total, there should be on average 10% of the observations in each of the 10 bins, but some bins are empty and others have more counts than expected. This fluctuation is due to randomness, or chance.</p>
<p>How can we thus detect whether what we see is compatible with the model we think generated the data? The key is to collect more observations: the bar height is the sample proportion, an average of 0/1 values with ones indicating that the observation is in the bin and zero otherwise.</p>
<p>Consider now what happens as we increase the sample size: the top panel of Figure <a href="hypothesis-testing.html#fig:uniformsamp2">2.5</a> shows uniform samples for increasing samples size. The histogram looks more and more like the true underlying distribution (flat, each bin with equal frequency) as the sample size increases. The sample distribution of points is nearly indistinguishable from the theoretical one (straight line) when <span class="math inline">\(n=10 000\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The formula shows that the standard error decreases by a tenfold every time the sample size increases by a factor 100.&lt;/p&gt;"><sup>6</sup></a> The bottom panel, on the other hand, isn’t from a uniform distribution and larger samples come closer to the population distribution. We couldn’t have spotted this difference in the first two plots, since the sampling variability is too important; there, the lack of data in some bins could have been attributed to chance, as they are comparable with the graph for data that are truly uniform. This is in line with most practical applications, in which the limited sample size restricts our capacity to disentangle real differences from sampling variability. We must embrace this uncertainty: in the next section, we outline how hypothesis testing helps us disentangle the signal from the noise.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:uniformsamp2"></span>
<img src="02-hypothesis_testing_files/figure-html/uniformsamp2-1.png" alt="Histograms of data from a uniform distribution (top) and non-uniform (bottom) with increasing sample sizes of 10, 100, 1000 and 10 000 (from left to right)." width="85%"><p class="caption">
Figure 2.5: Histograms of data from a uniform distribution (top) and non-uniform (bottom) with increasing sample sizes of 10, 100, 1000 and 10 000 (from left to right).
</p>
</div>
</div>
</div>
<div id="tests" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Hypothesis testing<a class="anchor" aria-label="anchor" href="#tests"><i class="fas fa-link"></i></a>
</h2>
<p>An hypothesis test is a binary decision rule (yes/no) used to evaluate the statistical evidence provided by a sample to make a decision regarding the underlying population. The main steps involved are:</p>
<ul>
<li>define the model parameters</li>
<li>formulate the alternative and null hypothesis</li>
<li>choose and calculate the test statistic</li>
<li>obtain the null distribution describing the behaviour of the test statistic under <span class="math inline">\(\mathscr{H}_0\)</span>
</li>
<li>calculate the <em>p</em>-value</li>
<li>conclude (reject or fail to reject <span class="math inline">\(\mathscr{H}_0\)</span>) in the context of the problem.</li>
</ul>
<p>A good analogy for hypothesis tests is a trial for murder on which you are appointed juror.</p>
<ul>
<li>The judge lets you choose between two mutually exclusive outcome, guilty or not guilty, based on the evidence presented in court.</li>
<li>The presumption of innocence applies and evidences are judged under this optic: are evidence remotely plausible if the person was innocent? The burden of the proof lies with the prosecution to avoid as much as possible judicial errors. The null hypothesis <span class="math inline">\(\mathscr{H}_0\)</span> is <em>not guilty</em>, whereas the alternative <span class="math inline">\(\mathscr{H}_a\)</span> is <em>guilty</em>. If there is a reasonable doubt, the verdict of the trial will be not guilty.</li>
<li>The test statistic (and the choice of test) represents the summary of the proof. The more overwhelming the evidence, the higher the chance the accused will be declared guilty. The prosecutor chooses the proof so as to best outline this: the choice of evidence (statistic) ultimately will maximize the evidence, which parallels the power of the test.</li>
<li>The null distribution is the benchmark against which to judge the evidence (jurisprudence). Given the proof, what are the odds assuming the person is innocent? Since this is possibly different for every test, it is common to report instead a <em>p</em>-value, which gives the level of evidence on a uniform scale which is most easily interpreted.</li>
<li>The final step is the verdict, a binary decision with outcomes: guilty or not guilty. For an hypothesis test performed at level <span class="math inline">\(\alpha\)</span>, one would reject (guilty) if the <em>p</em>-value is less than <span class="math inline">\(\alpha\)</span>. Even if we declare the person not guilty, this doesn’t mean the defendant is innocent and vice-versa.</li>
</ul>
<div id="hypothesis-1" class="section level3" number="2.3.1">
<h3>
<span class="header-section-number">2.3.1</span> Hypothesis<a class="anchor" aria-label="anchor" href="#hypothesis-1"><i class="fas fa-link"></i></a>
</h3>
<p>In statistical tests we have two hypotheses: the null hypothesis (<span class="math inline">\(\mathscr{H}_0\)</span>) and the alternative hypothesis (<span class="math inline">\(\mathscr{H}_a\)</span>). Usually, the null hypothesis (the ‘status quo’) is a single numerical value. The alternative is what we’re really interested in testing. In <a href="hypothesis-testing.html#fig:samplevar">2.3</a>, we could consider whether all five groups have the same mean <span class="math inline">\(\mathscr{H}_0: \mu_1 = \mu_2 = \cdots = \mu_5\)</span> against the alternative that at least two of them are different. These two outcomes are mutually exclusive and cover all possible scenarios. A statistical hypothesis test allows us to decide whether or not our data provides enough evidence to reject <span class="math inline">\(\mathscr{H}_0\)</span> in favor of <span class="math inline">\(\mathscr{H}_a\)</span>, subject to some pre-specified risk of error: while we know that the differences are just due to sampling variability in Figure <a href="hypothesis-testing.html#fig:samplevar">2.3</a> because the data is simulated, in practice we need to assess the evidence using a numerical summary.</p>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 2.3  (A/B testing (continued)) </strong></span>We follow-up with our A/B test experiment. Given <span class="math inline">\(\mu_1\)</span> the population average click conversation rate for the current webpage and <span class="math inline">\(\mu_2\)</span> that of the redesign, we are interested in the <em>one-sided hypothesis</em> that <span class="math inline">\(\mathscr{H}_0: \mu_2 \geq \mu_1\)</span>. This is logical: given the costs associated to changes to the interface and the resulting disruption, we only want to implement changes that improve the webpage design and allow us to generate more revenues.</p>
<p>One-sided hypothesis are directional: we care only about a specific direction, and so here <span class="math inline">\(\mathscr{H}_a: \mu_2 \leq \mu_1\)</span>. Indeed, if the experiment suggests that the conversion rate is worst with the new webpage design, we won’t go forward.</p>
<p>Since neither of these population values are known to us, we can work instead with <span class="math inline">\(\mathscr{H}_0: \mu_2-\mu_1 \geq 0\)</span>. We can use as estimator for the difference <span class="math inline">\(\mu_2-\mu_1\)</span> the difference in sample average in each subgroup.</p>
<p>The null hypothesis here is an interval, but it suffices the consider the most beneficial scenario, which is <span class="math inline">\(\mu_2-\mu_1=0\)</span>. Indeed, if we can disprove that there is no difference and see an increase of the click rate with the updated version, all more extreme cases are automatically discarded in favour of the alternative that the new design is better.</p>
</div>
</div>
<div id="test-statistic" class="section level3" number="2.3.2">
<h3>
<span class="header-section-number">2.3.2</span> Test statistic<a class="anchor" aria-label="anchor" href="#test-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>A test statistic <span class="math inline">\(T\)</span> is a function of the data which takes the data as input and outputs a summary of the information contained in the sample for a characteristic of interest, say the population mean. In order to assess whether the numerical value for <span class="math inline">\(T\)</span> is unusual, we need to know what are the potential values taken by <span class="math inline">\(T\)</span> and their relative probability if <span class="math inline">\(\mathscr{H}_0\)</span> is true. We need to know what values we should expect if, e.g., there was no difference in the averages of the different groups: this requires a benchmark.</p>
<p>Many statistics we will consider are of the form<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;This class of statistic, which includes &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;-tests, are called Wald statistics.&lt;/p&gt;'><sup>7</sup></a>
<span class="math display">\[\begin{align*}
T = \frac{\text{estimated effect}- \text{postulated effect}}{\text{estimated effect variability}} = \frac{\widehat{\theta} - \theta_0}{\mathrm{se}(\widehat{\theta})}
\end{align*}\]</span>
where <span class="math inline">\(\widehat{\theta}\)</span> is an estimator of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta_0\)</span> is the postulated value of the parameter and <span class="math inline">\(\mathrm{se}(\widehat{\theta})\)</span> is the standard error of the test statistic <span class="math inline">\(\widehat{\theta}\)</span>. This quantity is designed so that, if there is no difference, <span class="math inline">\(T\)</span> has approximately mean zero and variance one. This standardization makes comparison easier; in fact, the form of the test statistic is chosen so that it doesn’t depend on the units used.</p>
<p>For example, if we are interested in mean differences between treatment group and control group, denoted <span class="math inline">\(\mu_T\)</span> and <span class="math inline">\(\mu_C\)</span>, then <span class="math inline">\(\theta = \mu_T-\mu_C\)</span> and <span class="math inline">\(\mathscr{H}_0: \mu_T = \mu_C\)</span> corresponds to <span class="math inline">\(\mathscr{H}_0: \theta = 0\)</span> for no difference. The two-sample <span class="math inline">\(t\)</span>-test would have numerator <span class="math inline">\(\widehat{\theta} = \overline{Y}_T - \overline{Y}_C\)</span>, where <span class="math inline">\(\overline{Y}_T\)</span> is the sample average in treatment group and <span class="math inline">\(\overline{Y}_C\)</span> that of the control group.</p>
<p>The numerator would thus consist of the difference in sample means and the denominator the standard error of that quantity, calculated using a software.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Assuming equal variance, the denominator is estimated using the pooled variance estimator.&lt;/p&gt;"><sup>8</sup></a></p>
</div>
<div id="null-distribution-and-p-value" class="section level3" number="2.3.3">
<h3>
<span class="header-section-number">2.3.3</span> Null distribution and <em>p</em>-value<a class="anchor" aria-label="anchor" href="#null-distribution-and-p-value"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>p</em>-value allows us to decide whether the observed value of the test statistic <span class="math inline">\(T\)</span> is plausible under <span class="math inline">\(\mathscr{H}_0\)</span>. Specifically, the <em>p</em>-value is the probability that the test statistic is equal or more extreme to the estimate computed from the data, assuming <span class="math inline">\(\mathscr{H}_0\)</span> is true. Suppose that based on a random sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span> we obtain a statistic whose value <span class="math inline">\(T=t\)</span>. For a two-sided test <span class="math inline">\(\mathscr{H}_0:\theta=\theta_0\)</span> vs. <span class="math inline">\(\mathscr{H}_a:\theta \neq \theta_0\)</span>, the <em>p</em>-value is <span class="math inline">\(\mathsf{Pr}_0(|T| \geq |t|)\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;If the distribution of &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; is symmetric around zero, the &lt;em&gt;p&lt;/em&gt;-value reduces to &lt;span class="math inline"&gt;\(p = 2 \times \mathsf{Pr}_0(T \geq |t|).\)&lt;/span&gt;&lt;/p&gt;'><sup>9</sup></a></p>
<p>How do we determine the null distribution given that the true data generating mechanism is unknown to us? We ask a statistician! In simple cases, it might be possible to enumerate all possible outcomes and thus quantity the degree of outlyingness of our observed statistic. In more general settings, we can resort to simulations or to probability theory: the central limit theorem says that the sample mean behaves like a normal random variable with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma/\sqrt{n}\)</span> for <span class="math inline">\(n\)</span> large enough. The central limit theorem has broader applications since most statistics can be viewed as some form of average or transformation thereof, a fact used to derive benchmarks for most commonly used tests. Most software use these approximations as proxy by default: the normal, Student’s <span class="math inline">\(t\)</span>, <span class="math inline">\(\chi^2\)</span> and <span class="math inline">\(F\)</span> distributions are the reference distributions that arise the most often.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:power-plots"></span>
<img src="02-hypothesis_testing_files/figure-html/power-plots-1.png" alt="Density of _p_-values under the null hypothesis (left) and under an alternative with a signal-to-noise ratio of 0.5 (right). The probability of rejection is obtained by calculating the area under the density curve between zero and $\alpha=0.1$, here 0.1. Under the null, the model is calibrated and the distribution of _p_-values is uniform (i.e., a flat rectangle of height 1), meaning all values in the unit interval are equally likely. Under the alternative (right), small _p_-values are more likely to be observed." width="85%"><p class="caption">
Figure 2.6: Density of <em>p</em>-values under the null hypothesis (left) and under an alternative with a signal-to-noise ratio of 0.5 (right). The probability of rejection is obtained by calculating the area under the density curve between zero and <span class="math inline">\(\alpha=0.1\)</span>, here 0.1. Under the null, the model is calibrated and the distribution of <em>p</em>-values is uniform (i.e., a flat rectangle of height 1), meaning all values in the unit interval are equally likely. Under the alternative (right), small <em>p</em>-values are more likely to be observed.
</p>
</div>
<p>There are generally three ways of obtaining null distributions for assessing the degree of evidence against the null hypothesis</p>
<ul>
<li>exact calculations</li>
<li>large sample theory (aka ‘asymptotics’ in statistical lingo)</li>
<li>simulation</li>
</ul>
<p>While desirable, the first method is only applicable in simple cases (such as counting the probability of getting two six if you throw two fair die). The second method is most commonly used due to its generality and ease of use (particularly in older times where computing power was scarce), but fares poorly with small sample sizes (where ‘too small’ is context and test-dependent). The last approach can be used to approximate the null distribution in many scenarios, but adds a layer of randomness and the extra computations costs sometimes are not worth it.</p>
</div>
<div id="conclusion" class="section level3" number="2.3.4">
<h3>
<span class="header-section-number">2.3.4</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>p</em>-value allows us to make a decision about the null hypothesis. If <span class="math inline">\(\mathscr{H}_0\)</span> is true, the <em>p</em>-value follows a uniform distribution, as shown in Figure <a href="hypothesis-testing.html#fig:power-plots">2.6</a>. <a href="https://xkcd.com/1478/">Thus, if the <em>p</em>-value is small</a>, this means observing an outcome more extreme than <span class="math inline">\(T=t\)</span> is unlikely, and so we’re inclined to think that <span class="math inline">\(\mathscr{H}_0\)</span> is not true. There’s always some underlying risk that we’re making a mistake when we make a decision. In statistic, there are <a href="https://xkcd.com/2303/">two type of errors</a>:</p>
<ul>
<li>type I error: we reject <span class="math inline">\(\mathscr{H}_0\)</span> when <span class="math inline">\(\mathscr{H}_0\)</span> is true,</li>
<li>type II error: we fail to reject <span class="math inline">\(\mathscr{H}_0\)</span> when <span class="math inline">\(\mathscr{H}_0\)</span>.</li>
</ul>
<p>The two hypothesis are not judged equally: we seek to avoid error of type I (judicial errors, corresponding to condamning an innocent). To prevent this, we fix a the level of the test, <span class="math inline">\(\alpha\)</span>, which captures our tolerance to the risk of commiting a type I error: the higher the level of the test <span class="math inline">\(\alpha\)</span>, the more often we will reject the null hypothesis when the latter is true. The value of <span class="math inline">\(\alpha \in (0, 1)\)</span> is the probability of rejecting <span class="math inline">\(\mathscr{H}_0\)</span> when <span class="math inline">\(\mathscr{H}_0\)</span> is in fact true,
<span class="math display">\[\begin{align*}
\alpha = \mathsf{Pr}_0\left(\text{ reject } \mathscr{H}_0\right).
\end{align*}\]</span>
The level <span class="math inline">\(\alpha\)</span> is fixed beforehand, typically <span class="math inline">\(1\)</span>%, <span class="math inline">\(5\)</span>% or <span class="math inline">\(10\)</span>%. Keep in mind that the probability of type I error is <span class="math inline">\(\alpha\)</span> only if the null model for <span class="math inline">\(\mathscr{H}_0\)</span> is correct (sic) and correspond to the data generating mechanism.</p>
<p>The focus on type I error is best understood by thinking about costs of moving away from the status quo: a new website design or branding will be costly to implement, so you want to make sure there are enough evidence that the proposal is the better alternative and will lead to increased traffic or revenues.</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">
<strong>Decision</strong> \ <strong>true model</strong>
</th>
<th align="center"><span class="math inline">\(\mathscr{H}_0\)</span></th>
<th align="center"><span class="math inline">\(\mathscr{H}_a\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fail to reject <span class="math inline">\(\mathscr{H}_0\)</span>
</td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
<td align="center">type II error</td>
</tr>
<tr class="even">
<td align="left">reject <span class="math inline">\(\mathscr{H}_0\)</span>
</td>
<td align="center">type I error</td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
</tr>
</tbody>
</table></div>
<p>To make a decision, we compare our <em>p</em>-value <span class="math inline">\(P\)</span> with the level of the test <span class="math inline">\(\alpha\)</span>:</p>
<ul>
<li>if <span class="math inline">\(P &lt; \alpha\)</span>, we reject <span class="math inline">\(\mathscr{H}_0\)</span>;</li>
<li>if <span class="math inline">\(P \geq \alpha\)</span>, we fail to reject <span class="math inline">\(\mathscr{H}_0\)</span>.</li>
</ul>
<p>Do not mix up level of the test (a probability fixed beforehand by the researcher) and the <em>p</em>-value. If you do a test at level 5%, the probability of type I error (condemning an innocent by mistake) is by definition <span class="math inline">\(\alpha\)</span> and does not depend on the <em>p</em>-value. The latter is a conditional probability of observing a more extreme statistic given the null distribution <span class="math inline">\(\mathscr{H}_0\)</span> is true.</p>
<div class="pitfall">
<p>The <a href="https://doi.org/10.1080/00031305.2016.1154108">American Statistical Association (ASA) published a
list of principles</a> guiding (mis)interpretation of <em>p</em>-values, some of which are reproduced below:</p>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>
<em>P</em>-values do not measure the probability that the studied hypothesis is true.</li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Scientific conclusions and business or policy decisions should not be based only on whether a <em>p</em>-value passes a specific threshold.</li>
</ol>
</blockquote>
<blockquote>
<ol start="4" style="list-style-type: decimal">
<li>
<em>P</em>-values and related analyses should not be reported selectively.</li>
</ol>
</blockquote>
<blockquote>
<ol start="5" style="list-style-type: decimal">
<li>
<em>p</em>-value, or statistical significance, does not measure the size of an effect or the importance of a result.</li>
</ol>
</blockquote>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 2.4  (Gender inequality and permutation tests) </strong></span>We consider data from <span class="citation">Rosen and Jerdee (<a href="references.html#ref-Rosen:1974" role="doc-biblioref">1974</a>)</span>, who look at sex role stereotypes and their impacts on promotion and opportunities for women candidates. The experiment took place in 1972 and the experimental units, which consisted of 95 male bank supervisors, were submitted to various memorandums and asked to provide ratings or decisions based on the information provided.</p>
<p>We are interested in Experiment 1 related to promotion of employees: managers were requested to decide on whether or not to promote an employee to become branch manager based on recommendations and ratings on potential for customer and employee relations.</p>
<p>The authors intervention focused on the description of the nature (complexity) of the manager’s job (either simple or complex) and the sex of the candidate (male or female): all files were otherwise similar.</p>
<p>We consider for simplicity only sex as a factor and aggregate over job for the <span class="math inline">\(n=93\)</span> replies. Table <a href="hypothesis-testing.html#tab:rosen-table1">2.1</a> shows the counts for each possibility.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:rosen-table1">Table 2.1: </span>Promotion recommandation to branch manager based on sex of the applicant.
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
male
</th>
<th style="text-align:right;">
female
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
promote
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
19
</td>
</tr>
<tr>
<td style="text-align:left;">
hold file
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
30
</td>
</tr>
</tbody>
</table></div>
<p>The null hypothesis of interest here that sex has no impact, so the probability of promotion is the same for men and women. Let <span class="math inline">\(p_{\text{m}}\)</span> and <span class="math inline">\(p_{\text{w}}\)</span> denote these respective probabilities; we can thus write mathematically the null hypothesis as <span class="math inline">\(\mathscr{H}_0: p_{\text{m}} = p_{\text{w}}\)</span> against the alternative <span class="math inline">\(\mathscr{H}_a: p_{\text{m}} \neq p_{\text{w}}\)</span>.</p>
<p>The test statistic typically employed for two by two contingency tables is a chi-square test<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;If you have taken advanced modelling courses, this is a score test obtained by fitting a Poisson regression with &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;action&lt;/code&gt; as covariates; the null hypothesis corresponding to lack of interaction term between the two.&lt;/p&gt;"><sup>10</sup></a>, which compares the overall proportions of promoted to that in for each subgroup. The sample proportion for male is 32/42 = ~76%, compared to 19/49 or ~49% for female. While it seems that this difference of 16% is large, it could be spurious: the standard error for the sample proportions is roughly 3.2% for male and 3.4% for female.</p>
<p>If there was no discrimination based on sex, we would expect the proportion of people promoted to be the same overall; this is 51/93 =0.55 for the pooled sample. We could simply do a test for the mean difference, but rely instead on the Pearson contingency <span class="math inline">\(X^2_p\)</span> (aka chi-square) test, which compares the expected counts (based on equal promotion rates) to the observed counts, suitably standardized. If the discrepancy is large between expected and observed, than this casts doubt on the validity of the null hypothesis.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Create a 2x2 matrix (contingency table) with the counts</span></span>
<span><span class="va">dat_exper1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">32L</span>, <span class="fl">12L</span>, <span class="fl">19L</span>, <span class="fl">30L</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span>, nrow <span class="op">=</span> <span class="fl">2</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># Calculate the statistic on data</span></span>
<span><span class="va">obs_stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">dat_exper1</span>, correct <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co"># Tidy output to get a tibble</span></span>
<span><span class="va">test_res</span> <span class="op">&lt;-</span> <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="va">obs_stat</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:print-tab-example-chisq-test-rosen">Table 2.2: </span>Chi-square test for experiment 1 of Rosen and Jerdee (1974)
</caption>
<thead><tr>
<th style="text-align:right;">
statistic
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:right;">
parameter
</th>
<th style="text-align:left;">
method
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:right;">
10.8
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Pearson’s Chi-squared test
</td>
</tr></tbody>
</table></div>
<p>If the counts of each cell are large, the null distribution of the chi-square test is well approximated by a <span class="math inline">\(\chi^2\)</span> distribution. The output of the test includes the value of the statistic, the degrees of freedom of the <span class="math inline">\(\chi^2\)</span> approximation and the <em>p</em>-value, which gives the probability that a random draw from a <span class="math inline">\(\chi^2_1\)</span> distribution is larger than the observed test statistic <strong>assuming the null hypothesis is true</strong>. The <em>p</em>-value is very small, 0.001, which means such a result is quite unlikely to happen by chance if there was no sex-discrimination.</p>
<p>There are alternative test statistics that could be used, among which the odds ratio. The odds of an event is the ratio of the number of success over failure: in our example, this would be the number of promoted over held files. The odds of promotion for male is 32/12, whereas that of female is 19/30. The odds ratio for male versus female is thus <span class="math inline">\(\mathsf{OR}=\)</span> (32/12) / (19/30)= 4.21. Under the null hypothesis, <span class="math inline">\(\mathscr{H}_0: \mathsf{OR}=\)</span> 1 (same probability of being promoted) (why?)</p>
<p>Fisher’s test assumes that the row and sum totals are fixed (that is, the number of promoted/withheld files and male/female are fixed at the design stage) and uses this to derive the exact probability of observing this particular configuration if the proportion of success was the same. The numerical value of Fisher’s exact test, obtained by running <code>fisher.test(dat_exper1)</code>, is different from that obtained earlier, but so is the null distribution<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The null distribution for Fisher’s exact test is hypergeometric. This fact is well known in combinatorics, also known as the art of counting marbles.&lt;/p&gt;"><sup>11</sup></a>. On the contrary, the <em>p</em>-value is very close to the one reported for the <span class="math inline">\(\chi^2\)</span> test in Table <a href="hypothesis-testing.html#tab:print-tab-example-chisq-test-rosen">2.2</a>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:print-tab-example-fisher-test-rosen">Table 2.3: </span>Fisher’s exact test for experiment 1 of Rosen and Jerdee (1974)
</caption>
<thead><tr>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:left;">
method
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:right;">
4.1
</td>
<td style="text-align:right;">
0.0016
</td>
<td style="text-align:left;">
Fisher’s Exact Test for Count Data
</td>
</tr></tbody>
</table></div>
<p>Yet another alternative to obtain a benchmark to assess the outlyingness of the observed odds ratio is to use simulations. Consider a database containing the raw data with 93 rows, one for each manager, with for each an indicator of <code>action</code> and the <code>sex</code> of the hypothetical employee presented in the task.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:dat-long-test-rosen-print">Table 2.4: </span>First five rows of the database in long format for experiment 1 of Rosen and Jerdee.
</caption>
<thead><tr>
<th style="text-align:left;">
action
</th>
<th style="text-align:left;">
sex
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
hold file
</td>
<td style="text-align:left;">
female
</td>
</tr>
<tr>
<td style="text-align:left;">
promote
</td>
<td style="text-align:left;">
female
</td>
</tr>
<tr>
<td style="text-align:left;">
promote
</td>
<td style="text-align:left;">
male
</td>
</tr>
<tr>
<td style="text-align:left;">
hold file
</td>
<td style="text-align:left;">
female
</td>
</tr>
<tr>
<td style="text-align:left;">
hold file
</td>
<td style="text-align:left;">
female
</td>
</tr>
</tbody>
</table></div>
<p>Under the null hypothesis, sex has no incidence on the action of the manager. This means we could get an idea of the “what-if” world by shuffling the sex labels repeatedly. Thus, we could obtain a benchmark by repeating the following steps multiple times:</p>
<ol style="list-style-type: decimal">
<li>permute the labels for <code>sex</code>,</li>
<li>recreate a contingency table by aggregating counts,</li>
<li>calculate the odds ratio for the simulated table.</li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:infer-odds-ratio-permutation"></span>
<img src="02-hypothesis_testing_files/figure-html/infer-odds-ratio-permutation-1.png" alt="Histogram of the simulated null distribution of the odds ratio statistic obtained using a permutation test; the vertical red line indicates the sample odds ratio." width="85%"><p class="caption">
Figure 2.7: Histogram of the simulated null distribution of the odds ratio statistic obtained using a permutation test; the vertical red line indicates the sample odds ratio.
</p>
</div>
<p>The histogram in Figure <a href="hypothesis-testing.html#fig:infer-odds-ratio-permutation">2.7</a> shows the distribution of the odds ratio based on 10 000 permutations. Reassuringly, we again get roughly the same approximate <em>p</em>-value, here 0.002.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value obtained for the permutation test would change from one run to the next since it’s input is a random permutation. However, the precision of the proportion statistic is sufficient for decision making.&lt;/p&gt;"><sup>12</sup></a></p>
<p>The article concluded (in light of the above and further experiments)</p>
<blockquote>
<p>Results confirmed the hypothesis that male administrators tend to discriminate against female employees in personnel decisions involving promotion, development, and supervision.</p>
</blockquote>
</div>
<div class="pitfall">
<p>In the first experiment, managers were also asked to rank applications on their potential for both employee and customer relations using a Likert scale of six items ranging from (1) extremely unfavorable to (6) extremely favorable. However, only the averages are reported in Table 1 along with <span class="citation">(<a href="references.html#ref-Rosen:1974" role="doc-biblioref">Rosen and Jerdee 1974</a>)</span></p>
<blockquote>
<p>Mean rating for the male candidate was 4.73 compared to a mean rating of 4.25 for the female candidate (<span class="math inline">\(F=4.76\)</span>, <span class="math inline">\(\text{df} = 1/80\)</span>, <span class="math inline">\(p &lt; .05\)</span>)</p>
</blockquote>
<p>The degrees of freedom (80) are much too few compared to the number of observations, implying non-response that isn’t discussed.</p>
<p>Partial or selective reporting of statistical procedures hinders reproducibility. In general, the presentation should explicitly state the name of the test statistic employed, the sample size, mean and variance estimates, the null distribution used to assess significance and its parameters, if any. Without these, we are left to speculate.</p>
</div>
</div>
</div>
<div id="confidence-intervals" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Confidence intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>confidence interval</strong> is an alternative way to present the conclusions of an hypothesis test performed at significance level <span class="math inline">\(\alpha\)</span> by giving a range of all values for which the null isn’t rejected at the chosen level. It is often combined with a point estimator <span class="math inline">\(\hat{\theta}\)</span> to give an indication of the variability of the estimation procedure. Wald-based <span class="math inline">\((1-\alpha)\)</span> confidence intervals for a parameter <span class="math inline">\(\theta\)</span> are of the form
<span class="math display">\[\begin{align*}
\widehat{\theta} \pm \mathfrak{q}_{\alpha/2} \; \mathrm{se}(\widehat{\theta})
\end{align*}\]</span>
where <span class="math inline">\(\mathfrak{q}_{\alpha/2}\)</span> is the <span class="math inline">\(1-\alpha/2\)</span> quantile of the null distribution of the Wald statistic
<span class="math display">\[\begin{align*}
T =\frac{\widehat{\theta}-\theta}{\mathrm{se}(\widehat{\theta})},
\end{align*}\]</span>
where <span class="math inline">\(\theta\)</span> represents the postulated value for the fixed, but unknown value of the parameter. The bounds of the confidence intervals are random variables, since both estimators of the parameter and its standard error, <span class="math inline">\(\widehat{\theta}\)</span> and <span class="math inline">\(\mathrm{se}(\widehat{\theta})\)</span>, are random variables: their values will vary from one sample to the next.</p>
<p>Before the interval is calculated, there is a <span class="math inline">\(1-\alpha\)</span> probability that <span class="math inline">\(\theta\)</span> is contained in the <strong>random</strong> interval <span class="math inline">\((\widehat{\theta} - \mathfrak{q}_{\alpha/2} \; \mathrm{se}(\widehat{\theta}), \widehat{\theta} + \mathfrak{q}_{\alpha/2} \; \mathrm{se}(\widehat{\theta}))\)</span>, where <span class="math inline">\(\widehat{\theta}\)</span> denotes the estimator. Once we obtain a sample and calculate the confidence interval, there is no more notion of probability: the true value of the parameter <span class="math inline">\(\theta\)</span> is either in the confidence interval or not. We can interpret confidence interval’s as follows: if we were to repeat the experiment multiple times, and calculate a <span class="math inline">\(1-\alpha\)</span> confidence interval each time, then roughly <span class="math inline">\(1-\alpha\)</span> of the calculated confidence intervals would contain the true value of <span class="math inline">\(\theta\)</span> in repeated samples (in the same way, if you flip a coin, there is roughly a 50-50 chance of getting heads or tails, but any outcome will be either). Our confidence is in the <em>procedure</em> we use to calculate confidence intervals and not in the actual values we obtain from a sample.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:intconf"></span>
<img src="02-hypothesis_testing_files/figure-html/intconf-1.png" alt="95\% confidence intervals for the mean of a standard normal population for 100 random samples. On average, 5\% of these intervals fail to include the true mean value of zero (in red)." width="85%"><p class="caption">
Figure 2.8: 95% confidence intervals for the mean of a standard normal population for 100 random samples. On average, 5% of these intervals fail to include the true mean value of zero (in red).
</p>
</div>
<p>If we are only interested in the binary decision rule reject/fail to reject <span class="math inline">\(\mathscr{H}_0\)</span>, the confidence interval is equivalent to a <em>p</em>-value since it leads to the same conclusion. Whereas the <span class="math inline">\(1-\alpha\)</span> confidence interval gives the set of all values for which the test statistic doesn’t provide enough evidence to reject <span class="math inline">\(\mathscr{H}_0\)</span> at level <span class="math inline">\(\alpha\)</span>, the <em>p</em>-value gives the probability under the null of obtaining a result more extreme than the postulated value and so is more precise for this particular value. If the <em>p</em>-value is smaller than <span class="math inline">\(\alpha\)</span>, our null value <span class="math inline">\(\theta\)</span> will be outside of the confidence interval and vice-versa.</p>
</div>
<div id="power" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Power<a class="anchor" aria-label="anchor" href="#power"><i class="fas fa-link"></i></a>
</h2>
<p>There are two sides to an hypothesis test: either we want to show it is not unreasonable to assume the null hypothesis, or else we want to show beyond reasonable doubt that a difference or effect is significative: for example, one could wish to demonstrate that a new website design (alternative hypothesis) leads to a significant increase in sales relative to the status quo. Our ability to detect these improvements and make discoveries depends on the power of the test: the larger the power, the greater our ability to reject <span class="math inline">\(\mathscr{H}_0\)</span> when the latter is false.</p>
<p>Failing to reject <span class="math inline">\(\mathscr{H}_0\)</span> when <span class="math inline">\(\mathscr{H}_a\)</span> is true (not guilty verdict of a criminal) corresponds to the definition of type II error, say. The <strong>power of a test</strong> is the probability of <strong>correctly</strong> rejecting the null hypothesis <span class="math inline">\(\mathscr{H}_0\)</span> when <span class="math inline">\(\mathscr{H}_0\)</span> is false, i.e.,
<span class="math display">\[\begin{align*}
\mathsf{Pr}_a(\text{reject} \mathscr{H}_0)
\end{align*}\]</span>
Depending on the alternative models, it is more or less easy to detect that the null hypothesis is false and reject in favor of an alternative.
Power is thus a measure of our ability to detect real effects.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:power1"></span>
<img src="02-hypothesis_testing_files/figure-html/power1-1.png" alt="Comparison between null distribution (full curve) and a specific alternative for a *t*-test (dashed line). The power corresponds to the area under the curve of the density of the alternative distribution which is in the rejection area (in white)." width="85%"><p class="caption">
Figure 2.9: Comparison between null distribution (full curve) and a specific alternative for a <em>t</em>-test (dashed line). The power corresponds to the area under the curve of the density of the alternative distribution which is in the rejection area (in white).
</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:power2"></span>
<img src="02-hypothesis_testing_files/figure-html/power2-1.png" alt="Increase in power due to an increase in the mean difference between the null and alternative hypothesis. Power is the area in the rejection region (in white) under the alternative distribution (dashed): the latter is more shifted to the right relative to the null distribution (full line)." width="85%"><p class="caption">
Figure 2.10: Increase in power due to an increase in the mean difference between the null and alternative hypothesis. Power is the area in the rejection region (in white) under the alternative distribution (dashed): the latter is more shifted to the right relative to the null distribution (full line).
</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:power3"></span>
<img src="02-hypothesis_testing_files/figure-html/power3-1.png" alt="Increase of power due to an increase in the sample size or a decrease of standard deviation of the population: the null distribution (full line) is more concentrated. Power is given by the area (white) under the curve of the alternative distribution (dashed). In general, the null distribution changes with the sample size." width="85%"><p class="caption">
Figure 2.11: Increase of power due to an increase in the sample size or a decrease of standard deviation of the population: the null distribution (full line) is more concentrated. Power is given by the area (white) under the curve of the alternative distribution (dashed). In general, the null distribution changes with the sample size.
</p>
</div>
<p>We want to choose an experimental design and a test statistic that leads to high power, so that this power is as close as possible to one. Minimally, the power of the test should be <span class="math inline">\(\alpha\)</span> because we reject the null hypothesis <span class="math inline">\(\alpha\)</span> fraction of the time even when <span class="math inline">\(\mathscr{H}_0\)</span> is true. Power depends on many criteria, notably</p>
<ul>
<li>the effect size: the bigger the difference between the postulated value for <span class="math inline">\(\theta_0\)</span> under <span class="math inline">\(\mathscr{H}_0\)</span> and the observed behaviour, the easier it is to departures from <span class="math inline">\(\theta_0\)</span>.
(Figure <a href="hypothesis-testing.html#fig:power3">2.11</a>); it’s easier to spot an elephant in a room than a mouse.</li>
<li>variability: the less noisy your data, the easier it is to detect differences between the curves (big differences are easier to spot, as Figure <a href="hypothesis-testing.html#fig:power2">2.10</a> shows);</li>
<li>the sample size: the more observation, the higher our ability to detect significative differences because the amount of evidence increases as we gather more observations.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The standard error decreases with sample size &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; at a rate (typically) of &lt;span class="math inline"&gt;\(n^{-1/2}\)&lt;/span&gt;. The null distribution also becomes more concentrated as the sample size increase.&lt;/p&gt;'><sup>13</sup></a> In experimental designs, the power also depends on how many observations are allocated to each group.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;While the default is to assign an equal number to each subgroup, power may be maximized by specifying different sample size in each group if the variability of the measurement differ in these groups.&lt;/p&gt;"><sup>14</sup></a>
</li>
<li>the choice of test statistic: there is a plethora of possible statistics to choose from as a summary of the evidence against the null hypothesis. While some are clearly inferior, some less powerful choice may be preferable. For example, rank-based statistics discard information about the observed values of the response, focusing instead on their relative ranking. The resulting tests are typically less powerful, but they are less sensible to model assumption, model misspecification and outliers.</li>
</ul>
<p>Changing the value of <span class="math inline">\(\alpha\)</span> has an impact on the power, since larger values of <span class="math inline">\(\alpha\)</span> move the cutoff towards the bulk of the distribution. It entails a higher percentage of rejection also when the alternative is false. However, the value of <span class="math inline">\(\alpha\)</span> is fixed beforehand to control the type I error (avoid judicial mistakes). The power corresponds to the red shaded area on the right panel of Figure <a href="hypothesis-testing.html#fig:compareFnullalternative">2.12</a>, would become larger if we moved the cutoff value lower, corresponding to larger <span class="math inline">\(\alpha\)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:compareFnullalternative"></span>
<img src="02-hypothesis_testing_files/figure-html/compareFnullalternative-1.png" alt="Densities of the null (left) and alternative (right) distributions for the one-way analysis of variance: if some of the group means are different, the curve gets shifted to the right. The shaded blue area is the type I error (null hypothesis) and the type II error (alternative hypothesis); the power is the area of the red shaded region." width="85%"><p class="caption">
Figure 2.12: Densities of the null (left) and alternative (right) distributions for the one-way analysis of variance: if some of the group means are different, the curve gets shifted to the right. The shaded blue area is the type I error (null hypothesis) and the type II error (alternative hypothesis); the power is the area of the red shaded region.
</p>
</div>
</div>
<div id="conclusion-1" class="section level2" number="2.6">
<h2>
<span class="header-section-number">2.6</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion-1"><i class="fas fa-link"></i></a>
</h2>
<p>Richard McElreath in the <a href="http://xcelab.net/rmpubs/sr2/statisticalrethinking2_chapters1and2.pdf">first chapter</a> of his book <span class="citation">(<a href="references.html#ref-McElreath:2020" role="doc-biblioref">McElreath 2020</a>)</span> draws a parallel between statistical tests and golems (i.e., robots): neither</p>
<blockquote>
<p>discern when the context is inapropriate for its answers. It just knows its own procedure […] It just does as it’s told.</p>
</blockquote>
<p>The responsibility therefore lies with the user to correctly use statistical procedures and be aware of their limitations</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="introduction.html"><span class="header-section-number">1</span> Introduction</a></div>
<div class="next"><a href="CRT.html"><span class="header-section-number">3</span> Completely randomized designs</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#hypothesis-testing"><span class="header-section-number">2</span> Hypothesis testing</a></li>
<li><a class="nav-link" href="#hypothesis"><span class="header-section-number">2.1</span> Hypothesis</a></li>
<li><a class="nav-link" href="#sampling-variability"><span class="header-section-number">2.2</span> Sampling variability</a></li>
<li>
<a class="nav-link" href="#tests"><span class="header-section-number">2.3</span> Hypothesis testing</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hypothesis-1"><span class="header-section-number">2.3.1</span> Hypothesis</a></li>
<li><a class="nav-link" href="#test-statistic"><span class="header-section-number">2.3.2</span> Test statistic</a></li>
<li><a class="nav-link" href="#null-distribution-and-p-value"><span class="header-section-number">2.3.3</span> Null distribution and p-value</a></li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">2.3.4</span> Conclusion</a></li>
</ul>
</li>
<li><a class="nav-link" href="#confidence-intervals"><span class="header-section-number">2.4</span> Confidence intervals</a></li>
<li><a class="nav-link" href="#power"><span class="header-section-number">2.5</span> Power</a></li>
<li><a class="nav-link" href="#conclusion-1"><span class="header-section-number">2.6</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/lbelzile/math80667a/blob/master/02-hypothesis_testing.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/lbelzile/math80667a/edit/master/02-hypothesis_testing.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Experimental Design and Statistical Methods</strong>" was written by Léo Belzile. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
