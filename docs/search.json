[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimental design and statistical methods",
    "section": "",
    "text": "Léo Belzile\n\n2025-07-17\n\n\n\n\nWelcome\nThis book is a web complement to MATH 80667A Experimental Designs and Statistical Methods, a graduate course offered at HEC Montréal in the joint Ph.D. program in Management. Consult the course webpage for more details.\nThese notes are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License and were last compiled on Thursday, July 17 2025.\nThe objective of the course is to teach basic principles of experimental designs and statistical inference using the R programming language. We will pay particular attention to the correct reporting and interpretation of results and learn how to review critically scientific papers using experimental designs. The unifying theme of the book is that statistics are summary of evidence, and statistic as a field is the science of decision-making in the presence of uncertainty. We use examples drawn from published articles in management sciences to illustrate core concepts.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Study type\nThe advancement of science is built on our ability to study and assess research hypotheses. This chapter covers the basic concepts of experiments, starting with vocabulary associated with the field. Emphasis is placed on the difference between experiments and observations.\nThis course covers experimental designs. In an experiment, the researcher manipulates one or more features (say the complexity of a text the person must read, or the type of advertisement campaign displayed, etc.) to study their impact. In general, however (Cox 1958)\nThe purpose of experiments is to arrange data collection so as to be capable of disentangling the differences due to treatment from those due to the (often large) intrinsic variation of the measurements. We typically expect differences between treatments (and thus the effect) to be comparatively stable relative to the measurement variation.\nThere are two main categories of studies: observational and experimental. The main difference between the two is treatment assignment. In observational studies, a feature or potential cause is measured, but not assigned by the experimenter. By contrast, the treatment assignment mechanism is fully determined by the experimenter in the latter case.\nFor example, an economist studying the impact of interest rates on the price of housing can only look at historical records of sales. Similarly, surveys studying the labour market are also observational: people cannot influence the type of job performed by employees or their social benefits to see what could have happened. Observational studies can lead to detection of association, but only an experiment in which the researcher controls the allocation mechanism through randomization can lead to directly establish existence of a causal relationship. Because everything else is the same in a well controlled experiment, any treatment effect should be in principle caused by the experimental manipulation.1\nFigure 1.1: Two by two classification matrix for experiments based on sampling and study type. Source: Mine Çetinkaya-Rundel and OpenIntro, distributed under the CC BY-SA license.\nFigure 1.1 summarizes the two preceding sections. Random allocation of measurement units to treatment and random samples from the population lead to ideal studies, but may be impossible due to ethical considerations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#terminology",
    "href": "introduction.html#terminology",
    "title": "1  Introduction",
    "section": "1.2 Terminology",
    "text": "1.2 Terminology\nIn its simplest form, an experimental design is a comparison of two or more treatments (experimental conditions):\n\nThe subjects (or experimental units) in the different groups of treatment have similar characteristics and are treated exactly the same way in the experimentation except for the treatment they are receiving. Formally, an experimental unit is the smallest division such that any two units may receive different treatments.\nThe measurement unit is the smallest level (time point, individual) at which measurement are recorded; sometimes called observational unit in other textbooks.\nExplanatories (independent variables) are variables that impact the response. They can be continuous (dose) or categorical variables; in the latter case, they are termed factors.\nThe experimental treatments or conditions are manipulated and controlled by the researcher. Oftentimes, there is a control or baseline treatment relative to which we measure improvement (e.g., a placebo for drugs).\nAfter the different treatments have been administered to subjects participating in a study, the researcher measures one or more outcomes (also called responses or dependent variables) on each subject.\nObserved differences in the outcome variable between the experimental conditions (treatments) are called treatment effects.\n\n\nExample 1.1 (Pedagogical experience) Suppose we want to study the effectiveness of different pedagogical approaches to learning. Evidence-based pedagogical researchs point out that active learning leads to higher retention of information. To corroborate this research hypothesis, we can design an experiment in which different sections of a course are assigned to different teaching methods. In this example, each student in a class group receives the same teaching assignment, so the experimental units are the sections and the measurement units are the individual students. The treatment is the teaching method (traditional teaching versus flipped classroom).\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nThe marketing department of a company wants to know the value of its brand by determining how much more customers are willing to pay for their product relative to the cheaper generic product offered by the store. Economic theory suggests a substitution effect: while customers may prefer the brand product, they will switch to the generic version if the price tag is too high. To check this theory, one could design an experiment.\nAs a researcher, how would you conduct this study? Identify a specific product. For the latter, define\n\nan adequate response variable\nthe experimental and measurement units\npotential blocking factors\n\n\n\nThe main reason experiments should be preferred to collection of observational data is that they allow us, if they are conducted properly, to draw causal conclusions about the phenomenon of interest. If we take a random sample from the population of interest, split it randomly and manipulate only certain aspects, then all differences between groups must be due to those changes.\nAs Hariton and Locascio (2018) put it:\n\nRandomised controlled trials (RCTs) are the reference standard for studying causal relationships between interventions and outcomes as randomisation eliminates much of the bias inherent with other study designs\n\n\n\n\n\n\n\nQuasi experiments\n\n\n\nSometimes, it is impossible or unethical to conduct an experiment. This seemingly precludes study many social phenomena, such as the effect on women and infantile mortality of strict bans on abortions. When changes in legislation occur (such as the Supreme court overturning Roe and Wade), this offers a window to compare neighbouring states.\nCanadian economist David Card was co-awarded the 2021 Nobel Memorial Prize in Economic Sciences for his work in experimental economics. One of his most cited paper is Card and Krueger (1994), a study that looked at the impact of an increase in minimum wage on employment figures. Card and Krueger (1994) used a planned increase of the minimum wage of $0.80 USD in New Jersey to make comparisons with neighbouring Eastern Pennsylvania counties by studying 410 fast food outlets. The authors found no evidence of a negative impact on employment of this hike.\n\n\n\n\n\n\n\n\nPoint of terminology: internal and external validity\n\n\n\nA study from which we can study causal relationships is said to have internal validity. By design, good experiments should have this desirable property because the random allocation of treatment guarantees, if randomization is well performed, that the effect of interest is causal. There are many other aspects, not covered in the class, that can threaten internal validity.\nExternal validity refers directly to generalizability of the conclusions of a study: Figure 1.1 shows that external validity is directly related to random sampling from the population\n\n\n\n\n\n\n\n\nPoint of terminology: between-subjects and within-subjects designs\n\n\n\nIn between-subjects designs, subjects are randomly assigned to only one of the different experimental conditions. On the contrary, participants receive many or all of the experimental treatments in a within-subjects design, the order of assignment of the conditions typically being random.\nWhile within-subject designs allow for a better use of available ressources (it is cheaper to have fewer participants perform multiple tasks), observations from within-design are correlated and more subject to missingness and learning effects, all of which require special statistical treatment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#review-of-basic-concepts",
    "href": "introduction.html#review-of-basic-concepts",
    "title": "1  Introduction",
    "section": "1.3 Review of basic concepts",
    "text": "1.3 Review of basic concepts\n\n1.3.1 Variables\nThe choice of statistical model and test depends on the underlying type of the data collected. There are many choices: quantitative (discrete or continuous) if the variables are numeric, or qualitative (binary, nominal, ordinal) if they can be described using an adjective; I prefer the term categorical, which is more evocative. The choice of graphical representation for data is contingent on variable type. Specifically,\n\na variable represents a characteristic of the population, for example the sex of an individual, the price of an item, etc.\nan observation is a set of measures (variables) collected under identical conditions for an individual or at a given time.\n\nMost of the models we will deal with are so-called regression models, in which the mean of a quantitative variable is a function of other variables, termed explanatories. There are two types of numerical variables\n\na discrete variable takes a countable number of values, prime examples being binary variables or count variables.\na continuous variable can take (in theory) an infinite possible number of values, even when measurements are rounded or measured with a limited precision (time, width, mass). In many case, we could also consider discrete variables as continuous if they take enough values (e.g., money).\n\nCategorical variables take only a finite of values. They are regrouped in two groups, nominal if there is no ordering between levels (sex, colour, country of origin) or ordinal if they are ordered (Likert scale, salary scale) and this ordering should be reflected in graphs or tables. We will bundle every categorical variable using arbitrary encoding for the levels: for modelling, these variables taking \\(K\\) possible values (or levels) must be transformed into a set of \\(K-1\\) binary variables \\(T_1, \\ldots, T_K\\), each of which corresponds to the logical group \\(k\\) (yes = 1, no = 0), the omitted level corresponding to a baseline when all of the \\(K-1\\) indicators are zero. Failing to declare categorical variables in your software is a common mistake, especially when these are saved in the database using integers (1,2, \\(\\ldots\\)) rather than as text (Monday, Tuesday, \\(\\ldots\\)).\nWe can characterize the set of all potential values their measurements can take, together with their frequency, via a distribution. The latter can be represented graphically using an histogram or a density plot2 if the data are continuous, or a bar plot for discrete or categorical measurements.\n\nExample 1.2 (Die toss) The distribution of outcomes of a die toss is discrete and takes values \\(1, \\ldots, 6\\). Each outcome is equally likely with probability \\(1/6\\).\n\n\nExample 1.3 (Normal distribution) Mathematical theory suggests that, under general conditions, the distribution of a sample average is approximately distributed according to a normal (aka Gaussian) distribution: this result is central to most of statistics. Normally distributed data are continuous; the distribution is characterized by a bell curve, light tails and it is symmetric around it’s mean. The shape of the facade of Hallgrímskirkja church in Reykjavik, shown in Figure 1.2, closely resembles the density a normal distribution, which lead Khoa Vu to call it ‘a normal church’ (chuckles).\n\n\n\n\n\n\n\n\nFigure 1.2: Photography of the Hallgrímskirkja church in Reykjavik, Iceland by Dolf van der Haven, reproduced under a CC BY-ND-NC 2.0 license.\n\n\n\n\n\nThe normal distribution is fully characterized by two parameters: the average \\(\\mu\\) and the standard deviation \\(\\sigma\\). The left panel of Figure 1.3 shows an arbitrary continuous distribution and the values of a random sample of \\(n=1000\\) draws. The right panel shows the histogram of the sample mean value based on a very large number of random samples of size \\(n=25\\), drawn from the same distribution. The superimposed black curve is a normal density curve whose parameters match those given by the central limit theorem: the approximation is seemingly quite accurate.\nThis fact explains the omnipresence of the normal distribution in introductory data science courses, as well as the prevalence of sample mean and sample variance as key summary statistics.3\n\n\n\n\n\n\n\n\nFigure 1.3: Graphical representation of the distribution of a continuous variable, with histogram of a sample of \\(n=1000\\) observations drawn from the distribution (left) and distribution of the sample mean, obtained by repeatedly drawing random sample of \\(n=25\\) observations and computing their average (right). The curve shows the normal distribution approximation based on the central limit theorem.\n\n\n\n\n\n\n\n\n\n\n\n\nThinking outside the box\n\n\n\nOne key aspect, often neglected in studies, is the discussion of the metric used for measurement of the response. While previous research may have identified instruments (like questionnaires) and particular wording for studying a particular aspect of individuals, there is a lot of free room for researchers to choose from that may impact conclusions. For example, if one uses a Likert scale, what should be the range of the scale? Too coarse a choice may lead to limited capability to detect, but more truthfulness, while there may be larger intrinsic measurement with a finer scale.\nLikewise, many continuous measures (say fMRI signal) can be discretized to provide a single numerical value. Choosing the average signal, range, etc. as outcome variable may lead to different conclusions.\nChoosing a particular instrument or metric could be in principle done by studying (apriori) the distribution of the values for the chosen metric using a pilot study: this will give researchers some grasp of the variability of those measures.\n\n\nAt the heart of most analysis are measurements. The data presented in the course have been cleaned and oftentimes the choice of explanatory variables and experimental factor4 is evident from the context. In applications, however, this choice is not always trivial.\n\n\n1.3.2 Population and samples\nOnly for well-designed sampling schemes does results generalize beyond the group observed. It is thus of paramount importance to define the objective and the population of interest should we want to make conclusions.\nGenerally, we will seek to estimate characteristics of a population using only a sample (a sub-group of the population of smaller size). The population of interest is a collection of individuals which the study targets. For example, the Labour Force Survey (LFS) is a monthly study conducted by Statistics Canada, who define the target population as “all members of the selected household who are 15 years old and older, whether they work or not.” Asking every Canadian meeting this definition would be costly and the process would be long: the characteristic of interest (employment) is also a snapshot in time and can vary when the person leaves a job, enters the job market or become unemployed. In this example, collecting a census would be impossible and too costly.\nIn general, we therefore consider only samples to gather the information we seek to obtain. The purpose of statistical inference is to draw conclusions about the population, but using only a share of the latter and accounting for sources of variability. The pollster George Gallup made this great analogy between sample and population:\n\nOne spoonful can reflect the taste of the whole pot, if the soup is well-stirred\n\nA sample is a sub-group of individuals drawn at random from the population. We won’t focus on data collection, but keep in mind the following information: for a sample to be good, it must be representative of the population under study.\n\n\n\n\n\n\nYour turn\n\n\n\nThe Parcours AGIR at HEC Montréal is a pilot project for Bachelor in Administration students that was initiated to study the impact of flipped classroom and active learning on performance.\nDo you think we can draw conclusions about the efficacy of this teaching method by comparing the results of the students with those of the rest of the bachelor program? List potential issues with this approach addressing the internal and external validity, generalizability, effect of lurking variables, etc.\n\n\nBecause the individuals are selected at random to be part of the sample, the measurement of the characteristic of interest will also be random and change from one sample to the next. While larger samples typically carry more information, sample size is not a guarantee of quality, as the following example demonstrates.\n\nExample 1.4 (Polling for the 1936 USA Presidential Election) The Literary Digest surveyed 10 millions people by mail to know voting preferences for the 1936 USA Presidential Election. A sizeable share, 2.4 millions answered, giving Alf Landon (57%) over incumbent President Franklin D. Roosevelt (43%). The latter nevertheless won in a landslide election with 62% of votes cast, a 19% forecast error. Biased sampling and differential non-response are mostly responsible for the error: the sampling frame was built using ``phone number directories, drivers’ registrations, club memberships, etc.’’, all of which skewed the sample towards rich upper class white people more susceptible to vote for the GOP.\nIn contrast, Gallup correctly predicted the outcome by polling (only) 50K inhabitants. Read the full story here.\n\n\n\n\n\n\n\nThinking outside the box\n\n\n\nWhat are the considerations that could guide you in determining the population of interest for your study?\n\n\n\n\n1.3.3 Sampling\nBecause sampling is costly, we can only collect limited information about the variable of interest, drawing from the population through a sampling frame (phone books, population register, etc.) Good sampling frames can be purchased from sampling firms.\nIn general, randomization is necessary in order to obtain a representative sample5, one that match the characteristics of the population. Failing to randomize leads to introduction of bias and generally the conclusions drawn from a study won’t be generalizable.\nEven when measurement units are selected at random to participate, there may be bias introduced due to non-response. In the 1950s, conducting surveys was relatively easier because most people were listed in telephone books; nowadays, sampling firms rely on a mix of interactive voice response and live callers, with sampling frames mixing landlines, cellphones and online panels together with (heavy) weighting to correct for non-response. Sampling is a difficult problem with which we engage only cursorily, but readers are urged to exercise scrutiny when reading papers.\n\n\n\n\n\n\nThinking outside the box\n\n\n\nReflect on the choice of platform used to collect answers and think about how it could influence the composition of the sample returned or affect non-response in a systematic way.\n\n\nBefore examining problems related to sampling, we review the main random sampling methods. The simplest is simple random sampling, whereby \\(n\\) units are drawn completely at random (uniformly) from the \\(N\\) elements of the sampling frame. The second most common scheme is stratified sampling, whereby a certain numbers of units are drawn uniformly from strata, namely subgroups (e.g., gender). Finally, cluster sampling consists in sampling only from some of these subgroups.\n\nExample 1.5 (Sampling schemes in a picture) Suppose we wish to look at student satisfaction regarding the material taught in an introductory statistics course offered to multiple sections. The population consists of all students enrolled in the course in a given semester and this list provides the sampling frame. We can define strata to consist of class group. A simple random sample would be obtaining by sampling randomly abstracting from class groups, a stratified sample by drawing randomly a number from each class group and a cluster sampling by drawing all students from selected class groups. Cluster sampling is mostly useful if all groups are similar and if the costs associated to sampling from multiple strata are expensive.\nFigure 1.4 shows three sampling schemes: the middle corresponds to stratum (e.g., age bands) whereas the right contains clusters (e.g., villages or classrooms).\n\n\n\n\n\n\n\n\nFigure 1.4: Illustration of three sampling schemes from nine groups: simple random sampling (left), stratified sampling (middle) and cluster sampling (right).\n\n\n\n\n\nStratified sampling is typically superior if we care about having similar proportions of sampled in each group and is useful for reweighting: in Figure 1.4, the true proportion of sampled is \\(1/3\\), with the simple random sampling having a range of [\\(0.22, 0.39\\)] among the strata, compared to [\\(0.31, 0.33\\)] for the stratified sample.\n\n\n\n\n\n\n\nThinking outside the box\n\n\n\nThe credibility of a study relies in large part on the quality of the data collection. Why is it customary to report descriptive statistics of the sample and a description of the population?\n\n\nThere are other instances of sampling, most of which are non-random and to be avoided whenever possible. These include convenience samples, consisting of measurement units that are easy to access or include (e.g., friends, students from a university, passerby in the street). Much like for anecdotal reports, these measurement units need not be representative of the whole population and it is very difficult to understand how they relate to the latter.\nIn recent years, there has been a proliferation of studies employing data obtained from web experimentation plateforms such as Amazon’s Mechanical Turk (MTurk), to the point that the Journal of Management commissioned a review (Aguinis, Villamor, and Ramani 2021). These samples are subject to self-selection bias and articles using them should be read with a healthy dose of skepticism. Unless good manipulation checks are conducted (e.g., to ensure participants are faithful and answer in a reasonable amount of time), I would reserve these tools for paired samples (e.g., asking people to perform multiple tasks presented in random order) for which the composition of the population is less important. To make sure your sample matches the target population, you can use statistical tests and informal comparison and compare the distribution of individuals with the composition obtained from the census.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#examples-of-experimental-designs",
    "href": "introduction.html#examples-of-experimental-designs",
    "title": "1  Introduction",
    "section": "1.4 Examples of experimental designs",
    "text": "1.4 Examples of experimental designs\nThe field of experimental design has a long history, starting with agricultural field trials.\n\nExample 1.6 (Agricultural field trials at the Rothamsted Research Station.) The Rothamsted Research Station in the UK has been conducting experiments since 1843. Ronald A. Fisher, who worked 14 years at Rothamsted from 1919, developed much of the statistical theory underlying experimental design, inspired from his work there. Yates (1964) provides a recollection of his contribution to the field.\n\n\n\n\n\n\n\n\nFigure 1.5: 1958 plan for the Highfield Ley–Arable Experiment. Source: Rothamsted Research Station, reproduced under the CC BY 4.0 license.\n\n\n\n\n\n\nExperimental design revolves in large part in understanding how best to allocate our resources, determine the impact of policies or choosing the most effective “treatment” from a series of option.\n\nExample 1.7 (Modern experiments and A/B testing) Most modern experiments happen online, with tech companies running thousands of experiments on an ongoing basis in order to discover improvement to their interfaces that lead to increased profits. An Harvard Business Review article (Kohavi and Thomke 2017) details how small tweaks to the display of advertisements in the Microsoft Bing search engine landing page lead to a whooping 12% increase in revenues. Such randomized control trials, termed A/B experiments, involve splitting incoming traffic into separate groups; each group will see different views of the webpage that differ only ever slightly. The experimenters then compare traffic and click revenues. At large scale, even small effects can have major financial consequences and can be learned despite the large variability in customer behaviour.\n\nThere are also multiple examples of randomized control experiments used for policy making.\n\nExample 1.8 (Experiments on wellness programs) Song and Baicker (2019) conducted a large randomized trial over a period of 18 months to study the impact of wellness programs in US companies. The industry, worth more than 8 billions USD, has significantly increased following the passage of the Affordable Care Act, aka Obamacare. The findings are vulgarized in a press release by Jake Miller from Harvard News & Research: they show that, while there was seemingly an impact on physical activity and well-being, there were no evidence of changes in absenteeism, job tenure and job performance. Jones, Molitor, and Reif (2019) reach similar conclusion.\n\nThese findings are strikingly different from previous observational studies, which found increase participation in sportive activities, increased job duration, reduced medical spendings.\n\nExample 1.9 (STAR) The Tennessee’s Student Teacher Achievement Ratio (STAR) project (Achilles et al. 2008) is another important example of large scale experiment with broad ramifications. The study suggested that smaller class sizes lead to better outcomes of pupils.\n\nOver 7,000 students in 79 schools were randomly assigned into one of 3 interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher’s aide). Classroom teachers were also randomly assigned to the classes they would teach. The interventions were initiated as the students entered school in kindergarten and continued through third grade.\n\n\n\nExample 1.10 (RAND health care programs) In a large-scale multiyear experiment conducted by the RAND Corporation (Brook et al. 2006), participants who paid for a share of their health care used fewer health services than a comparison group given free care. The study concluded that cost sharing reduced “inappropriate or unnecessary” medical care (overutilization), but also reduced “appropriate or needed” medical care.\n\nThe HIE was a large-scale, randomized experiment conducted between 1971 and 1982. For the study, RAND recruited 2,750 families encompassing more than 7,700 individuals, all of whom were under the age of 65. They were chosen from six sites across the United States to provide a regional and urban/rural balance. Participants were randomly assigned to one of five types of health insurance plans created specifically for the experiment. There were four basic types of fee-for-service plans: One type offered free care; the other three types involved varying levels of cost sharing — 25 percent, 50 percent, or 95 percent coinsurance (the percentage of medical charges that the consumer must pay). The fifth type of health insurance plan was a nonprofit, HMO-style group cooperative. Those assigned to the HMO received their care free of charge. For poorer families in plans that involved cost sharing, the amount of cost sharing was income-adjusted to one of three levels: 5, 10, or 15 percent of income. Out-of-pocket spending was capped at these percentages of income or at $1,000 annually (roughly $3,000 annually if adjusted from 1977 to 2005 levels), whichever was lower.\n\n\nFamilies participated in the experiment for 3–5 years. The upper age limit for adults at the time of enrollment was 61, so that no participants would become eligible for Medicare before the experiment ended. To assess participant service use, costs, and quality of care, RAND served as the families’ insurer and processed their claims. To assess participant health, RAND administered surveys at the beginning and end of the experiment and also conducted comprehensive physical exams. Sixty percent of participants were randomly chosen to receive exams at the beginning of the study, and all received physicals at the end. The random use of physicals at the beginning was intended to control for possible health effects that might be stimulated by the physical exam alone, independent of further participation in the experiment.\n\n\nThere are many other great examples in the dedicated section of Chapter 10 of Telling stories with data by Rohan Alexander (Alexander 2022). Section 1.4 of Berger, Maurer, and Celli (2018) also lists various applications of experimental designs in a variety of fields.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#requirements-for-good-experiments",
    "href": "introduction.html#requirements-for-good-experiments",
    "title": "1  Introduction",
    "section": "1.5 Requirements for good experiments",
    "text": "1.5 Requirements for good experiments\nSection 1.2 of Cox (1958) describes the various requirements that are necessary for experiments to be useful. These are\n\nabsence of systematic error\nprecision\nrange of validity\nsimplicity\n\nWe review each in turn.\n\n1.5.1 Absence of systematic error\nThis point requires careful planning and listing potential confounding variables that could affect the response.\n\nExample 1.11 (Systematic example) Suppose we wish to consider the differences in student performance between two instructors. If the first teaches only morning classes, while the second only teaches in the evening, it will be impossible to disentangle the effect of timing with that of instructor performance. Such comparisons should only be undertaken if there is compelling prior evidence that timing does not impact the outcome of interest.\n\nThe first point raised by Cox is thus that we\n\nensure that experimental units receiving one treatment differ in no systematic way from those receiving another treatment.\n\nThis point also motivates use of double-blind procedures (where both experimenters and participants are unaware of the treatment allocation) and use of placebo in control groups (to avoid psychological effects, etc. associated with receiving treatment or lack thereof).\nRandomization6 is at the core of achieving this goal, and ensuring measurements are independent of one another also comes out as corollary.\n\n\n1.5.2 Variability\nThe second point listed by Cox (1958) is that of the variability of estimator. Much of the precision can be captured by the signal-to-noise ratio, in which the difference in mean treatment is divided by its standard error, a form of effect size. The intuition should be that it’s easier to detect something when the signal is large and the background noise is low. The latter is a function of\n\nthe accuracy of the experimental work and measurements apparatus and the intrinsic variability of the phenomenon under study,\nthe number of experimental and measurement units (the sample size).\nthe choice of design and statistical procedures.\n\nPoint (a) typically cannot be influenced by the experimenter outside of choosing the response variable to obtain more reliable measurements. Point (c) related to the method of analysis, is usually standard unless there are robustness considerations. Point (b) is at the core of the planning, notably in choosing the number of units to use and the allocation of treatment to the different (sub)-units.\n\n\n1.5.3 Generalizability\nMost studies are done with an objective of generalizing the findings beyond the particular units analyzed. The range of validity thus crucially depends with the choice of population from which a sample is drawn and the particular sampling scheme. Non-random sampling severely limits the extrapolation of the results to more general settings. This leads Cox to advocate having\n\nnot just empirical knowledge about what the treatment differences are, but also some understanding of the reasons for the differences.\n\nEven if we believe a factor to have no effect, it may be wise to introduce it in the experiment to check this assumption: if it is not a source of variability, it shouldn’t impact the findings and at the same time would provide some more robustness.\nIf we look at a continuous treatment, than it is probably only safe to draw conclusions within the range of doses administered. Comic in Figure 2.3 is absurd, but makes this point.\n\n\n\n\n\n\n\n\nFigure 1.6: xkcd comic 605 (Extrapolating) by Randall Munroe. Alt text: By the third trimester, there will be thousands of babies inside you. Cartoon reprinted under the CC BY-NC 2.5 license.\n\n\n\n\n\n\nExample 1.12 (Generalizability) Replication studies done in university often draw participants from students enrolled in the institutions. The findings are thus not necessarily robust if extrapolated to the whole population if there are characteristics for which they have strong (familiarity to technology, acquaintance with administrative system, political views, etc). These samples are often convenience samples.\n\n\nExample 1.13 (Spratt-Archer barley in Ireland) Example 1.9 in Cox (1958) mentions recollections of “Student”7 on Spratt-Archer barley, a new variety of barley that performed well in experiments and whose culture the Irish Department of Agriculture encouraged. Fuelled by a district skepticism with the new variety, the Department ran an experiment comparing the yield of the Spratt-Archer barley with that of the native race. Their findings surprised the experimenters: the native barley grew more quickly and was more resistant to weeds, leading to higher yields. It was concluded that the initial experiments were misleading because Spratt-Archer barley had been experimented in well-farmed areas, exempt of nuisance.\n\n\n\n1.5.4 Simplicity\nThe fourth requirement is one of simplicity of design, which almost invariably leads to simplicity of the statistical analysis. Randomized control-trials are often viewed as the golden rule for determining efficacy of policies or treatments because the set of assumptions they make is pretty minimalist due to randomization. Most researchers in management are not necessarily comfortable with advanced statistical techniques and this also minimizes the burden. Figure 1.7 shows an hypothetical graph on the efficacy of the Moderna MRNA vaccine for Covid: if the difference is clearly visible in a suitable experimental setting, then conclusions are easily drawn.\nRandomization justifies the use of the statistical tools we will use under very weak assumptions, if units measurements are independent from one another. Drawing conclusions from observational studies, in contrast to experimental designs, requires making often unrealistic or unverifiable assumptions and the choice of techniques required to handle the lack of randomness is often beyond the toolbox of applied researchers.\n\n\n\n\n\n\n\n\nFigure 1.7: xkcd comic 2400 (Statistics) by Randall Munroe. Alt text: We reject the null hypothesis based on the ‘hot damn, check out this chart’ test. Cartoon reprinted under the CC BY-NC 2.5 license.\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\n\nDefine the following terms in your own word: experimental unit, factor, treatment.\nWhat is the main benefit of experimental studies over observational studies?\n\n\n\n\n\n\n\nAchilles, C. M., Helen Pate Bain, Fred Bellott, Jayne Boyd-Zaharias, Jeremy Finn, John Folger, John Johnston, and Elizabeth Word. 2008. “Tennessee’s Student Teacher Achievement Ratio (STAR) Project.” Harvard Dataverse. https://doi.org/10.7910/DVN/SIWH9F.\n\n\nAguinis, Herman, Isabel Villamor, and Ravi S. Ramani. 2021. “MTurk Research: Review and Recommendations.” Journal of Management 47 (4): 823–37. https://doi.org/10.1177/0149206320969787.\n\n\nAlexander, Rohan. 2022. Telling Stories with Data. CRC Press. https://www.tellingstorieswithdata.com/.\n\n\nBerger, Paul, Robert Maurer, and Giovana B Celli. 2018. Experimental Design with Application in Management, Engineering, and the Sciences. 2nd ed. Springer. https://doi.org/10.1007/978-3-319-64583-4.\n\n\nBrook, Robert H., Emmett B. Keeler, Kathleen N. Lohr, Joseph P. Newhouse, John E. Ware, William H. Rogers, Allyson Ross Davies, et al. 2006. The Health Insurance Experiment: A Classic RAND Study Speaks to the Current Health Care Reform Debate. Santa Monica, CA: RAND Corporation.\n\n\nCard, David, and Alan B. Krueger. 1994. “Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania.” The American Economic Review 84 (4): 772–93. http://www.jstor.org/stable/2118030.\n\n\nCox, David R. 1958. Planning of Experiments. New York, NY: Wiley.\n\n\nHariton, Eduardo, and Joseph J Locascio. 2018. “Randomised Controlled Trials – the Gold Standard for Effectiveness Research.” BJOG: An International Journal of Obstetrics & Gynaecology 125 (13): 1716–16. https://doi.org/https://doi.org/10.1111/1471-0528.15199.\n\n\nJones, Damon, David Molitor, and Julian Reif. 2019. “What Do Workplace Wellness Programs Do? Evidence from the Illinois Workplace Wellness Study.” The Quarterly Journal of Economics 134 (4): 1747–91. https://doi.org/10.1093/qje/qjz023.\n\n\nKohavi, Ron, and Stefan Thomke. 2017. “The Surprising Power of Online Experiments.” Harvard Business Review September–October: 74–82. https://hbr.org/2017/09/the-surprising-power-of-online-experiments.\n\n\nSong, Zirui, and Katherine Baicker. 2019. “Effect of a Workplace Wellness Program on Employee Health and Economic Outcomes: A Randomized Clinical Trial.” JAMA 321 (15): 1491–501. https://doi.org/10.1001/jama.2019.3307.\n\n\nYates, F. 1964. “Sir Ronald Fisher and the Design of Experiments.” Biometrics 20 (2): 307–21. http://www.jstor.org/stable/2528399.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "The preceding paragraph shouldn’t be taken to mean that one cannot get meaningful conclusions from observational studies. Rather, I wish to highlight that controlling for the non-random allocation and potential confounding is a much more complicated task, requires practitioners to make stronger (and sometimes unverifiable) assumptions and requires using a different toolbox (including, but not limited to differences in differences, propensity score weighting, instrumental variables). The book The Effect: An Introduction to Research Design and Causality by Nick Huntington-Klein gives a gentle nontechnical introduction to some of these methods.↩︎\nSince continuous data can take any value in the interval, we can’t talk about the probability of a specific value. Rather, the density curve encodes the probability for any given area: the higher the curve, the more likely the outcome.↩︎\nThe parameters of most commonly used theoretical distributions do not directly relate to the mean and the variance, unlike the normal distribution.↩︎\nA factor is a categorical variable, thus the experimental factor encodes the different groups to compare↩︎\nNote this randomization is different from the one in assigning treatments to experimental units!↩︎\nThe percentage of participants need not be equiprobable, nor do we need to assign the same probability to each participant. However, going away from equal number of people per group has consequences and makes the statistical analysis more complicated.↩︎\nWilliam Sealy Gosset↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html",
    "href": "hypothesis_testing.html",
    "title": "2  Hypothesis testing",
    "section": "",
    "text": "2.1 Hypothesis\nIn most applied domains, empirical evidences drive the advancement of the field and data from well designed experiments contribute to the built up of science. In order to draw conclusions in favour or against a theory, researchers turn (often unwillingly) to statistics to back up their claims. This has led to the prevalence of the use of the null hypothesis statistical testing (NHST) framework. One important aspect of the reproducibility crisis is the misuse of \\(p\\)-values in journal articles: falsification of a null hypothesis is not enough to provide substantive findings for a theory.\nBecause introductory statistics course typically present hypothesis tests without giving much thoughts to the underlying construction principles of such procedures, users often have a reductive view of statistics as a catalogue of pre-determined procedures. To make a culinary analogy, users focus on learning recipes rather than trying to understand the basics of cookery. This chapter focuses on understanding of key ideas related to testing.\nThe first step of a design is formulating a research question. Generally, this hypothesis will specify potential differences between population characteristics due to some intervention (a treatment) that the researcher wants to quantify. This is the step during which researchers decide on sample size, choice of response variable and metric for the measurement, write down the study plan, etc.\nIt is important to note that most research questions cannot be answered by simple tools. Researchers wishing to perform innovative methodological research should contact experts and consult with statisticians before they collect their data to get information on how best to proceed for what they have in mind so as to avoid the risk of making misleading and false claims based on incorrect analysis or data collection.\nFigure 2.1: xkcd comic 2569 (Hypothesis generation) by Randall Munroe. Alt text: Frazzled scientists are requesting that everyone please stop generating hypotheses for a little bit while they work through the backlog. Cartoon reprinted under the CC BY-NC 2.5 license.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#sampling-variability",
    "href": "hypothesis_testing.html#sampling-variability",
    "title": "2  Hypothesis testing",
    "section": "2.2 Sampling variability",
    "text": "2.2 Sampling variability\nGiven data, a researcher will be interested in estimating particular characteristics of the population. We can characterize the set of all potential values their measurements can take, together with their frequency, via a distribution.\nThe purpose of this section is to illustrate how we cannot simply use raw differences between groups to make meaningful comparisons: due to sampling variability, samples will be alike even if they are generated in the same way, but there will be always be differences between their summary statistics. Such differences tend to attenuate (or increase) as we collect more sample. Inherent to this is the fact that as we gather more data (and thus more information) about our target, the portrait becomes more precise. This is ultimately what allows us to draw meaningful conclusions but, in order to do so, we need first to determine what is likely or plausible and could be a stroke of luck, and what is not likely to occur solely due to randomness.\n\nExample 2.1 (A/B testing) Consider two webpage design: one is the current version (status quo) and the other implementation contains a clickable banner in a location where eyetracker suggest that viewers eyes spend more time or attention. The number of clicks on those headlines are what generate longer viewing, and thus higher revenues from advertisement. The characteristic of interest here would be the average click conversation rate for each of the webpage design.\nIt is fairly simple to redirect traffic so that a random fraction gets assigned to the new design for study. After a suitable period of time, the data can be analyzed to see if the new webpage generates more clicks.\n\nAn hypothesis test will focus on one or multiple of these characteristics. Suppose for simplicity that we have only two groups, control and treatment, whose population averages are \\(\\mu_C\\) and \\(\\mu_T\\) we wish to compare. People commonly look at the difference in average, say \\(\\delta=\\mu_T - \\mu_C\\) as a measure of the effectiveness of the treatment.1 If we properly randomized observations in each subgroup and nothing else changes, then this measures the impact of the treatment. Because we only have a sample at hand and not the whole population, we don’t know for sure the values of \\(\\mu_C\\) and \\(\\mu_T\\). These quantities exist, but are unknown to us so the best we can do is estimate them using our sample. If we have a random sample from the population, then the characteristics of the sample will be (noisy) proxys of those of the population.\nWe call numerical summaries of the data statistics. Its important to distinguish between procedures/formulas and their numerical values. An estimator is a rule or formula used to calculate an estimate of some parameter or quantity of interest based on observed data (like a recipe for cake). Once we have observed data we can actually compute the sample mean, that is, we have an estimate — an actual value (the cake), which is a single realization and not random. In other words,\n\nan estimand is our conceptual target, like the population characteristic of interest (population mean).\nan estimator is the procedure or formula telling us how to transform the sample data into a numerical summary that is a proxy of our target.\nan estimate is a number, the numerical value obtained once we apply the formula to observed data.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Estimand\n\n\n\n\n\n\n\n\n\n\n\n(b) Estimator\n\n\n\n\n\n\n\n\n\n\n\n(c) Estimate\n\n\n\n\n\n\n\nFigure 2.2: Estimand (left), estimator (middle) and estimate (right) illustrated with cakes and based on an original idea of Simon Grund. Cake photos shared under CC BY-NC 2.0 license.\n\n\n\nFor example, we may use as estimand the population average of \\(Y_1, \\ldots\\), say \\(\\mu\\). The estimator will be sample mean, i.e., the sum of the elements in the sample divided by the sample size, \\(\\overline{Y}=(Y_1 + \\cdots + Y_n)/n\\). The estimate will be a numerical value, say 4.3.\nBecause the inputs of the estimator are random, the output is also random and change from one sample to the next: even if you repeat a recipe, you won’t get the exact same result every time, as in Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.3: xkcd comic 2581 (Health Stats) by Randall Munroe. Alt text: You will live on forever in our hearts, pushing a little extra blood toward our left hands now and then to give them a squeeze. Cartoon reprinted under the CC BY-NC 2.5 license.\n\n\n\n\n\nTo illustrate this point, Figure 2.4 shows five simple random samples of size \\(n=10\\) drawn from an hypothetical population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), along with their sample mean \\(\\overline{y}\\). Because of the sampling variability, the sample means of the subgroups will differ even if they originate from the same distribution. You can view sampling variability as noise: our goal is to extract the signal (typically differences in means) but accounting for spurious results due to the background noise.\n\n\n\n\n\n\n\n\nFigure 2.4: Five samples of size \\(n=10\\) drawn from a common population with mean \\(\\mu\\) (horizontal line). The colored segments show the sample means of each sample.\n\n\n\n\n\nThe astute eye might even notice that the sample means (thick horizontal segments) are less dispersed around the full black horizontal line representing the population average \\(\\mu\\) than are the individual measurements. This is a fundamental principle of statistics: information accumulates as you get more data.\nValues of the sample mean don’t tell the whole picture and studying differences in mean (between groups, or relative to a postulated reference value) is not enough to draw conclusions. In most settings, there is no guarantee that the sample mean will be equal to it’s true value because it changes from one sample to the next: the only guarantee we have is that it will be on average equal to the population average in repeated samples. Depending on the choice of measurement and variability in the population, there may be considerable differences from one observation to the next and this means the observed difference could be a fluke.\nTo get an idea of how certain something is, we have to consider the variability of an observation \\(Y_i\\). This variance of an observation drawn from the population is typically denoted \\(\\sigma^2\\) and it’s square root, the standard deviation, by \\(\\sigma\\).\nThe sample variance \\(S_n\\) is an estimator of the standard deviation \\(\\sigma\\), where \\[\\begin{align*}\nS^2_n &= \\frac{1}{n-1} \\sum_{i=1}^n (Y_i-\\overline{Y})^2\n\\end{align*}\\] is the sum of squared difference between observations and the sample average, scaled by a factor proportional to the sample size.\nThe standard deviation of a statistic is termed standard error; it should not be confused with the standard deviation \\(\\sigma\\) of the population from which the sample observations \\(Y_1, \\ldots, Y_n\\) are drawn. Both standard deviation and standard error are expressed in the same units as the measurements, so are easier to interpret than variance. Since the standard error is a function of the sample size, it is however good practice to report the estimated standard deviation in reports.\n\nExample 2.2 (Sample proportion and uniform draws) To illustrate the concept of sampling variability, we follow the lead of Matthew Crump and consider samples from a uniform distribution on \\(\\{1, 2, \\ldots, 10\\}\\) each number in this interval is equally likely to be sampled.\n\n\n\n\n\n\n\n\nFigure 2.5: Histograms for 10 random samples of size \\(n=20\\) from a discrete uniform distribution.\n\n\n\n\n\nEven if they are drawn from the same population, the 10 samples in Figure 2.5 look quite different. The only thing at play here is the sample variability: since there are \\(n=20\\) observations in total, there should be on average 10% of the observations in each of the 10 bins, but some bins are empty and others have more counts than expected. This fluctuation is due to randomness, or chance.\nHow can we thus detect whether what we see is compatible with the model we think generated the data? The key is to collect more observations: the bar height is the sample proportion, an average of 0/1 values with ones indicating that the observation is in the bin and zero otherwise.\nConsider now what happens as we increase the sample size: the top panel of Figure 2.6 shows uniform samples for increasing samples size. The histogram looks more and more like the true underlying distribution (flat, each bin with equal frequency) as the sample size increases. The sample distribution of points is nearly indistinguishable from the theoretical one (straight line) when \\(n=10 000\\).2 The bottom panel, on the other hand, isn’t from a uniform distribution and larger samples come closer to the population distribution. We couldn’t have spotted this difference in the first two plots, since the sampling variability is too important; there, the lack of data in some bins could have been attributed to chance, as they are comparable with the graph for data that are truly uniform. This is in line with most practical applications, in which the limited sample size restricts our capacity to disentangle real differences from sampling variability. We must embrace this uncertainty: in the next section, we outline how hypothesis testing helps us disentangle the signal from the noise.\n\n\n\n\n\n\n\n\nFigure 2.6: Histograms of data from a uniform distribution (top) and non-uniform (bottom) with increasing sample sizes of 10, 100, 1000 and 10 000 (from left to right).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#tests",
    "href": "hypothesis_testing.html#tests",
    "title": "2  Hypothesis testing",
    "section": "2.3 Hypothesis testing",
    "text": "2.3 Hypothesis testing\nAn hypothesis test is a binary decision rule (yes/no) used to evaluate the statistical evidence provided by a sample to make a decision regarding the underlying population. The main steps involved are:\n\ndefine the model parameters\nformulate the alternative and null hypothesis\nchoose and calculate the test statistic\nobtain the null distribution describing the behaviour of the test statistic under \\(\\mathscr{H}_0\\)\ncalculate the p-value\nconclude (reject or fail to reject \\(\\mathscr{H}_0\\)) in the context of the problem.\n\nA good analogy for hypothesis tests is a trial for murder on which you are appointed juror.\n\nThe judge lets you choose between two mutually exclusive outcome, guilty or not guilty, based on the evidence presented in court.\nThe presumption of innocence applies and evidences are judged under this optic: are evidence remotely plausible if the person was innocent? The burden of the proof lies with the prosecution to avoid as much as possible judicial errors. The null hypothesis \\(\\mathscr{H}_0\\) is not guilty, whereas the alternative \\(\\mathscr{H}_a\\) is guilty. If there is a reasonable doubt, the verdict of the trial will be not guilty.\nThe test statistic (and the choice of test) represents the summary of the proof. The more overwhelming the evidence, the higher the chance the accused will be declared guilty. The prosecutor chooses the proof so as to best outline this: the choice of evidence (statistic) ultimately will maximize the evidence, which parallels the power of the test.\nThe null distribution is the benchmark against which to judge the evidence (jurisprudence). Given the proof, what are the odds assuming the person is innocent? Since this is possibly different for every test, it is common to report instead a p-value, which gives the level of evidence on a uniform scale which is most easily interpreted.\nThe final step is the verdict, a binary decision with outcomes: guilty or not guilty. For an hypothesis test performed at level \\(\\alpha\\), one would reject (guilty) if the p-value is less than \\(\\alpha\\). Even if we declare the person not guilty, this doesn’t mean the defendant is innocent and vice-versa.\n\n\n2.3.1 Hypothesis\nIn statistical tests we have two hypotheses: the null hypothesis (\\(\\mathscr{H}_0\\)) and the alternative hypothesis (\\(\\mathscr{H}_a\\)). Usually, the null hypothesis (the ‘status quo’) is a single numerical value. The alternative is what we’re really interested in testing. In Figure 2.4, we could consider whether all five groups have the same mean \\(\\mathscr{H}_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_5\\) against the alternative that at least two of them are different. These two outcomes are mutually exclusive and cover all possible scenarios. A statistical hypothesis test allows us to decide whether or not our data provides enough evidence to reject \\(\\mathscr{H}_0\\) in favor of \\(\\mathscr{H}_a\\), subject to some pre-specified risk of error: while we know that the differences are just due to sampling variability in Figure 2.4 because the data is simulated, in practice we need to assess the evidence using a numerical summary.\n\nExample 2.3 (A/B testing (continued)) We follow-up with our A/B test experiment. Given \\(\\mu_1\\) the population average click conversation rate for the current webpage and \\(\\mu_2\\), that of the redesign, we are interested in the one-sided hypothesis that \\(\\mathscr{H}_0: \\mu_2 \\leq \\mu_1\\) against the alternative (that we are trying to prove) \\(\\mathscr{H}_a: \\mu_2 &gt; \\mu_1\\). In choosing as null hypothesis that the new design is no better or worst, we are putting all our weight to make sure the changes carry forward if there is overwhelming evidence that the new design is better and allow us to generate more revenues, given the costs associated to changes to the interface and the resulting disruption.\nOne-sided hypothesis are directional: we care only about a specific direction, and so here \\(\\mathscr{H}_a: \\mu_2 &gt; \\mu_1\\). Indeed, if the experiment suggests that the conversion rate is worst with the new webpage design, we won’t go forward.\nSince neither of these population averages \\(\\mu_1\\) and \\(\\mu_2\\) are known to us, we can work instead with \\(\\mathscr{H}_0: \\mu_2-\\mu_1 \\geq 0\\). We can use as estimator for the difference \\(\\mu_2-\\mu_1\\) the difference in sample average in each subgroup.\nThe null hypothesis here is an interval, but it suffices the consider the most beneficial scenario, which is \\(\\mu_2-\\mu_1=0\\). Indeed, if we can disprove that there is no difference and see an increase of the click rate with the updated version, all more extreme cases are automatically discarded in favour of the alternative that the new design is better.\nOne-sided tests for which the evidence runs contrary to the hypothesis (say the mean conversion rate is higher for the current design than for the new one) lead to p-values of 1, since there is no proof against the null hypothesis that the old design (the status quo) is better.\n\nThe previous example illustrates the fact that, when writing down null and alternative hypotheses, what we are trying to prove is typically the alternative.\nIn pairwise comparisons or contrasts, we can assign a directionality. The benefit is that, if we are sure of the direction of the postulated effect, we only consider as extreme scenarios that run in the direction we postulated3 However, if the empirical evidence runs contrary to our guess, then there is no support for the hypothesis.\nIn more general statistical models, it helps to view the null hypothesis as a simplification of a more complex model: the latter will fit the data better because it is more flexible, but we would fail to reject the null unless this improvement is drastic. For example, in an analysis of variance model, we compare different mean in each of \\(K\\) groups against a single common average.\n\n\n2.3.2 Test statistic\nA test statistic \\(T\\) is a function of the data which takes the data as input and outputs a summary of the information contained in the sample for a characteristic of interest, say the population mean. In order to assess whether the numerical value for \\(T\\) is unusual, we need to know what are the potential values taken by \\(T\\) and their relative probability if \\(\\mathscr{H}_0\\) is true. We need to know what values we should expect if, e.g., there was no difference in the averages of the different groups: this requires a benchmark.\nMany statistics we will consider are of the form4 \\[\\begin{align*}\nT = \\frac{\\text{estimated effect}- \\text{postulated effect}}{\\text{estimated effect variability}} = \\frac{\\widehat{\\theta} - \\theta_0}{\\mathrm{se}(\\widehat{\\theta})}\n\\end{align*}\\] where \\(\\widehat{\\theta}\\) is an estimator of \\(\\theta\\), \\(\\theta_0\\) is the postulated value of the parameter and \\(\\mathrm{se}(\\widehat{\\theta})\\) is the standard error of the test statistic \\(\\widehat{\\theta}\\). This quantity is designed so that, if our postulated value \\(\\theta_0\\) is correct, \\(T\\) has approximately mean zero and variance one. This standardization makes comparison easier; in fact, the form of the test statistic is chosen so that it doesn’t depend on the measurement units.\nFor example, if we are interested in mean differences between treatment group and control group, denoted \\(\\mu_T\\) and \\(\\mu_C\\), then \\(\\theta = \\mu_T-\\mu_C\\) and \\(\\mathscr{H}_0: \\mu_T = \\mu_C\\) corresponds to \\(\\mathscr{H}_0: \\theta = 0\\) for no difference. The two-sample \\(t\\)-test would have numerator \\(\\widehat{\\theta} = \\overline{Y}_T - \\overline{Y}_C\\), where \\(\\overline{Y}_T\\) is the sample average in treatment group and \\(\\overline{Y}_C\\) that of the control group. The postulated value for the mean difference is zero.\nThe numerator would thus consist of the difference in sample means and the denominator the standard error of that quantity, calculated using a software.5\n\n\n2.3.3 Null distribution and p-value\nThe p-value allows us to decide whether the observed value of the test statistic \\(T\\) is plausible under \\(\\mathscr{H}_0\\). Specifically, the p-value is the probability that the test statistic is equal or more extreme to the estimate computed from the data, assuming \\(\\mathscr{H}_0\\) is true. Suppose that based on a random sample \\(Y_1, \\ldots, Y_n\\) we obtain a statistic whose value \\(T=t\\). For a two-sided test \\(\\mathscr{H}_0:\\theta=\\theta_0\\) vs. \\(\\mathscr{H}_a:\\theta \\neq \\theta_0\\), the p-value is \\(\\mathsf{Pr}_0(|T| \\geq |t|)\\).6\nHow do we determine the null distribution given that the true data generating mechanism is unknown to us? We ask a statistician! In simple cases, it might be possible to enumerate all possible outcomes and thus quantity the degree of outlyingness of our observed statistic. In more general settings, we can resort to simulations or to probability theory: the central limit theorem says that the sample mean behaves like a normal random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\) for \\(n\\) large enough. The central limit theorem has broader applications since most statistics can be viewed as some form of average or transformation thereof, a fact used to derive benchmarks for most commonly used tests. Most software use these approximations as proxy by default: the normal, Student’s \\(t\\), \\(\\chi^2\\) and \\(F\\) distributions are the reference distributions that arise the most often.\nFigure 2.7 shows the distribution of \\(p\\)-values for two scenarios: one in which there are no differences and the null is true, the other under an alternative. The probability of rejection is obtained by calculating the area under the density curve between zero and \\(\\alpha=0.1\\), here 0.1 on the left. Under the null, the model is calibrated and the distribution of p-values is uniform (i.e., a flat rectangle of height 1), meaning all values in the unit interval are equally likely. Under the alternative (right), small p-values are more likely to be observed.\n\n\n\n\n\n\n\n\nFigure 2.7: Density of p-values under the null hypothesis (left) and under an alternative with a signal-to-noise ratio of 0.5 (right).\n\n\n\n\n\nThere are generally three ways of obtaining null distributions for assessing the degree of evidence against the null hypothesis\n\nexact calculations\nlarge sample theory (aka ‘asymptotics’ in statistical lingo)\nsimulation\n\nWhile desirable, the first method is only applicable in simple cases (such as counting the probability of getting two six if you throw two fair die). The second method is most commonly used due to its generality and ease of use (particularly in older times where computing power was scarce), but fares poorly with small sample sizes (where ‘too small’ is context and test-dependent). The last approach can be used to approximate the null distribution in many scenarios, but adds a layer of randomness and the extra computations costs sometimes are not worth it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#confidence-intervals",
    "href": "hypothesis_testing.html#confidence-intervals",
    "title": "2  Hypothesis testing",
    "section": "2.4 Confidence intervals",
    "text": "2.4 Confidence intervals\nA confidence interval is an alternative way to present the conclusions of an hypothesis test performed at significance level \\(\\alpha\\) by giving a range of all values for which the null isn’t rejected at the chosen level. It is often combined with a point estimator \\(\\hat{\\theta}\\) to give an indication of the variability of the estimation procedure. Wald-based \\((1-\\alpha)\\) confidence intervals for a scalar parameter \\(\\theta\\) are of the form \\[\\begin{align*}\n\\widehat{\\theta} + \\text{critical value} \\; \\mathrm{se}(\\widehat{\\theta})\n\\end{align*}\\] based on the Wald statistic \\(W\\), \\[\\begin{align*}\nW =\\frac{\\widehat{\\theta}-\\theta}{\\mathrm{se}(\\widehat{\\theta})},\n\\end{align*}\\] and where \\(\\theta\\) represents the postulated value for the fixed, but unknown value of the parameter. The critical values are quantile of the null distribution and are chosen so that the probability of being more extreme is \\(\\alpha\\).\nThe bounds of the confidence intervals are random variables, since both estimators of the parameter and its standard error, \\(\\widehat{\\theta}\\) and \\(\\mathrm{se}(\\widehat{\\theta})\\), are random: their values will vary from one sample to the next.\nFor generic random samples, there is a \\(1-\\alpha\\) probability that \\(\\theta\\) is contained in the random confidence interval computed. Once we obtain a sample and calculate the confidence interval, there is no more notion of probability: the true value of the parameter \\(\\theta\\) is either inside the confidence interval or not. We can interpret confidence interval’s as follows: if we were to repeat the experiment multiple times, and calculate a \\(1-\\alpha\\) confidence interval each time, then roughly \\(1-\\alpha\\) of the calculated confidence intervals would contain the true value of \\(\\theta\\) in repeated samples (in the same way, if you flip a coin, there is roughly a 50-50 chance of getting heads or tails, but any outcome will be either). Our confidence is in the procedure we use to calculate confidence intervals and not in the actual values we obtain from a sample.\n\n\n\n\n\n95% confidence intervals for the mean of a standard normal population for 100 random samples. On average, 5% of these intervals fail to include the true mean value of zero (in red).\n\n\n\n\nIf we are only interested in the binary decision rule reject/fail to reject \\(\\mathscr{H}_0\\), the confidence interval is equivalent to a p-value since it leads to the same conclusion. Whereas the \\(1-\\alpha\\) confidence interval gives the set of all values for which the test statistic doesn’t provide enough evidence to reject \\(\\mathscr{H}_0\\) at level \\(\\alpha\\), the p-value gives the probability under the null of obtaining a result more extreme than the postulated value and so is more precise for this particular value. If the p-value is smaller than \\(\\alpha\\), our null value \\(\\theta\\) will be outside of the confidence interval and vice-versa.\n\n2.4.1 Conclusion\nThe p-value allows us to make a decision about the null hypothesis. If \\(\\mathscr{H}_0\\) is true, the p-value follows a uniform distribution, as shown in Figure 2.7. Thus, if the p-value is small, this means observing an outcome more extreme than \\(T=t\\) is unlikely, and so we’re inclined to think that \\(\\mathscr{H}_0\\) is not true. There’s always some underlying risk that we’re making a mistake when we make a decision. In statistic, there are two type of errors:\n\ntype I error: we reject the null hypothesis \\(\\mathscr{H}_0\\) when the null is true,\ntype II error: we fail to reject the null hypothesis \\(\\mathscr{H}_0\\) when the alternative is true.\n\nThe two hypothesis are not judged equally: we seek to avoid error of type I (judicial errors, corresponding to condamning an innocent). To prevent this, we fix a the level of the test, \\(\\alpha\\), which captures our tolerance to the risk of commiting a type I error: the higher the level of the test \\(\\alpha\\), the more often we will reject the null hypothesis when the latter is true. The value of \\(\\alpha \\in (0, 1)\\) is the probability of rejecting \\(\\mathscr{H}_0\\) when \\(\\mathscr{H}_0\\) is in fact true, \\[\\begin{align*}\n\\alpha = \\mathsf{Pr}_0\\left(\\text{ reject } \\mathscr{H}_0\\right).\n\\end{align*}\\] The level \\(\\alpha\\) is fixed beforehand, typically \\(1\\)%, \\(5\\)% or \\(10\\)%. Keep in mind that the probability of type I error is \\(\\alpha\\) only if the null model for \\(\\mathscr{H}_0\\) is correct (sic) and correspond to the data generating mechanism.\nThe focus on type I error is best understood by thinking about costs of moving away from the status quo: a new website design or branding will be costly to implement, so you want to make sure there are enough evidence that the proposal is the better alternative and will lead to increased traffic or revenues.\n\n\n\nDecision \\ true model\n\\(\\mathscr{H}_0\\)\n\\(\\mathscr{H}_a\\)\n\n\n\n\nfail to reject \\(\\mathscr{H}_0\\)\n\\(\\checkmark\\)\ntype II error\n\n\nreject \\(\\mathscr{H}_0\\)\ntype I error\n\\(\\checkmark\\)\n\n\n\nTo make a decision, we compare our p-value \\(P\\) with the level of the test \\(\\alpha\\):\n\nif \\(P &lt; \\alpha\\), we reject \\(\\mathscr{H}_0\\);\nif \\(P \\geq \\alpha\\), we fail to reject \\(\\mathscr{H}_0\\).\n\nDo not mix up level of the test (a probability fixed beforehand by the researcher) and the p-value. If you do a test at level 5%, the probability of type I error (condemning an innocent by mistake) is by definition \\(\\alpha\\) and does not depend on the p-value. The latter is a conditional probability of observing a more extreme statistic given the null distribution \\(\\mathscr{H}_0\\) is true.\n\n\n\n\n\n\nPitfall\n\n\n\nThe American Statistical Association (ASA) published a list of principles guiding (mis)interpretation of p-values, some of which are reproduced below:\n\n\nP-values do not measure the probability that the studied hypothesis is true.\n\n\n\n\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n\n\n\n\nP-values and related analyses should not be reported selectively.\n\n\n\n\np-value, or statistical significance, does not measure the size of an effect or the importance of a result.\n\n\n\n\n\n\n2.4.2 Examples\n\nExample 2.4 (Gender inequality and permutation tests) We consider data from Rosen and Jerdee (1974), who look at sex role stereotypes and their impacts on promotion and opportunities for women candidates. The experiment took place in 1972 and the experimental units, which consisted of 95 male bank supervisors, were submitted to various memorandums and asked to provide ratings or decisions based on the information provided.\nWe are interested in Experiment 1 related to promotion of employees: managers were requested to decide on whether or not to promote an employee to become branch manager based on recommendations and ratings on potential for customer and employee relations.\nThe authors intervention focused on the description of the nature (complexity) of the manager’s job (either simple or complex) and the sex of the candidate (male or female): all files were otherwise similar.\nWe consider for simplicity only sex as a factor and aggregate over job for the \\(n=93\\) replies. Table 2.1 shows the counts for each possibility.\n\n\n\n\nTable 2.1: Promotion recommandation to branch manager based on sex of the applicant.\n\n\n\n\n\n\n\nmale\nfemale\n\n\n\n\npromote\n32\n19\n\n\nhold file\n12\n30\n\n\n\n\n\n\n\n\n\n\nThe null hypothesis of interest here that sex has no impact, so the probability of promotion is the same for men and women. Let \\(p_{\\text{m}}\\) and \\(p_{\\text{w}}\\) denote these respective probabilities; we can thus write mathematically the null hypothesis as \\(\\mathscr{H}_0: p_{\\text{m}} = p_{\\text{w}}\\) against the alternative \\(\\mathscr{H}_a: p_{\\text{m}} \\neq p_{\\text{w}}\\).\nThe test statistic typically employed for contingency tables is a chi-square test7, which compares the overall proportions of promoted to that in for each subgroup. The sample proportion for male is 32/42 = ~76%, compared to 19/49 or ~49% for female. While it seems that this difference of 16% is large, it could be spurious: the standard error for the sample proportions is roughly 3.2% for male and 3.4% for female.\nIf there was no discrimination based on sex, we would expect the proportion of people promoted to be the same overall; this is 51/93 =0.55 for the pooled sample. We could simply do a test for the mean difference, but rely instead on the Pearson contingency \\(X^2_p\\) (aka chi-square) test, which compares the expected counts (based on equal promotion rates) to the observed counts, suitably standardized. If the discrepancy is large between expected and observed, than this casts doubt on the validity of the null hypothesis.\nIf the counts of each cell are large, the null distribution of the chi-square test is well approximated by a \\(\\chi^2\\) distribution. The output of the test includes the value of the statistic, \\(10.79\\), the degrees of freedom of the \\(\\chi^2\\) approximation and the p-value, which gives the probability that a random draw from a \\(\\chi^2_1\\) distribution is larger than the observed test statistic assuming the null hypothesis is true. The p-value is very small, \\(0.001\\), which means such a result is quite unlikely to happen by chance if there was no sex-discrimination.\n\nAnother alternative to obtain a benchmark to assess the outlyingness of the observed odds ratio is to use simulations: permutation tests are well illustrated by Jared Wilber. Consider a database containing the raw data with 93 rows, one for each manager, with for each an indicator of action and the sex of the hypothetical employee presented in the task.\n\n\n\n\nTable 2.2: First five rows of the database in long format for experiment 1 of Rosen and Jerdee.\n\n\n\n\n\n\naction\nsex\n\n\n\n\npromote\nmale\n\n\nhold file\nfemale\n\n\npromote\nmale\n\n\nhold file\nfemale\n\n\nhold file\nmale\n\n\n\n\n\n\n\n\n\n\nUnder the null hypothesis, sex has no incidence on the action of the manager. This means we could get an idea of the “what-if” world by shuffling the sex labels repeatedly. Thus, we could obtain a benchmark by repeating the following steps multiple times:\n\npermute the labels for sex,\nrecreate a contingency table by aggregating counts,\ncalculate a test statistic for the simulated table.\n\nAs test statistic, we use odds ratio: the odds of an event is the ratio of the number of success over failure: in our example, this would be the number of promoted over held files. The odds of promotion for male is \\(32/12\\), whereas that of female is \\(19/30\\). The odds ratio for male versus female is thus \\(\\mathsf{OR}=(32/12) / (19/30)= 4.21\\). Under the null hypothesis, \\(\\mathscr{H}_0: \\mathsf{OR}= 1\\) (same probability of being promoted) (why?)\n\n\n\n\n\n\n\n\nFigure 2.8: Histogram of the simulated null distribution of the odds ratio statistic obtained using a permutation test; the vertical red line indicates the sample odds ratio.\n\n\n\n\n\nThe histogram in Figure 2.8 shows the distribution of the odds ratio based on 10 000 permutations. Reassuringly, we again get roughly the same approximate p-value, here 0.002.8\nThe article concluded (in light of the above and further experiments)\n\nResults confirmed the hypothesis that male administrators tend to discriminate against female employees in personnel decisions involving promotion, development, and supervision.\n\nRecap\n\nModel parameters: probability of promotion for men and women, respectively \\(p_{\\text{m}}\\) and \\(p_{\\text{w}}\\).\nHypotheses: no discrimination based on gender, meaning equal probability of promotion (null hypothesis \\(\\mathscr{H}_0: p_{\\text{m}}=p_{\\text{w}}\\), versus alternative hypothesis \\(\\mathscr{H}_a: p_{\\text{m}}\\neq p_{\\text{w}}\\)).\nTest statistic: (1) chi-square test for contingency tables and (2) odds ratio.\n\\(p\\)-value: (1) \\(.0010\\) and (2)  \\(.0024\\) based on permutation test.\nConclusion: reject null hypothesis, as there is evidence of a gender-discrimination with different probability of promotion for men and women.\n\nFollowing the APA guidelines, the \\(\\chi^2\\) statistic would be reported as \\(\\chi^2(1, n = 93) = 10.79\\), \\(p = .001\\) along with counts and sample proportions.\n\n\n\n\n\n\n\nPitfall\n\n\n\nIn the first experiment, managers were also asked to rank applications on their potential for both employee and customer relations using a Likert scale of six items ranging from (1) extremely unfavorable to (6) extremely favorable. However, only the averages are reported in Table 1 along with (Rosen and Jerdee 1974)\n\nMean rating for the male candidate was 4.73 compared to a mean rating of 4.25 for the female candidate (\\(F=4.76\\), \\(\\text{df} = 1/80\\), \\(p &lt; .05\\))\n\nThe degrees of freedom (80) are much too few compared to the number of observations, implying non-response that isn’t discussed.\nPartial or selective reporting of statistical procedures hinders reproducibility. In general, the presentation should explicitly state the name of the test statistic employed, the sample size, mean and variance estimates, the null distribution used to assess significance and its parameters, if any. Without these, we are left to speculate.\n\n\n\nExample 2.5 (“The Surprise of Reaching Out”) Liu et al. (2023) studies social interactions and the impact of surprise on people reaching out if this contact is unexpected. Experiment 1 focuses on questionnaires where the experimental condition is the perceived appreciation of reaching out to someone (vs being reached to). The study used a questionnaire administered to 200 American adults recruited on the Prolific Academic platform. The response index consists of the average of four questions measured on a Likert scale ranging from 1 to 7, with higher values indicating higher appreciation.\n\nWe can begin by inspecting summary statistics for the sociodemographic variables (gender and age) to assess whether the sample is representative of the general population as a whole. The proportion of other (including non-binary people) is much higher than that of the general census, and the population skews quite young according to Table 2.3.\n\n\n\n\nTable 2.3: Summary statistics of the age of participants, and counts per gender\n\n\n\n\n\n\ngender\nmin\nmax\nmean\nn\n\n\n\n\nmale\n18\n78\n32.03\n105\n\n\nfemale\n19\n68\n36.50\n92\n\n\nother\n24\n30\n27.67\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4: Mean ratings, standard deviation and number of participants per experimental condition.\n\n\n\n\n\n\nrole\nmean\nsd\nn\n\n\n\n\ninitiator\n5.50\n1.28\n103\n\n\nresponder\n5.87\n1.27\n97\n\n\n\n\n\n\n\n\n\n\nSince there are only two groups, initiator and responder, we are dealing with a pairwise comparison. The logical test one could use is a two sample t-test, or a variant thereof. Using Welch two sample \\(t\\)-test statistic, both group average and standard deviation are estimated using the data provided and the latter are used to build a statistic. This explains the non-integer degrees of freedom.\nThe software returns \\(t(197.52) = -2.05\\), \\(p = .041\\), which leads to the rejection of the null hypothesis of no difference in appreciation depending on the role of the individual (initiator or responder). The estimated mean difference is \\(\\Delta M = -0.37\\), 95% CI \\([-0.73, -0.01]\\); since \\(0\\) is not included in the confidence interval, we also reject the null hypothesis at level 5%. The estimate suggests that initiators underestimate the appreciation of reaching out.9\nRecap\n\nModel parameters: average expected appreciation score \\(\\mu_{\\mathrm{i}}\\) and \\(\\mu_{\\mathrm{r}}\\) of initiators and responder, respectively\nHypothesis: expected appreciation score is the same for initiator and responders, \\(\\mathscr{H}_0: \\mu_{\\mathrm{i}}=\\mu_{\\mathrm{r}}\\) against alternative \\(\\mathscr{H}_0: \\mu_{\\mathrm{i}} \\neq \\mu_{\\mathrm{r}}\\) that they are different.\nTest statistic: Welch two sample \\(t\\)-test\n\\(p\\)-value: 0.041\nConclusion: reject the null hypothesis, average appreciation score differs depending on the role\n\n\nExample 2.6 (Virtual communication curbs creative idea generation) A Nature study performed an experiment to see how virtual communications teamwork by comparing the output both in terms of ideas generated during a brainstorming session by pairs and of the quality of ideas, as measured by external referees. The sample consisted of 301 pairs of participants who interacted via either videoconference or face-to-face.\nThe authors compared the number of creative ideas, a subset of the ideas generated with creativity score above average. The mean number of the number of creative ideas for face-to-face \\(7.92\\) ideas (sd \\(3.40\\)) relative to videoconferencing \\(6.73\\) ideas (sd \\(3.27\\)).\nBrucks and Levav (2022) used a negative binomial regression model: in their model, the expected number creative ideas generated is \\[\\begin{align*}\n\\mathsf{E}(\\texttt{ncreative}) = \\exp(\\beta_0 + \\beta_1 \\texttt{video})\n\\end{align*}\\] where \\(\\texttt{video}=0\\) if the pair are in the same room and \\(\\texttt{video}=1\\) if they interact instead via videoconferencing.\nThe mean number of ideas for videoconferencing is thus \\(\\exp(\\beta_1)\\) times that of the face-to-face: the estimate of the multiplicative factor is \\(\\exp(\\beta_1)\\) is \\(0.85\\) 95% CI \\([0.77, 0.94]\\).\nNo difference between experimental conditions translates into the null hypothesis as \\(\\mathscr{H}_0: \\beta_1=0\\) vs \\(\\mathscr{H}_0: \\beta_1 \\neq 0\\) or equivalently \\(\\mathscr{H}_0: \\exp(\\beta_1)=1\\). The likelihood ratio test comparing the regression model with and without \\(\\texttt{video}\\) the statistic is \\(R=9.89\\) (\\(p\\)-value based on \\(\\chi^2_1\\) of \\(.002\\)). We conclude the average number of ideas is different, with summary statistics suggesting that virtual pairs generate fewer ideas.\nIf we had resorted to a two sample \\(t\\)-test, we would have found a mean difference in number of creative idea of \\(\\Delta M = 1.19\\), 95% CI \\([0.43, 1.95]\\), \\(t(299) = 3.09\\), \\(p = .002\\).\nBoth tests come with slightly different sets of assumptions, but yield similar conclusions: there is evidence of a smaller number of creative ideas when people interact via videoconferencing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#conclusion-1",
    "href": "hypothesis_testing.html#conclusion-1",
    "title": "2  Hypothesis testing",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nThis chapter has focused on presenting the tools of the trade and some examples outlining the key ingredients that are common to any statistical procedure and the reporting of the latter. The reader is not expected to know which test statistic to adopt, but rather should understand at this stage how our ability to do (scientific) discoveries depends on a number of factors.\nRichard McElreath in the first chapter of his book (McElreath 2020) draws a parallel between statistical tests and golems (i.e., robots): neither\n\ndiscern when the context is inapropriate for its answers. It just knows its own procedure […] It just does as it’s told.\n\nThe responsibility therefore lies with the user to correctly use statistical procedures and be aware of their limitations. A p-value does not indicate whether the hypothesis is reasonable, whether the design is proper, whether the choice of measurement is adequate, etc.\n\n\n\n\n\n\nYour turn\n\n\n\nPick a journal paper (e.g., one of the dataset documented in the course webpage) and a particular study.\nLook up for the ingredients of the testing procedure (parameters, hypotheses, test statistic name and value, summary statistics, p-value, conclusion).\nYou may encounter other measures, such as effect size, that will be discussed later.\n\n\n\n\n\n\nBrucks, Melanie S., and Jonathan Levav. 2022. “Virtual Communication Curbs Creative Idea Generation.” Nature 605 (7908): 108–12. https://doi.org/10.1038/s41586-022-04643-y.\n\n\nLiu, Peggy J., SoYon Rim, Lauren Min, and Kate E. Min. 2023. “The Surprise of Reaching Out: Appreciated More Than We Think.” Journal of Personality and Social Psychology 124 (4): 754–71. https://doi.org/10.1037/pspi0000402.\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed. CRC Press. https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919.\n\n\nRosen, B., and T. H. Jerdee. 1974. “Influence of Sex Role Stereotypes on Personnel Decisions.” Journal of Applied Psychology 59: 9–14.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#footnotes",
    "href": "hypothesis_testing.html#footnotes",
    "title": "2  Hypothesis testing",
    "section": "",
    "text": "We could look at the ratio \\(\\mu_T/\\mu_C\\) instead.↩︎\nThe formula shows that the standard error decreases by a tenfold every time the sample size increases by a factor 100.↩︎\nThis implies that the level \\(\\alpha\\) is all on one side, rather than split equally between both tails of the distribution. In practice, this translates into increased power of detection provided the effect is in the postulated direction.↩︎\nThis class of statistic, which includes \\(t\\)-tests, are called Wald statistics.↩︎\nAssuming equal variance, the denominator is estimated using the pooled variance estimator.↩︎\nIf the distribution of \\(T\\) is symmetric around zero, the p-value reduces to \\(p = 2 \\times \\mathsf{Pr}_0(T \\geq |t|).\\)↩︎\nIf you have taken advanced modelling courses, this is a score test obtained by fitting a Poisson regression with sex and action as covariates; the null hypothesis corresponding to lack of interaction term between the two.↩︎\nThe p-value obtained for the permutation test would change from one run to the next since it’s input is random. However, the precision of the proportion statistic is sufficient for decision making purposes.↩︎\nAssuming that the variance of each subgroup were equal, we could have used a two-sample \\(t\\)-test instead. The difference in the conclusion is immaterial, with a nearly equal p-value.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "completely_randomized_trials.html",
    "href": "completely_randomized_trials.html",
    "title": "3  Completely randomized designs",
    "section": "",
    "text": "3.1 One-way analysis of variance\nThis chapter focuses on experiments where potentially multiple factors of interest are manipulated by the experimenter to study their impact. If the allocation of measurement units to each treatment combination is completely random, the resulting experiment is a completely randomized design.\nThe one-way analysis of variance describes the most simple experimental setup one can consider: completely randomized experiments with one factor, in which we are solely interested in the effect of a single treatment variable with multiple levels.\nThe focus is on comparisons of the average of a single outcome variable with \\(K\\) different treatments levels, each defining a sub-population differing only in the experimental condition they received. A one-way analysis of variance compares the sample averages of each treatment group \\(T_1, \\ldots, T_K\\) to try and determine if the population averages could be the same. Since we have \\(K\\) groups, there will be \\(K\\) averages (one per group) to estimate.\nLet \\(\\mu_1, \\ldots, \\mu_K\\) denote the theoretical (unknown) mean (aka expectation) of each of the \\(K\\) sub-populations defined by the different treatments. Lack of difference between treatments is equivalent to equality of means, which translates into the hypotheses \\[\\begin{align*}\n\\mathscr{H}_0: & \\mu_1 = \\cdots = \\mu_K \\\\\n\\mathscr{H}_a: & \\text{at least two treatments have different averages, }\n\\end{align*}\\] The null hypothesis is, as usual, a single numerical value, \\(\\mu\\). The alternative consists of all potential scenarios for which not all expectations are equal. Going from \\(K\\) averages to one requires imposing \\(K-1\\) restrictions (the number of equality signs), as the value of the global mean \\(\\mu\\) is left unspecified.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Completely randomized designs</span>"
    ]
  },
  {
    "objectID": "completely_randomized_trials.html#one-way-analysis-of-variance",
    "href": "completely_randomized_trials.html#one-way-analysis-of-variance",
    "title": "3  Completely randomized designs",
    "section": "",
    "text": "3.1.1 Parametrizations and contrasts\nThis section can be skipped on first reading. It focuses on the interpretation of the coefficients obtained from a linear model or analysis of variance model.\nThe most natural parametrization is in terms of group averages: the (theoretical unknown) average for treatment \\(T_j\\) is \\(\\mu_j\\), so we obtain \\(K\\) parameters \\(\\mu_1, \\ldots, \\mu_K\\) whose estimates are the sample averages \\(\\widehat{\\mu}_1, \\ldots, \\widehat{\\mu}_K\\). One slight complication arising from the above is that the values of the population average are unknown, so this formulation is ill-suited for hypothesis testing because none of the \\(\\mu_i\\) values are known in practice and we need to make comparisons in terms of a known numerical value.\nThe most common parametrization for the linear model is in terms of differences to a baseline, say \\(T_1\\). The theoretical average of each group is written as \\(\\mu_1 + a_i\\) for treatment \\(T_i\\), where \\(a_1=0\\) for \\(T_1\\) and \\(a_i = \\mu_i-\\mu_1\\) otherwise. The parameters are \\(\\mu_1, a_2, \\ldots, a_K\\).\nAn equivalent formulation writes for each treatment group the average of subpopulation \\(j\\) as \\(\\mu_j = \\mu + \\delta_j\\), where \\(\\delta_j\\) is the difference between the treatment average \\(\\mu_j\\) and the global average of all groups. Imposing the constraint \\(\\delta_1 + \\cdots + \\delta_K=0\\) ensures that the average of effects equals \\(\\mu\\). Thus, if we know any \\(K-1\\) of \\(\\{\\delta_1, \\ldots, \\delta_K\\}\\), we automatically can deduce the last one.\n\nExample 3.1 (Impact of encouragement on teaching) In R, the lm function fits a linear model based on a formula of the form response ~ explanatory. If the explanatory is categorical (i.e., a factor), the parameters of this model are the intercept, which is the sample average of the baseline group and the other parameters are simply contrasts, i.e., the \\(a_i\\)’s.\nThe sum-to-zero parametrization is obtained with contrasts = list(... = contr.sum), where the ellipsis is replaced by the name of the categorical variable; an easier alternative is aov, which enforces this parametrization by default. With the sum-to-zero parametrization, the intercept is the average of each treatment average, \\((\\widehat{\\mu}_1 + \\cdots + \\widehat{\\mu}_5)/5\\); this need not coincide with the (overall) mean of the response \\(\\widehat{\\mu} = \\overline{y}\\) unless the sample the number of observations in each group is the same.1 The other coefficients of the sum-to-zero parametrization are the differences between this intercept and the group means.\nWe show the function call to fit a one-way ANOVA in the different parametrizations along with the sample average of each arithmetic group (the two controls who were taught separately and the groups that were praised, reproved and ignored in the third class). Note that the omitted category changes depending on the parametrization.\n\nmod_contrast &lt;- lm(score ~ group, \n                   data = arithmetic)\nmod_sum2zero &lt;- lm(score ~ group, \n                   data = arithmetic,\n                   contrasts = list(group = contr.sum))\n\n\n\n\n\nTable 3.1: Coefficients of the analysis of variance model for the arithmetic scores using different parametrizations.\n\n\n\n\n\n\ngroup\nmean\ncontrasts\nsum-to-zero\n\n\n\n\nintercept\n\n19.67\n21.00\n\n\ncontrol 1\n19.67\n\n-1.33\n\n\ncontrol 2\n18.33\n-1.33\n-2.67\n\n\npraise\n27.44\n7.78\n6.44\n\n\nreprove\n23.44\n3.78\n2.44\n\n\nignore\n16.11\n-3.56\n\n\n\n\n\n\n\n\n\n\nWe can still assess the hypothesis by comparing the sample means in each group, which are noisy estimates of the population mean: their inherent variability will limit our ability to detect differences in averages if the signal-to-noise ratio is small.\n\n\n3.1.2 Sum of squares decomposition\nThe following section can be safely skipped on first reading: it attempts to shed some light into how the \\(F\\)-test statistic works as a summary of evidence, as it isn’t straightforward in the way it appears.\nThe usual notation for the sum of squares decomposition is as follows: suppose \\(y_{ik}\\) represents the \\(i\\)th person in the \\(k\\)th treatment group (\\(k=1, \\ldots, K\\)) and the sample size \\(n\\) can be split between groups as \\(n_1, \\ldots, n_K\\); in the case of a balanced sample, \\(n_1=\\cdots=n_K = n/K\\) and the number of observations in each group is the same. We denote by \\(\\widehat{\\mu}_k\\) the sample average in group \\(k\\) and \\(\\widehat{\\mu}\\) the overall average \\((y_{11} + \\cdots + y_{n_KK})/n = \\sum_k  \\sum_i y_{ik}/n\\), where \\(\\sum_i\\) denotes the sum over all individuals in the group.\nUnder the null model, all groups have the same mean, so the natural estimator for the latter is the sample average of the pooled sample \\(\\widehat{\\mu}\\) and likewise the group averages \\(\\widehat{\\mu}_1, \\ldots, \\widehat{\\mu}_K\\) are the best estimators for the group averages if each group has a (potentially) different mean. The more complex model, which has more parameters, will always fit better because it has more possibility to accommodate differences observed in a group, even if these are spurious. The sum of squares measures the (squared) distance between the observation and the fitted values, with the terminology total, within and between sum of squares linked to the decomposition \\[\\begin{align*}\n\\underset{\\text{total sum of squares} }{\\sum_{i}\\sum_{k} (y_{ik} - \\widehat{\\mu})^2} &= \\underset{\\text{within sum of squares} }{\\sum_i \\sum_k (y_{ik} - \\widehat{\\mu}_k)^2} +  \\underset{\\text{between sum of squares} }{\\sum_k n_i (\\widehat{\\mu}_k - \\widehat{\\mu})^2}.\n\\end{align*}\\] The term on the left is a measure of the variability for the null model \\((\\mu_1 = \\cdots = \\mu_K)\\) under which all observations are predicted by the overall average \\(\\widehat{\\mu}\\). The within sum of squares measures the distance between observations and their group mean, which describes the alternative model in which each group has (potentially) a different average, but the same variability.\nWe can measure how much worst we do with the alternative model (different average per group) relative to the null by calculating the between sum of square. This quantity in itself varies with the sample size (the more observations, the larger it is) so we must standardize as usual this quantity so that we have a suitable benchmark.\nThe \\(F\\)-statistic is\n\\[\n\\begin{split}\nF &= \\frac{\\text{between-group variability}}{\\text{within-group variability}} \\\\\n&= \\frac{\\text{between sum of squares}/(K-1)}{\\text{within sum of squares}/(n-K)}\n\\end{split}\n\\tag{3.1}\\]\nIf there is no mean difference (null hypothesis), the numerator is an estimator of the population variance, and so is the denominator of eq. \\(\\ref{eq-Fstatheuristic}\\) and the ratio of the two is approximately 1 on average. However, the between sum of square is more variable and this induces skewness: for large enough sample, the null distribution of the F-statistic is approximately an F-distribution, whose shape is governed by two parameters named degrees of freedom which appear in Equation 3.1 as scaling factors to ensure proper standardization. The first degree of freedom is the number of restrictions imposed by the null hypothesis (\\(K-1\\), the number of groups minus one for the one-way analysis of variance), and the second degree of freedom is the number of observations minus the number of parameters estimates for the mean (\\(n-K\\), where \\(n\\) is the overall sample size and \\(K\\) is the number of groups).2\nFigure 3.1 shows how the difference between these distances can encompass information that the null is wrong. The sum of squares is obtained by computing the squared length of these vectors and adding them up. The left panel shows strong signal-to-noise ratio, so that, on average, the black segments are much longer than the colored ones. This indicates that the model obtained by letting each group have its own mean is much better than the other. The picture in the right panel is not as clear: on average, the colored arrows are shorter, but the difference in length is much smaller relative to the colored arrows.\n\n\n\n\n\n\n\n\nFigure 3.1: Observations drawn from three groups from a model with a strong (left) and weak (right) signal-to-noise ratio, along with their sample mean (colored horizontal segments) and the overall average (horizontal line). Arrows indicate the magnitude of the difference between the observation and the (group/average) mean.\n\n\n\n\n\nThe \\(F\\)-distribution is what we call a large sample approximation to the behaviour of the statistic if there is truly no difference between group averages (and if model assumptions are satisfied): it tells us what to expect if there is nothing going on. The quality of the approximation depends on the sample size in each group: it is more accurate when there are more observations in each group, as average estimation becomes more reliable3.\nAs was alluded to in the last chapter, large sample approximations are not the only option for assessing the null, but they are cheap and easy to obtain. If the distributions are the same under the null and alternative except for a location shift, we could instead resort to a permutation-based approach to generate those alternative samples by simply shuffling the labels. We see in Figure 3.2 that the histogram of the \\(F\\)-statistic values obtained from 1000 permutations closely matches that of the large-sample \\(F\\)-distribution when there are on average 20 observations per group (right), so the computational burden associated with running this simulation outweights the benefits. However, with smaller samples (left), the large sample approximation appears underdispersed relative to the permutation-based distribution, with more extreme outcomes; the latter should be viewed as more accurate in this setting.\n\n\n\n\n\n\n\n\nFigure 3.2: One-way analysis of variance for a sample of size 20 (left) and 100 (right), split in five groups. The histograms shows the computed test values based on 1000 permutations, which are compared to the density of the large-sample F-distribution.\n\n\n\n\n\nMore interestingly perhaps is what happens to the values taken by the statistic when not all of the averages are the same. We can see in Figure 3.3 that, when there are some difference between group means, the values taken by the statistic for a random sample are more to the right than the null distribution: the larger those differences, the more the curve will shift to the right and the more often we will obtain a value in the rejection region (in red).\nIf there are only two groups, then one can show that the \\(F\\)-statistic is mathematically equivalent to squaring the \\(t\\)-statistic: the null distributions are \\(\\mathsf{St}(n-K)\\) and \\(\\mathsf{F}(1, n-K)\\) and lead to the same \\(p\\)-values and thus same statistical inference and conclusions.\n\n\n\n\n\n\n\n\nFigure 3.3: Distribution of the \\(F\\)-test statistic for the one-way analysis of variance when the true group means are equal (top) and under a specific alternative when they are not (bottom). Any value falling within the red region leads to rejection of the null hypothesis at level \\(\\alpha=0.05\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Completely randomized designs</span>"
    ]
  },
  {
    "objectID": "completely_randomized_trials.html#graphical-representation",
    "href": "completely_randomized_trials.html#graphical-representation",
    "title": "3  Completely randomized designs",
    "section": "3.2 Graphical representation",
    "text": "3.2 Graphical representation\nHow to represent data for a one-way analysis in a publication? The purpose of the visualization is to provide intuition that extends beyond the reported descriptive statistics and to check the model assumptions. Most of the time, we will be interested in averages and dispersion, but plotting the raw data can be insightful. It is also important to keep in mind that summary statistics are estimators of population quantities that are perhaps unreliable (much too variable) in small samples to be meaningful quantities. Since the mean estimates will likely be reported in the text, the graphics should be used to convey additional information about the data. If the samples are extremely large, then graphics will be typically be used to present salient features of the distributions.\n\n\n\n\n\n\n\n\nFigure 3.4: Two graphical representations of the arithmetic data: dynamite plot (left) showing the sample average with one standard error above and below, and dot plot with the sample mean (right).\n\n\n\n\n\nIn a one-way analysis of variance, the outcome is a continuous numerical variable, whereas the treatment or explanatory is a categorical variable. Basic graphics include dot plots, histograms and density plots, or rugs for the raw data.\nTypically, scatterplots are not a good option because observations get overlaid. There are multiple workarounds, involving transparency, bubble plots for discrete data with ties, adding noise (jitter) to every observation or drawing values using a thin line (rugs) if the data are continuous and take on few distinct values.\nJournals are plagued with poor vizualisations, a prime example of which is the infamous dynamite plot: it consists of a bar plot with one standard error interval. The problem with this (or with other summary statistics) is that they hide precious information about the spread and values taken by the data, as many different data could give rise to the same average while being quite different in nature. The height of the bar is the sample average and the bars extend beyond one standard error: this makes little sense as we end up comparing areas, whereas the mean is a single number. The right panel of Figure 3.4 shows instead a dot plot for the data, i.e., sample values with ties stacked for clarity, along with the sample average and a 95% confidence interval for the latter as a line underneath. In this example, there are not enough observations per group to produce histograms, and a five number summary of nine observations isn’t really necessary so boxplot are useless. Weissgerber et al. (2015) discusses alternative solutions and can be referenced when fighting reviewers who insist on bad visualizations.\nIf we have a lot of data, it sometimes help to represent selected summary statistics or group data. A box-and-whiskers plot (or boxplot) is a commonly used graphic representing the whole data distribution using five numbers\n\nThe box gives the quartiles, say \\(q_1\\), \\(q_2\\) (median) and \\(q_3\\) of the distribution: 50% of the observations are smaller or larger than \\(q_2\\), 25% are smaller than \\(q_1\\) and 75% are smaller than \\(q_3\\) for the sample.\nThe whiskers extend up to \\(1.5\\) times the box width (\\(q_3-q_1\\)) (so the largest observation that is smaller than \\(q_3+1.5(q_3-q_1)\\), etc.)\n\nObservations beyond the whiskers are represented by dots or circles, sometimes termed outliers. However, beware of this terminology: the larger the sample size, the more values will fall outside the whiskers (about 0.7% for normal data). This is a drawback of boxplots, which were conceived at a time where big data didn’t exist. If you want to combine boxplots with the raw data, remove the display of outliers to avoid artefacts.\n\n\n\n\n\n\n\n\nFigure 3.5: Box-and-whiskers plot\n\n\n\n\n\nWeissgerber et al. (2019) contains many examples of how to build effective visualizations, including highlighting particular aspects using color, jittering, transparency and how to adequately select the display zone.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Completely randomized designs</span>"
    ]
  },
  {
    "objectID": "completely_randomized_trials.html#pairwise-tests",
    "href": "completely_randomized_trials.html#pairwise-tests",
    "title": "3  Completely randomized designs",
    "section": "3.3 Pairwise tests",
    "text": "3.3 Pairwise tests\nIf the global test of equality of mean for the one-way ANOVA leads to rejection of the null, the conclusion is that one of the group has a different mean. However, the test does not indicate which of the groups differ from the rest nor does it say how many are different. There are different options: one is custom contrasts, a special instance of which is pairwise comparisons.\nWe are interested in looking at the difference between the (population) average of group \\(i\\) and \\(j\\), say. The null hypothesis of no difference translate into \\(\\mu_i-\\mu_j=0\\), so the numerator of our statistic will be the estimator \\(\\widehat{\\mu}_i - \\widehat{\\mu}_j\\) of the difference in sample mean, minus zero.\nAssuming equal variances, the two-sample \\(t\\)-test statistic is \\[\\begin{align*}\nt_{ij} = \\frac{(\\widehat{\\mu}_i - \\widehat{\\mu}_j) - 0}{\\mathsf{se}(\\widehat{\\mu}_i - \\widehat{\\mu}_j)} =\\frac{\\widehat{\\mu}_i - \\widehat{\\mu}_j}{\\widehat{\\sigma} \\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)^{1/2}},\n\\end{align*}\\] where \\(\\widehat{\\mu}_i\\) and \\(n_i\\) are respectively the sample average and the number of observations of group \\(i\\), and \\(\\widehat{\\sigma}\\) is the estimator of the standard deviation derived using the whole sample (assuming equal variance). As usual, the denominator of \\(t_{ij}\\) is the standard error of the \\(\\widehat{\\mu}_i - \\widehat{\\mu}_j\\), whose postulated difference is zero. We can compare the value of the observed statistic to a Student-\\(t\\) distribution with \\(n-K\\) degrees of freedom, denoted \\(\\mathsf{St}(n-K)\\). For a two-sided alternative, we reject if \\(|t_{ij}| &gt; \\mathfrak{t}_{1-\\alpha/2}\\), for \\(\\mathfrak{t}_{1-\\alpha/2}\\) the \\(1-\\alpha/2\\) quantile of \\(\\mathsf{St}(n-K)\\).\nFigure 3.6 shows the density of the benchmark distribution for pairwise comparisons in mean for the arithmetic data. The blue area under the curve defines the set of values for which we fail to reject the null hypothesis, whereas all values of the test statistic falling in the red area lead to rejection at level \\(5\\)%.\n\n\n\n\n\n\n\n\nFigure 3.6: Student-t null distribution and rejection region for a t-test.\n\n\n\n\n\nWe fail to reject \\(\\mathscr{H}_0\\) as \\(\\mathfrak{t}_{\\alpha/2} \\leq t_{ij} \\leq \\mathfrak{t}_{1-\\alpha/2}\\)4: this gives us another way of presenting the same conclusion in terms of the set of mean differences \\(\\delta_{ij} = \\mu_i - \\mu_j\\) for which \\[\\begin{align*}\n\\mathfrak{t}_{\\alpha/2} \\leq \\frac{\\widehat{\\delta}_{ij} - \\delta_{ij}}{\\mathsf{se}\\left(\\widehat{\\delta}_{ij}\\right)} \\leq \\mathfrak{t}_{1-\\alpha/2}\n\\end{align*}\\] which is equivalent upon rearranging to the \\((1-\\alpha)\\) confidence interval for \\(\\delta_{ij}\\), \\[\\begin{align*}\n\\mathsf{CI} = \\left[\\widehat{\\delta}_{ij} - \\mathfrak{t}_{1-\\alpha/2}\\mathsf{se}\\left(\\widehat{\\delta}_{ij}\\right), \\widehat{\\delta}_{ij} - \\mathfrak{t}_{\\alpha/2}\\mathsf{se}\\left(\\widehat{\\delta}_{ij}\\right)\\right].\n\\end{align*}\\]\n\nExample 3.2 (Calculation of pairwise comparisons) We consider the pairwise average difference in scores between the praised (group C) and the reproved (group D) of the arithmetic study. The sample averages are respectively \\(\\widehat{\\mu}_C = 27.4\\) and \\(\\widehat{\\mu}_D = 23.4\\) and the estimated pooled standard deviation for the five groups is \\(1.15\\). Thus, the estimated average difference between groups \\(C\\) and \\(D\\) is \\(\\widehat{\\delta}_{CD} = 4\\) and the standard error for the difference is \\(\\mathsf{se}(\\widehat{\\delta}_{CD}) = 1.6216\\); all of these are calculated by software.\nIf we take as null hypothesis \\(\\mathscr{H}_0: \\delta_{CD}=0\\), the \\(t\\) statistic is \\[\\begin{align*}t=\\frac{\\widehat{\\delta}_{CD} - 0}{\\mathsf{se}(\\widehat{\\delta}_{CD})} = \\frac{4}{1.6216}=2.467\n\\end{align*}\\] and the \\(p\\)-value is \\(p=0.018\\). We therefore reject the null hypothesis at level \\(\\alpha=0.05\\) to conclude that there is a significant difference (at level \\(\\alpha=0.05\\)) between the average scores of students praised and reproved.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Completely randomized designs</span>"
    ]
  },
  {
    "objectID": "completely_randomized_trials.html#model-assumptions",
    "href": "completely_randomized_trials.html#model-assumptions",
    "title": "3  Completely randomized designs",
    "section": "3.4 Model assumptions",
    "text": "3.4 Model assumptions\nSo far, we have brushed all of the model assumptions under the carpet. These are necessary requirements for the inference to be valid: any statement related to p-values, etc. will approximately hold only if a set of assumptions is met in the first place. This section is devoted to the discussion of these assumptions, showcasing examples of where things can go wrong.\nIt is customary to write the \\(i\\)th observation of the \\(k\\)th group in the one-way analysis of variance model as \\[\n\\underset{\\text{observation}}{Y_{ik}} = \\underset{\\text{mean of group $k$}}{\\mu_k} + \\underset{\\text{error term}}{\\varepsilon_{ik}},\n\\tag{3.2}\\] where the error terms \\(\\varepsilon_{ik}\\), which account for unexplained variability and individual differences, are independent from one with mean zero and variance \\(\\sigma^2\\).\n\n3.4.1 Additivity\nThe basic assumption of most designs is that we can decompose the outcome into two components (Cox 1958) \\[\n\\begin{pmatrix} \\text{quantity depending} \\\\\n\\text{on the treatment used}\\end{pmatrix} +\n\\begin{pmatrix} \\text{quantity depending only } \\\\\n\\text{on the particular unit}\n\\end{pmatrix}\n\\tag{3.3}\\]\nThis additive decomposition further assumes that each unit is unaffected by the treatment of the other units and that the average effect of the treatment is constant. Thus, it is justified to use difference in sample mean to estimate the treatment effect since on average, the individual effect is zero.\nThe decomposition of observations in terms of group average and mean-zero noise in Equation 3.2 suggests that we could plot the error term \\(\\varepsilon_{ik}\\) against observations, or against other factors or explanatories, to see if there is any unusual structure unexplained by the model and indicating problems with the randomization or additivity. However, we do not have access to \\(\\varepsilon_{ik}\\) since both the true group mean \\(\\mu_k\\) and the error \\(\\varepsilon_{ik}\\) are unknown. However, a good proxy is the ordinary residual \\(e_{ik} = y_{ik} - \\widehat{\\mu}_k\\) where \\(\\widehat{\\mu}_k\\) is the sample mean of all observations in experimental group \\(k\\). By construction, the sample mean of the residuals will be zero, but local deviations may indicate violations of the analysis (for example, plotting residuals against time could show a learning effect).\nMany graphical diagnostics use residuals, i.e., some variant of the observations minus the group mean \\(y_{ik} - \\widehat{\\mu}_k\\), to look for violation of the assumptions.\n\n\n\n\n\n\n\n\nFigure 3.7: Data satisfying the assumptions of the one-way analysis of variance model, with additive effects, independent observations and common variance.\n\n\n\n\n\nMore generally, the test statistic may make further assumptions. The \\(F\\)-test of the global null \\(\\mu_1 = \\cdots \\mu_K\\) assumes that the \\(i\\)th observation of group \\(k\\), say \\(y_{ik}\\), has average \\(\\mathsf{E}(Y_{ik}) = \\mu_k\\) and variance \\(\\mathsf{Va}(Y_{ik}) = \\sigma^2\\). The latter is estimated using all of the residuals, with \\(\\widehat{\\sigma}^2 = \\sum_k\\sum_i (y_{ik} - \\widehat{\\mu}_k)^2/(n-K)\\). Under these assumptions, the \\(F\\)-test statistic for the global null \\(\\mu_1 = \\cdots = \\mu_K\\) is the most powerful because it uses all of the data to get a more precise estimation of the variability. Generally, there may be other considerations than power that may guide the choice of test statistic, including robustness (sensitivity to extremes and outliers). For unequal variance, other statistics than the \\(F\\)-test statistic may be more powerful.\n\nExample 3.3 (Additivity and transformations) Chapter 2 of Cox (1958) discusses the assumption of additivity and provides useful examples showing when it cannot be taken for granted. One of them, Example 2.3, is a scenario in which the experimental units are participants and they are asked to provide a ranking of different kindergarten students on their capacity to interact with others in games, ranked on a scale of 0 to 100. A random group of students receives additional orthopedagogical support, while the balance is in the business-as-usual setting (control group). Since there are intrinsic differences at the student level, one could consider a paired experiment and take as outcome the difference in sociability scores at the beginning and at the end of the school year.\nOne can expect the treatment to have more impact on people with low sociability skills who were struggling to make contacts: a student who scored 50 initially might see an improvement of 20 points with support relative to 10 in the business-as-usual scenario, whereas another who is well integrated and scored high initially may see an improvement of only 5 more had (s)he been assigned to the support group. This implies that the treatment effects are not constant over the scale, a violation of the additivity assumption. One way to deal with this is via transformations: Cox (1958) discusses the transformation \\(\\log\\{(x+0.5)/(100.5-x)\\}\\) to reduce the warping due to scale.\n\nAnother example is in experiments where the effect of treatment is multiplicative, so that the output is of the form \\[\\begin{align*}\n\\begin{pmatrix} \\text{quantity depending only } \\\\\n\\text{on the particular unit}\n\\end{pmatrix} \\times\n\\begin{pmatrix} \\text{quantity depending} \\\\\n\\text{on the treatment used}\\end{pmatrix}\n\\end{align*}\\] Usually, this arises for positive responses and treatments, in which case taking natural logarithms on both sides, with \\(\\log(xy) = \\log x + \\log y\\) yielding again an additive decomposition.\n\nExample 3.4 (Inadequacy of additivity based on context) This example is adapted from Cox (1958), Example 2.2. Children suffering from attention deficit hyperactivity disorder (ADHD) may receive medication to increase their attention span, measured on a scale of 0 to 100, with 0 indicating normal attention span. An experiment can be designed to assess the impact of a standardized dose in a laboratory by comparing performances of students on a series of task before and after, when to a placebo. To make a case, suppose that students with ADHD fall into two categories: low symptoms and strong symptoms. In the low symptom group, the average attention is 8 per cent with the drug and 12 per cent with the placebo, whereas for people with strong symptoms, the average is 40 per cent among treated and 60 per cent with the placebo. If these two categories are equally represented in the experiment and the population, we would estimate an average reduction of 12 percent in the score (thus higher attention span among treated). Yet, this quantity is artificial, and a better measure would be that symptoms are for the treatment are 2/3 of those of the control (the ratio of proportions).\n\nEquation 3.3 also implies that the effect of the treatment is constant for all individuals. This often isn’t the case: in an experimental study on the impact of teaching delivery type (online, hybrid, in person), it may be that the response to the choice of delivery mode depends on the different preferences of learning types (auditory, visual, kinestetic, etc.) Thus, recording additional measurements that are susceptible to interact may be useful; likewise, treatment allotment must factor in this variability should we wish to make it detectable. The solution to this would be to setup a more complex model (two-way analysis of variance, general linear model) or stratify by the explanatory variable (for example, compute the difference within each level).\n\n\n\n\n\n\n\n\nFigure 3.8: Difference in average response; while the treatment seems to lead to a decrease in the response variable, a stratification by age group reveals this only occurs in the younger population aged less than 25 years, with a seemingly reversed effect for the older adults. Thus, the marginal model implied by the one-way analysis of variance is misleading.\n\n\n\n\n\n\n\n3.4.2 Heterogeneity\nThe one-way ANOVA builds on the fact that the variance in each group is equal, so that upon recentering, we can estimate it from the variance of the residuals \\(y_{ik} - \\widehat{\\mu}_k\\). Specifically, the unbiased variance estimator is the denominator of the \\(F\\)-statistic formula, i.e., the within sum of squares divided by \\(n-K\\) with \\(n\\) the total number of observations and \\(K\\) the number of groups under comparison.\nFor the time being, we consider hypothesis tests for the homogeneity (equal) variance assumption. The most commonly used tests are Bartlett’s test5 and Levene’s test (a more robust alternative, less sensitive to outliers). For both tests, the null distribution is \\(\\mathscr{H}_0: \\sigma^2_1 = \\cdots = \\sigma^2_K\\) against the alternative that at least two differ. The Bartlett test statistic has a \\(\\chi^2\\) null distribution with \\(K-1\\) degrees of freedom, whereas Levene’s test has an \\(F\\)-distribution with (\\(K-1\\), \\(n-K\\)) degrees of freedom: it is equivalent to computing the one-way ANOVA \\(F\\)-statistic with the absolute value of the centered residuals, \\(|y_{ik} - \\widehat{\\mu}_k|\\), as observations.\n\nbartlett.test(score ~ group,\n              data = arithmetic)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  score by group\nBartlett's K-squared = 2.3515, df = 4, p-value = 0.6714\n\ncar::leveneTest(score ~ group,\n                data = arithmetic,\n                center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df F value Pr(&gt;F)\ngroup  4   1.569 0.2013\n      40               \n\n# compare with one-way ANOVA\nmod &lt;- lm(score ~ group, data = arithmetic)\narithmetic$absresid &lt;- abs(resid(mod)) #|y_{ik}-mean_k|\nanova(aov(absresid ~ group, data = arithmetic))\n\nAnalysis of Variance Table\n\nResponse: absresid\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\ngroup      4  17.354  4.3385   1.569 0.2013\nResiduals 40 110.606  2.7652               \n\n\nWe can see in both cases that the \\(p\\)-values are large enough to dismiss any concern about the inequality of variance. However, should the latter be a problem, we can proceed with a test statistic that does not require variances to be equal. The most common choice is a modification due to Satterthwaite called Welch’s ANOVA. It is most commonly encountered in the case of two groups (\\(K=2\\)) and is the default option in R with t.test or oneway.test.\nWhat happens with the example of the arithmetic data when we use this instead of the usual \\(F\\) statistic? Here, the evidence is overwhelming so no changes to the conclusion. Generally, the only drawback of using Welch’s ANOVA over the usual \\(F\\) statistic is the need to have enough observations in each of the group to reliably estimate a separate variance6. For Welch’s ANOVA, we have to estimate \\(2K\\) parameters (one mean and one variance per group), rather than \\(K+1\\) parameters for the one-way ANOVA (one mean per group, one overall variance).\n\n# Welch ANOVA\noneway.test(score ~ group, data = arithmetic, \n            var.equal = FALSE)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  score and group\nF = 18.537, num df = 4.000, denom df = 19.807, p-value = 1.776e-06\n\n# Usual F-test statistic\noneway.test(score ~ group, data = arithmetic, \n            var.equal = TRUE)\n\n\n    One-way analysis of means\n\ndata:  score and group\nF = 15.268, num df = 4, denom df = 40, p-value = 1.163e-07\n\n\nNotice how the degrees of freedom of the denominator have decreased. If we use pairwise.t.test with argument pool.sd=FALSE, this amounts to running Welch \\(t\\)-tests separately for each pair of variable.\nWhat are the impacts of unequal variance if we use the \\(F\\)-test instead? For one, the pooled variance will be based on a weighted average of the variance in each group, where the weight is a function of the sample size. This can lead to size distortion (meaning that the proportion of type I error is not the nominal level \\(\\alpha\\) as claimed) and potential loss of power. The following toy example illustrates this.\n\nExample 3.5 (Violation of the null hypothesis of equal variance)  \n\n\n\n\n\n\n\n\nFigure 3.9: Histogram of the null distribution of \\(p\\)-values obtained through simulation using the classical analysis of variance \\(F\\)-test (left) and Welch’s unequal variance alternative (right), based on 10 000 simulations. Each simulated sample consist of 50 observations from a \\(\\mathsf{Normal}(0, 1)\\) distribution and 10 observations from \\(\\mathsf{Normal}(0, 9)\\). The uniform distribution would have 5% in each of the 20 bins used for the display.\n\n\n\n\n\nWe consider for simplicity a problem with \\(K=2\\) groups, which is the two-sample \\(t\\)-test. We simulated 50 observations from a \\(\\mathsf{Normal}(0, 1)\\) distribution and 10 observations from \\(\\mathsf{Normal}(0, 9)\\), comparing the distribution of the \\(p\\)-values for the Welch and the \\(F\\)-test statistics. Figure 3.9 shows the results. The percentage of \\(p\\)-values less than \\(\\alpha=0.05\\) based on 10 000 replicates is estimated to be 4.76% for the Welch statistic, not far from the level. By contrast, we reject 28.95% of the time with the one-way ANOVA global \\(F\\)-test: this is a large share of innocents sentenced to jail based on false premises! While the size distortion is not always as striking, heterogeneity should be accounted in the design by requiring sufficient sample sizes (whenever costs permits) in each group to be able to estimate the variance reliably and using an adequate statistic.\n\nThere are alternative graphical ways of checking the assumption of equal variance, many including the standardized residuals \\(r_{ik} = (y_{ik} - \\widehat{\\mu}_k)/\\widehat{\\sigma}\\) against the fitted values \\(\\widehat{\\mu}_k\\). We will cover these in later sections.\nOftentimes, unequal variance occurs because the model is not additive. You could use variance-stabilizing transformations (e.g., log for multiplicative effects) to ensure approximately equal variance in each group. Another option is to use a model that is suitable for the type of response you have (including count and binary data). Lastly, it may be necessary to explicitly model the variance in more complex design (including repeated measures) where there is a learning effect over time and variability decreases as a result. Consult an expert if needed.\n\n\n3.4.3 Normality\nThere is a persistent yet incorrect claim in the literature that the data (either response, explanatory or both) must be normal in order to use (so-called parametric) models like the one-way analysis of variance. With normal data and equal variances, the eponymous distributions of the \\(F\\) and \\(t\\) tests are exact: knowing the exact distribution does no harm and is convenient for mathematical derivations. However, it should be stressed that this condition is unnecessary: the results hold approximately for large samples by virtue of the central limit theorem. This probability results dictates that, under general conditions nearly universally met, the sample mean behaves like a normal distribution in large samples. This applet lets you explore the impact of the underlying population from which the data are drawn and the interplay with the sample size before the central limit theorem kicks in. You can view this in Figure 3.2, where the simulated and theoretical large-sample distributions are undistinguishable with approximately 20 observations per group.\nWhile many authors may advocate rules of thumbs (sample size of \\(n&gt;20\\) or \\(n&gt;30\\) per group, say), these rules are arbitrary: the approximation is not much worst at \\(n=19\\) than at \\(n=20\\). How large must the sample size be for the approximation to hold? It largely depends on the distribution in the population: the more extremes, skewness, etc. you have, the larger the number of observation must be in order for the approximation to be valid. Figure 3.10 shows a skewed to the right bimodal distribution and the distribution of the sample mean under repeated sampling. Even with \\(n=5\\) observations (bottom left), the approximation is not bad but it may still be very far off with \\(n=50\\) for heavy-tailed data.\n\n\n\n\n\n\n\n\nFigure 3.10: Graphical representation of the central limit theorem. Top left: density of the underlying population from which samples are drawn. Top right: a sample of 20 observations with its sample mean (vertical red). Bottom panels: histogram of sample averages for samples of size 5 (left) and 20 (right) with normal approximation superimposed. As the sample size increases, the normal approximation for the mean is more accurate and the standard error decreases.\n\n\n\n\n\nIt is important to keep in mind that all statistical statements are typically approximate and their reliability depends on the sample size: too small a sample may hampers the strength of your conclusions. The default graphic for checking whether a sample matches a postulated distribution is the quantile-quantile plot.\n\n\n3.4.4 Independence\nWhile I am not allowed to talk of independence as a Quebecer7, this simply means that knowing the value of one observation tells us nothing about the value of any other in the sample. Independence may fail to hold in case of group structure (family dyads, cluster sampling) which have common characteristics or more simply in the case of repeated measurements. Random assignment to treatment is thus key to ensure that the measure holds, and ensuring at the measurement phase that there is no spillover.\n\nExample 3.6 (Independence of measurements) There are many hidden ways in which measurements can impact the response. Physical devices that need to be calibrated before use (scales, microscope) require tuning: if measurements are done by different experimenters or on different days, it may impact and add systematic shift in means for the whole batch.\n\nWhat is the impact of dependence between measurements? Heuristically, correlated measurements carry less information than independent ones. In the most extreme case, there is no additional information and measurements are identical, but adding them multiple times unduly inflates the statistic and leads to more frequent rejections.\n\n\n\n\n\n\n\n\nFigure 3.11: Percentage of rejection of the null hypothesis for the \\(F\\)-test of equality of means for the one way ANOVA with data generated with equal mean and variance from an equicorrelation model (within group observations are correlated, between group observations are independent). The nominal level of the test is 5%.\n\n\n\n\n\nThe lack of independence can also have drastic consequences on inference and lead to false conclusions: Figure 3.11 shows an example with correlated samples within group (or equivalently repeated measurements from individuals) with 25 observations per group. The \\(y\\)-axis shows the proportion of times the null is rejected when it shouldn’t be. Here, since the data are generated from the null model (equal mean) with equal variance, the inflation in the number of spurious discoveries, false alarm or type I error is alarming and the inflation is substantial even with very limited correlation between measurements.\n\n\n\n\nCox, David R. 1958. Planning of Experiments. New York, NY: Wiley.\n\n\nWeissgerber, Tracey L., Natasa M. Milic, Stacey J. Winham, and Vesna D. Garovic. 2015. “Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm.” PLOS Biology 13 (4): 1–10. https://doi.org/10.1371/journal.pbio.1002128.\n\n\nWeissgerber, Tracey L., Stacey J. Winham, Ethan P. Heinzen, Jelena S. Milin-Lazovic, Oscar Garcia-Valencia, Zoran Bukumiric, Marko D. Savic, Vesna D. Garovic, and Natasa M. Milic. 2019. “Reveal, Don’t Conceal.” Circulation 140 (18): 1506–18. https://doi.org/10.1161/CIRCULATIONAHA.118.037777.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Completely randomized designs</span>"
    ]
  },
  {
    "objectID": "completely_randomized_trials.html#footnotes",
    "href": "completely_randomized_trials.html#footnotes",
    "title": "3  Completely randomized designs",
    "section": "",
    "text": "We say a sample is balanced if each (sub)group contains the same number of observations.↩︎\nThere are only \\(K\\) parameter estimates for the mean, since the overall mean is full determined by the other averages with \\(n\\widehat{\\mu} =n_1\\widehat{\\mu}_1 + \\cdots + n_K \\widehat{\\mu}_K\\).↩︎\nMostly because the central limit theorem kicks in↩︎\nNote that the Student-\\(t\\) distribution is symmetric, so \\(\\mathfrak{t}_{1-\\alpha/2} = -\\mathfrak{t}_{\\alpha/2}\\).↩︎\nFor the connoisseur, this is a likelihood ratio test under the assumption of normally distributed data, with a Bartlett correction to improve the \\(\\chi^2\\) approximation to the null distribution.↩︎\nCoupled with a slight loss of power if the variance are truly equal, more on this later.↩︎\nAll credits for this pun are due to C. Genest↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Completely randomized designs</span>"
    ]
  },
  {
    "objectID": "contrasts_multipletesting.html",
    "href": "contrasts_multipletesting.html",
    "title": "4  Contrasts and multiple testing",
    "section": "",
    "text": "4.1 Contrasts\nSuppose we perform an analysis of variance and the \\(F\\)-test for the (global) null hypothesis that the averages of all groups are equal is very large: we reject the null hypothesis in favor of the alternative, which states that at least one of the group average is different. The follow-up question will be where these differences lie. Indeed, in an experimental context, this implies one or more of the manipulation has a different effect from the others on the mean response. Oftentimes, this isn’t interesting in itself: we could be interested in comparing different options relative to a status quo (e.g., for new drugs or medical treatment), or determine whether specific combinations work better than separately, or find the best treatment by comparing all pairs.\nThe scientific question of interest that warranted the experiment may lead to a specific set of hypotheses, which can be formulated by researchers as comparisons between means of different subgroups. We can normally express these as contrasts. As Dr. Lukas Meier puts it, if the global \\(F\\)-test for equality of means is equivalent to a dimly lit room, contrasts are akin to spotlight that let one focus on particular aspects of differences in treatments.\nFormally speaking, a contrast is a linear combination of averages: in plain English, this means we assign a weight to each group average and add them up, and then compare that summary to a postulated value \\(a\\), typically zero. Contrasts encode research question of interest: if \\(c_i\\) denotes the weight of group average \\(\\mu_i\\) \\((i=1, \\ldots, K)\\), then we can write the contrast as \\(C = c_1 \\mu_1 + \\cdots + c_K \\mu_K\\) with the null hypothesis \\(\\mathscr{H}_0: C=a\\) for a two-sided alternative. The sample estimate of the linear contrast is obtained by replacing the unknown population average \\(\\mu_i\\) by the sample average of that group, \\(\\widehat{\\mu}_i = \\overline{y}_{i}\\). We can easily obtain the standard error of the linear combination \\(C.\\)1 We can then build a \\(t\\) statistic as usual by looking at the difference between our postulated value and the observed weighted mean, suitably standardized. If the global \\(F\\)-test leads to rejection of the null, there exists a contrast which is significant at the same level.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contrasts and multiple testing</span>"
    ]
  },
  {
    "objectID": "contrasts_multipletesting.html#contrasts",
    "href": "contrasts_multipletesting.html#contrasts",
    "title": "4  Contrasts and multiple testing",
    "section": "",
    "text": "4.1.1 Orthogonal contrasts\nSometimes, linear contrasts encode disjoint bits of information about the sample: for example, one contrast that compares groups the first two groups versus one that compares the third and fourth is in effect using data from two disjoint samples, as contrasts are based on sample averages. Whenever the contrasts vectors are orthogonal, the tests will be uncorrelated. Mathematically, if we let \\(c_{i}\\) and \\(c^{*}_{i}\\) denote weights attached to the mean of group \\(i\\) comprising \\(n_i\\) observations, contrasts are orthogonal if \\(c_{1}c^{*}_{1}/n_1 + \\cdots + c_{K}c^{*}_K/n_K = 0\\); if the sample is balanced with the same number of observations in each group, \\(n/K = n_1 =\\cdots = n_K\\), we can consider the dot product of the two contrast vectors and neglect the subsample sizes.\nIf we have \\(K\\) groups, there are \\(K-1\\) contrasts for pairwise differences, the last one being captured by the sample mean for the overall effect2. If we care only about difference between groups (as opposed to the overall effect of all treatments), we impose a sum-to-zero constraint on the weights so \\(c_1 + \\cdots + c_K=0\\). Keep in mind that, although independent tests are nice mathematically, contrasts should encode the hypothesis of interest to the researchers: we choose contrasts because they are meaningful, not because they are orthogonal.\n\nExample 4.1 (Contrasts for encouragement on teaching) The arithmetic data example considered five different treatment groups with 9 individuals in each. Two of them were control groups, one received praise, another was reproved and the last was ignored.\nSuppose that researchers were interested in assessing whether the experimental manipulation had an effect, and whether the impact of positive and negative feedback is the same on students.3\nSuppose we have five groups in the order (control 1, control 2, praised, reproved, ignored). We can express these hypothesis as\n\n\\(\\mathscr{H}_{01}\\): \\(\\mu_{\\text{praise}} = \\mu_{\\text{reproved}}\\)\n\\(\\mathscr{H}_{02}\\): \\[\\begin{align*}\n\\frac{1}{2}(\\mu_{\\text{control}_1}+\\mu_{\\text{control}_2}) = \\frac{1}{3}\\mu_{\\text{praised}} + \\frac{1}{3}\\mu_{\\text{reproved}} + \\frac{1}{3}\\mu_{\\text{ignored}}\n\\end{align*}\\]\n\nNote that, for the hypothesis of control vs experimental manipulation, we look at average of the different groups associated with each item. Using the ordering, the weights of the contrast vector are \\((1/2, 1/2, -1/3, -1/3, -1/3)\\) and \\((0, 0, 1, -1, 0)\\). There are many equivalent formulation: we could multiply the weights by any number (different from zero) and we would get the same test statistic, as the latter is standardized.\n\nlibrary(emmeans)\ndata(arithmetic, package = \"hecedsm\")\nlinmod &lt;- aov(score ~ group, data = arithmetic)\nlinmod_emm &lt;- emmeans(linmod, specs = 'group')\ncontrast_specif &lt;- list(\n  controlvsmanip = c(0.5, 0.5, -1/3, -1/3, -1/3),\n  praisedvsreproved = c(0, 0, 1, -1, 0)\n)\ncontrasts_res &lt;- \n  contrast(object = linmod_emm, \n                    method = contrast_specif)\n# Obtain confidence intervals instead of p-values\nconfint(contrasts_res)\n\n\n\n\n\nTable 4.1: Contrasts estimates for the arithmetic data\n\n\n\n\n\n\ncontrast\nestimate\nstd. error\ndf\nlower (CI)\nupper (conf. limit)CI)\n\n\n\n\ncontrol vs manip\n-3.33\n1.05\n40\n-5.45\n-1.22\n\n\npraised vs reproved\n4.00\n1.62\n40\n0.72\n7.28\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4.2 (Teaching to read) We consider data from Baumann, Seifert-Kessell, and Jones (1992). The abstract of the paper provides a brief description of the study\n\nThis study investigated the effectiveness of explicit instruction in think aloud as a means to promote elementary students’ comprehension monitoring abilities. Sixty-six fourth-grade students were randomly assigned to one of three experimental groups: (a) a Think-Aloud (TA) group, in which students were taught various comprehension monitoring strategies for reading stories (e.g., self-questioning, prediction, retelling, rereading) through the medium of thinking aloud; (b) a Directed reading-Thinking Activity (DRTA) group, in which students were taught a predict-verify strategy for reading and responding to stories; or (c) a Directed reading Activity (DRA) group, an instructed control, in which students engaged in a noninteractive, guided reading of stories.\n\nLooking at Table 4.2, we can see that DRTA has the highest average, followed by TA and directed reading (DR).\n\nlibrary(emmeans) #load package\ndata(BSJ92, package = \"hecedsm\")\nmod_post &lt;- aov(posttest1 ~ group, data = BSJ92)\nemmeans_post &lt;- emmeans(object = mod_post, \n                        specs = \"group\")\n\n\n\n\n\nTable 4.2: Estimated group averages with standard errors and 95% confidence intervals for post-test 1.\n\n\n\n\n\n\nterms\nmarg. mean\nstd. err.\ndof\nlower (CI)\nupper (CI)\n\n\n\n\nDR\n6.68\n0.68\n63\n5.32\n8.04\n\n\nDRTA\n9.77\n0.68\n63\n8.41\n11.13\n\n\nTA\n7.77\n0.68\n63\n6.41\n9.13\n\n\n\n\n\n\n\n\n\n\nThe purpose of Baumann, Seifert-Kessell, and Jones (1992) was to make a particular comparison between treatment groups. From the abstract:\n\nThe primary quantitative analyses involved two planned orthogonal contrasts—effect of instruction (TA + DRTA vs. 2 x DRA) and intensity of instruction (TA vs. DRTA)—for three whole-sample dependent measures: (a) an error detection test, (b) a comprehension monitoring questionnaire, and (c) a modified cloze test.\n\nThe hypothesis of Baumann, Seifert-Kessell, and Jones (1992) is \\(\\mathscr{H}_0: \\mu_{\\mathrm{TA}} + \\mu_{\\mathrm{DRTA}} = 2 \\mu_{\\mathrm{DRA}}\\) or, rewritten slightly, \\[\\begin{align*}\n\\mathscr{H}_0: - 2 \\mu_{\\mathrm{DR}} + \\mu_{\\mathrm{DRTA}} + \\mu_{\\mathrm{TA}} = 0.\n\\end{align*}\\] with weights \\((-2, 1, 1)\\); the order of the levels for the treatment are (\\(\\mathrm{DRA}\\), \\(\\mathrm{DRTA}\\), \\(\\mathrm{TA}\\)) and it must match that of the coefficients. An equivalent formulation is \\((2, -1, -1)\\) or \\((1, -1/2, -1/2)\\): in either case, the estimated differences will be different (up to a constant multiple or a sign change). The vector of weights for \\(\\mathscr{H}_0:  \\mu_{\\mathrm{TA}} = \\mu_{\\mathrm{DRTA}}\\) is (\\(0\\), \\(-1\\), \\(1\\)): the zero appears because the first component, \\(\\mathrm{DRA}\\) doesn’t appear. The two contrasts are orthogonal since \\((-2 \\times 0) + (1 \\times -1) + (1 \\times 1) = 0\\).\n\n# Identify the order of the level of the variables\nwith(BSJ92, levels(group))\n\n[1] \"DR\"   \"DRTA\" \"TA\"  \n\n# DR, DRTA, TA (alphabetical)\ncontrasts_list &lt;- list(\n  \"C1: DRTA+TA vs 2DR\" = c(-2, 1, 1), \n  # Contrasts: linear combination of means, coefficients sum to zero\n  # 2xDR = DRTA + TA =&gt; -2*DR + 1*DRTA + 1*TA = 0 and -2+1+1 = 0\n  \"C1: average (DRTA+TA) vs DR\" = c(-1, 0.5, 0.5), \n  #same thing, but halved so in terms of average\n  \"C2: DRTA vs TA\" = c(0, 1, -1),\n  \"C2: TA vs DRTA\" = c(0, -1, 1) \n  # same, but sign flipped\n)\ncontrasts_post &lt;- \n  contrast(object = emmeans_post,\n           method = contrasts_list)\ncontrasts_summary_post &lt;- summary(contrasts_post)\n\n\n\n\n\nTable 4.3: Estimated contrasts for post-test 1.\n\n\n\n\n\n\ncontrast\nestimate\nstd. err.\ndof\nstat\np-value\n\n\n\n\nC1: DRTA+TA vs 2DR\n4.18\n1.67\n63\n2.51\n0.01\n\n\nC1: average (DRTA+TA) vs DR\n2.09\n0.83\n63\n2.51\n0.01\n\n\nC2: DRTA vs TA\n2.00\n0.96\n63\n2.08\n0.04\n\n\nC2: TA vs DRTA\n-2.00\n0.96\n63\n-2.08\n0.04\n\n\n\n\n\n\n\n\n\n\nWe can look at these differences; since DRTA versus TA is a pairwise difference, we could have obtained the \\(t\\)-statistic directly from the pairwise contrasts using pairs(emmeans_post). Note that the two different ways of writing the comparison between DR and the average of the other two methods yield different point estimates, but same inference (i.e., the same \\(p\\)-values). For contrast \\(C_{1b}\\), we get half the estimate (but the standard error is also halved) and likewise for the second contrasts we get an estimate of \\(\\mu_{\\mathrm{DRTA}} - \\mu_{\\mathrm{TA}}\\) in the first case (\\(C_2\\)) and \\(\\mu_{\\mathrm{TA}} - \\mu_{\\mathrm{DRTA}}\\): the difference in group averages is the same up to sign.\nWhat is the conclusion of our analysis of contrasts? It looks like the methods involving teaching aloud have a strong impact on reading comprehension relative to only directed reading. The evidence is not as strong when we compare the method that combines directed reading-thinking activity and thinking aloud.\n\n\nExample 4.3 (Paper or plastic) Sokolova, Krishna, and Döring (2023) consider consumer bias when assessing how eco-friendly packages are. Items such as cereal are packaged in plastic bags, which themselves are covered in a box. They conjecture (and find) that consumers tend to view the packaging as being more eco-friendly when the amount of cardboard or paper surrounding the box is large, relative to the sole plastic package. We consider the data Study 2A, which measures the perceived environmental friendliness (PEF) as a function of the proportion of paper wrapping (either none, half of the area of the plastic, equal or twice). The authors are interested in comparing none with other choices.\nIf \\(\\mu_{0}, \\mu_{0.5}, \\mu_{1}, \\mu_2\\) denote the true mean of the PEF score as a function of the proportion of paper, we are interested in pairwise differences, but only relative to the reference \\(\\mu_{0}\\): \\[\\begin{align*}\n\\mu_0 = \\mu_{0.5}  & \\iff 1\\mu_0 - 1\\mu_{0.5} + 0\\mu_{1} + 0 \\mu_{2} = 0\\\\\n\\mu_0 = \\mu_{1} & \\iff 1\\mu_0 + 0\\mu_{0.5} -1\\mu_{1} + 0 \\mu_{2} = 0\\\\\n\\mu_0 = \\mu_{2} & \\iff 1\\mu_0 + 0\\mu_{0.5} + 0\\mu_{1} -1 \\mu_{2} = 0\n\\end{align*}\\] so contrast vectors \\((1, -1, 0, 0)\\), \\((1, 0, -1, 0)\\) and \\((1, 0, 0, -1)\\) would allow one to test the hypothesis.\n\ndata(SKD23_S2A, package = \"hecedsm\") # load data\nlinmod &lt;- lm(pef ~ proportion, data = SKD23_S2A) # fit simple linear regression\nanova(linmod) # check for significance of slope\ncoef(linmod) # extract intercept and slope\nanovamod &lt;- lm(pef ~ factor(proportion), data = SKD23_S2A) # one-way ANOVA\nmargmean &lt;- anovamod |&gt;  emmeans::emmeans(specs = \"proportion\") # group means\ncontrastlist &lt;- list( # specify contrast vectors\n   refvshalf = c(1, -1, 0, 0),\n   refvsone =  c(1, 0, -1, 0),\n   refvstwo =  c(1, 0, 0, -1))\n# compute contrasts relative to reference \nmargmean |&gt; emmeans::contrast(method = contrastlist)\n\nThe group averages are reported in Table 9.1, match those reported by the authors in the paper. They suggest an increased perceived environmental friendliness as the amount of paper used in the wrapping increases. We could fit a simple regression model to assess the average change, treating the proportion as a continuous explanatory variable. The estimated slope for the change in PEF score, which ranges from 1 to 7 in increments of 0.25, is 0.53 per area of paper. There is however strong evidence, given the data, that the change isn’t quite linear, as the fit of the linear regression model is significantly worse than the corresponding linear model.\n\n\n\n\nTable 4.4: Estimated group averages of PEF per proportion with standard errors\n\n\n\n\n\n\nproportion\nmarg. mean\nstd. err.\ndof\nlower (CI)\nupper (CI)\n\n\n\n\n0.0\n2.16\n0.093\n798\n1.98\n2.34\n\n\n0.5\n2.91\n0.093\n798\n2.73\n3.09\n\n\n1.0\n3.06\n0.092\n798\n2.88\n3.24\n\n\n2.0\n3.34\n0.089\n798\n3.17\n3.52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.5: Estimated contrasts for differences of PEF to no paper.\n\n\n\n\n\n\ncontrast\nestimate\nstd. err.\ndof\nstat\np-value\n\n\n\n\nrefvshalf\n-0.75\n0.13\n798\n-5.71\n0\n\n\nrefvsone\n-0.90\n0.13\n798\n-6.89\n0\n\n\nrefvstwo\n-1.18\n0.13\n798\n-9.20\n0\n\n\n\n\n\n\n\n\n\n\nAll differences reported in Table 9.2 are significant and positive, in line with the researcher’s hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contrasts and multiple testing</span>"
    ]
  },
  {
    "objectID": "contrasts_multipletesting.html#multiple-testing",
    "href": "contrasts_multipletesting.html#multiple-testing",
    "title": "4  Contrasts and multiple testing",
    "section": "4.2 Multiple testing",
    "text": "4.2 Multiple testing\nBeyond looking at the global null, we will be interested in a set of contrast statistics and typically this number can be large-ish. There is however a catch in starting to test multiple hypothesis at once.\nIf you do a single hypothesis test and the testing procedure is well calibrated (meaning that the model model assumptions hold), \\(p\\)-values are generated uniformly on the interval \\([0,1]\\) and there is a probability of \\(\\alpha\\) of making a type I error (i.e., concluding in favour of the alternative and rejecting the null incorrectly) if the null is true. The problem of the above approach is that the more tests you perform, the higher the chance of finding (incorrectly) something: with 20 independent tests, we expect that, on average, one of them will yield a \\(p\\)-value less than 5% even if this is a fluke. The problem with multiple testing is not so much that it occurs, but more than researchers tend to report selectively findings and only give the results of tests for which \\(p \\leq \\alpha\\), even if these are typically the product of chance. This makes most findings will not replicate: if we rerun the experiment, we will typically not find the same result.\nThere is an infinite potential number of contrasts with more than two factos. Not all tests are of interest: standard software will report all possible pairwise comparisons, but this may not be of interest as showcased in Example 4.4. If there are \\(K\\) groups to compare and any comparison is of interest, than we could performs \\(\\binom{K}{2}\\) pairwise comparisons with \\(\\mathscr{H}_{0}: \\mu_i = \\mu_j\\) for \\(i \\neq j\\). For \\(K=3\\), there are three such comparisons, 10 pairwise comparisons if \\(K=5\\) and 45 pairwise comparisons if \\(K=10\\). The number of pairwise comparisons grows quickly.\nThe number of tests performed in the course of an analysis can be very large. Y. Benjamini investigated the number of tests performed in each study of the Psychology replication project (Nosek et al. 2015): this number ranged from 4 to 700, with an average of 72 — most studies did not account for the fact they were performing multiple tests or selected the model and thus some ‘discoveries’ are bound to be spurious. It is natural to ask then how many results are spurious findings that correspond to type I errors. The paramount (absurd) illustration is the cartoon presented in Figure 4.1: note how there is little scientific backing for the theory (thus such test shouldn’t be of interest to begin with) and likewise the selective reporting made of the conclusions, despite nuanced conclusions.\nWe can also assess mathematically the problem. Assume for simplicity that all tests are independent4, then the probability of any rejecting the null incorrectly is \\(\\alpha\\), but larger over the collection (with tests \\(A\\) and \\(B\\), we could reject by mistake if \\(A\\) is a type I error and \\(B\\) isn’t, or vice-versa, or if both are incorrect rejections.\nThe probability of making at least one type I error if each test is conducted at level \\(\\alpha\\), say \\(\\alpha^{\\star}\\), is5 \\[\\begin{align}\n\\alpha^{\\star} &= 1 - \\text{probability of making no type I error}\n\\\\ &= 1- (1-\\alpha)^m\n\\\\ & \\leq m\\alpha\n\\end{align}\\]\nWith \\(\\alpha = 5\\)% and \\(m=4\\) tests, \\(\\alpha^{\\star} \\approx 0.185\\) whereas for \\(m=72\\) tests, \\(\\alpha^{\\star} \\approx 0.975\\): this means we are almost guaranteed even when nothing is going on to find “statistically significant” yet meaningless results.\n\n\n\n\n\n\n\n\nFigure 4.1: xkcd 882: Significant. The alt text is ‘So, uh, we did the green study again and got no link. It was probably a–’ ‘RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!’\n\n\n\n\n\nIt is sensible to try and reduce or bound the number of false positive or control the probability of getting spurious findings. We consider a family of \\(m\\) null hypothesis \\(\\mathscr{H}_{01}, \\ldots, \\mathscr{H}_{0m}\\), i.e. a collection of \\(m\\) hypothesis tests. The exact set depends on the context, but this comprises all hypothesis that are scientifically relevant and could be reported. These comparisons are called pre-planned comparisons: they should be chosen before the experiment takes place and pre-registered to avoid data dredging and selective reporting. The number of planned comparisons should be kept small relative to the number of parameters: for a one-way ANOVA, a general rule of thumb is to make no more comparisons than the number of groups, \\(K\\).\nSuppose that we perform \\(m\\) hypothesis tests in a study and define binary indicators \\[\\begin{align}\nR_i &= \\begin{cases} 1 & \\text{if we reject the null hypothesis }  \\mathscr{H}_{0i} \\\\\n0 & \\text{if we fail to reject } \\mathscr{H}_{0i}\n\\end{cases}\\\\\nV_i &=\\begin{cases} 1 & \\text{type I error for } \\mathscr{H}_{0i}\\quad  (R_i=1 \\text{ and  }\\mathscr{H}_{0i} \\text{ is true}) \\\\ 0 & \\text{otherwise}.\n\\end{cases}\n\\end{align}\\] With this notation, \\(R=R_1 + \\cdots + R_m\\) simply encodes the total number of rejections (\\(0 \\leq R \\leq m\\)), and \\(V = V_1 + \\cdots + V_m\\) is the number of null hypothesis rejected by mistake (\\(0 \\leq V \\leq R\\)).\nThe familywise error rate is the probability of making at least one type I error for the whole collection or test, in other words per family, is \\[\\begin{align*}\n\\mathsf{FWER} = \\Pr(V \\geq 1).\n\\end{align*}\\] To control the familywise error rate, one must be more stringent in rejecting the null and perform each test with a smaller level \\(\\alpha\\) so that the overall or simultaneous probability is less than \\(\\mathsf{FWER}\\).\n\n4.2.1 Bonferroni’s procedure\nThe easiest way to control for multiple testing is to perform each test at level \\(\\alpha/m\\), thereby ensuring that the family-wise error is controlled at level \\(\\alpha\\). This is a good option if \\(m\\) is small and the Bonferroni adjustment also controls the per-family error rate, which is the expected (theoretical average) number of false positive \\(\\mathsf{PFER} = \\mathsf{E}(V)\\). The latter is a more stringent criterion than the familywise error rate because \\(\\Pr(V \\geq 1) \\leq \\mathsf{E}(V)\\): the familywise error rate does not make a distinction between having one or multiple type I errors.6\nWhy is Bonferroni’s procedure popular? It is conceptually easy to understand and simple, and it applies to any design and regardless of the dependence between the tests. However, the number of tests to adjust for, \\(m\\), must be prespecified and the procedure leads to low power when the size of the family is large, meaning it makes detection of non-null effects more difficult. Moreover, if our sole objective is to control for the familywise error rate, then there are other procedures that are always better in the sense that they still control the \\(\\mathsf{FWER}\\) while leading to increased capacity of detection when the null is false.\nIf the raw (i.e., unadjusted) \\(p\\)-values are reported, we reject hypothesis \\(\\mathscr{H}_{0i}\\) if \\(m \\times p_i \\ge \\alpha\\): operationally, we multiply each \\(p\\)-value by \\(m\\) and reject if the result exceeds \\(\\alpha\\).\n\n\n4.2.2 Holm–Bonferroni’s procedure\nThe idea of Holm’s procedure is to use a sharper inequality bound and amounts to performing tests at different levels, with more stringent for smaller \\(p\\)-values. To perform Holm–Bonferroni,\n\norder the \\(p\\)-values of the family of \\(m\\) tests from smallest to largest, \\(p_{(1)} \\leq \\cdots \\leq p_{(m)}\\)\ntest sequentially the hypotheses: coupling Holm’s method with Bonferroni’s procedure, we compare \\(p_{(1)}\\) to \\(\\alpha_{(1)} = \\alpha/m\\), \\(p_{(2)}\\) to \\(\\alpha_{(2)}=\\alpha/(m-1)\\), etc. If \\(p_{(j)} \\geq \\alpha_{(j)}\\) but \\(p_{(i)} \\leq \\alpha_{(i)}\\) for \\(i=1, \\ldots, j-1\\) (all smaller \\(p\\)-values), we reject the associated hypothesis \\(\\mathscr{H}_{0(1)}, \\ldots, \\mathscr{H}_{0(j-1)}\\) but fail to reject \\(\\mathscr{H}_{0(j)}, \\ldots, \\mathscr{H}_{0(m)}\\).\n\nIf all of the \\(p\\)-values are less than their respective levels, than we still reject each null hypothesis. Otherwise, we reject all the tests whose \\(p\\)-values exceeds the smallest nonsignificant one. This procedure doesn’t control the per-family error rate, but is uniformly more powerful (lingo to say that it’s universally better for control) and thus leads to increased detection than Bonferroni’s method. To see this, consider a family of \\(m=3\\) \\(p\\)-values with values \\(0.01\\), \\(0.04\\) and \\(0.02\\). Bonferroni’s adjustment would lead us to reject the second and third hypotheses at level \\(\\alpha=0.05\\), but not Holm-Bonferroni.\n\n\n4.2.3 Multiple testing methods for analysis of variance\nThere are specialized procedures for the analysis of variance problem that leverages some of the assumptions (equal variance, large sample approximation for the distribution of means). There are three scenarios\n\nDunnett’s method for comparison to a reference or control group, controlling only for \\(K-1\\) pairwise differences\nTukey’s range procedure, also termed honestly significant difference (HSD), for all pairwise differences. We can obtain control on the type I error by looking at what happens between the minimum and maximum group averages under the null.\nScheffe’s method for contrasts. This is useful when the number of contrasts of interest is not specified apriori.\n\nIf the global \\(F\\)-test does not find differences at level \\(\\alpha\\), then Scheffe’s method will also find no significant contrast \\(\\alpha\\) but nothing can be said about other methods. Generally, the more tests we control the type error for, the more conservative the procedures are.\nIn R, we can use the multcomp or emmeans packages for the tests to adjust, or compute results manually. The test statistics do not change, only the benchmark null distribution is different. Figure 4.2 shows what the \\(p\\)-value would be depending on how we control for contrasts. For reasonable values, we get larger \\(p\\)-values for the methods that provide control.\n\n\n\n\n\n\n\n\nFigure 4.2: P-value as a function of the squared t-statistic for a contrast for no adjustment (full line), Tukey’s HSD (dashed line) and Scheffe’s adjustment (dotted).\n\n\n\n\n\n\nExample 4.4 (Multiple testing for paper or plastic) Sokolova, Krishna, and Döring (2023) considered pairwise difference relative to the control where only plastic wrapping is used. We could use either Bonferroni, Holm–Bonferoni or Dunnett’s method. Since the \\(p\\)-values are tiny (less than \\(10^{-4}\\)), this has no impact on the conclusions whatsoever. To better appreciate the impact in small samples, we subsample 20 observation per group to inflate \\(p\\)-values. We can also see differences by inspecting the width of the confidence intervals for the pairwise differences to the reference group: more conservative references lead to wider intervals.\n\ndata(SKD23_S2A, package = \"hecedsm\") # load data\nset.seed(80667) # Set seed for reproducibility\nSKD23_S2A_sub &lt;- SKD23_S2A |&gt;\n  # Create a categorical variable (factor) and ensure reference is 0\n  # By default, it would be (first alphanumerical value of labels)\n  dplyr::mutate(propfact = relevel(factor(proportion), ref = \"0\")) |&gt;\n  # Sample only fourty observations by group -\n  # for illustration purposes only, otherwise p-values are too small\n  dplyr::slice_sample(n = 20, by = propfact)\nanovamod &lt;- lm(pef ~ propfact, data = SKD23_S2A_sub) \nlibrary(emmeans)\nmargmean &lt;- emmeans(\n  anovamod, # fitted model\n  # 'specs': vector with names of factors to adjust for\n  specs = \"propfact\") \ncontrastlist &lt;- list( # specify contrast vectors\n  refvshalf = c(1, -1, 0, 0),\n  refvsone =  c(1, 0, -1, 0),\n  refvstwo =  c(1, 0, 0, -1))\ncontrasts &lt;- margmean |&gt; contrast(method = contrastlist)\n# Bonferroni and Holm-Bonferroni adjustments\nsummary(contrasts, adjust = \"bonferroni\")\n\n contrast  estimate    SE df t.ratio p.value\n refvshalf   -0.450 0.331 76  -1.359  0.5346\n refvsone    -1.150 0.331 76  -3.473  0.0026\n refvstwo    -0.662 0.331 76  -2.001  0.1470\n\nP value adjustment: bonferroni method for 3 tests \n\nsummary(contrasts, adjust = \"holm\")\n\n contrast  estimate    SE df t.ratio p.value\n refvshalf   -0.450 0.331 76  -1.359  0.1782\n refvsone    -1.150 0.331 76  -3.473  0.0026\n refvstwo    -0.662 0.331 76  -2.001  0.0980\n\nP value adjustment: holm method for 3 tests \n\n# Note that the p-values for the latter are equal or smaller\n\n# Adjustments for ANOVA to get simultaneous statements\n# Number of groups minus 1 for Scheffe (correct here)\n# This 'rank' often needs to be manually specified in multi-way ANOVA\nsummary(contrasts, adjust = \"scheffe\", scheffe.rank = 3)\n\n contrast  estimate    SE df t.ratio p.value\n refvshalf   -0.450 0.331 76  -1.359  0.6070\n refvsone    -1.150 0.331 76  -3.473  0.0104\n refvstwo    -0.662 0.331 76  -2.001  0.2696\n\nP value adjustment: scheffe method with rank 3 \n\n# This would be the better option here\nsummary(contrasts, adjust = \"dunnett\")\n\n contrast  estimate    SE df t.ratio p.value\n refvshalf   -0.450 0.331 76  -1.359  0.3934\n refvsone    -1.150 0.331 76  -3.473  0.0025\n refvstwo    -0.662 0.331 76  -2.001  0.1260\n\nP value adjustment: dunnettx method for 3 tests \n\n# The less you adjust for, the smaller the p-values\n# For Tukey, use 'contrast(method = \"pairwise\")' instead\n\n# Since we have a small number of pairwise comparisons\n# We could use the less stringent of Holm-Bonferroni and Dunnett's\n# The latter provides shorter intervals here.\ncontrasts |&gt; confint(adjust = \"dunnett\")\n\n contrast  estimate    SE df lower.CL upper.CL\n refvshalf   -0.450 0.331 76    -1.25    0.348\n refvsone    -1.150 0.331 76    -1.95   -0.352\n refvstwo    -0.662 0.331 76    -1.46    0.135\n\nConfidence level used: 0.95 \nConf-level adjustment: dunnettx method for 3 estimates \n\ncontrasts |&gt; confint(adjust = \"holm\")\n\n contrast  estimate    SE df lower.CL upper.CL\n refvshalf   -0.450 0.331 76    -1.26    0.361\n refvsone    -1.150 0.331 76    -1.96   -0.339\n refvstwo    -0.662 0.331 76    -1.47    0.148\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \n\n\nWe can see that more stringent adjustments lead to higher \\(p\\)-values and wider intervals.\n\nIf we wanted to perform tests for multiple variables, or for subgroups, we can obtain overall control by using a procedure in each subset with a lower \\(\\alpha\\), and combining the overall errors afterwards. If the data arise from different independent samples, the tests are indeed independent.\n\n\n\n\nBaumann, James F., Nancy Seifert-Kessell, and Leah A. Jones. 1992. “Effect of Think-Aloud Instruction on Elementary Students’ Comprehension Monitoring Abilities.” Journal of Reading Behavior 24 (2): 143–72. https://doi.org/10.1080/10862969209547770.\n\n\nNosek, Brian, Johanna Cohoon, Mallory Kidwell, and Jeffrey Spies. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251). https://doi.org/10.1126/science.aac4716.\n\n\nSokolova, Tatiana, Aradhna Krishna, and Tim Döring. 2023. “Paper Meets Plastic: The Perceived Environmental Friendliness of Product Packaging.” Journal of Consumer Research 50 (3): 468–91. https://doi.org/10.1093/jcr/ucad008.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contrasts and multiple testing</span>"
    ]
  },
  {
    "objectID": "contrasts_multipletesting.html#footnotes",
    "href": "contrasts_multipletesting.html#footnotes",
    "title": "4  Contrasts and multiple testing",
    "section": "",
    "text": "Should you ever need the formula, the standard error assuming subsample size of \\(n_1, \\ldots, n_K\\) and a common variance \\(\\sigma^2\\) is \\(\\sqrt{\\mathsf{Va}(\\widehat{C})}\\), where \\[\\mathsf{Va}(\\widehat{C}) = \\widehat{\\sigma}^2\\left(\\frac{c_1^2}{n_1} + \\cdots + \\frac{c_K^2}{n_K}\\right).\\]↩︎\nThe constraint \\(c_1 + \\cdots + c_K=0\\) ensures that linear contrasts are orthogonal to the mean, which has weight \\(c_i=n_i/n\\) and for balanced samples \\(c_i =1/n\\).↩︎\nThese would be formulated at registration time, but for the sake of the argument we proceed as if they were.↩︎\nThis is the case if tests are based on different data, or if the contrasts considered are orthogonal under normality.↩︎\nThe second line holds with independent observations, the second follows from the use of Boole’s inequality and does not require independent tests.↩︎\nBy definition, the expected number of false positive (PFER) is \\(\\mathsf{E}(V) = \\sum_{i=1}^m i \\Pr(V=i) \\geq \\sum_{i=1}^m \\Pr(V=i) = \\Pr(V \\geq 1)\\), so larger than the probability of making at least type 1 error. Thus, any procedure that controls the per-family error rate (e.g., Bonferroni) also automatically bounds the familywise error rate.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Contrasts and multiple testing</span>"
    ]
  },
  {
    "objectID": "twoway.html",
    "href": "twoway.html",
    "title": "5  Complete factorial designs",
    "section": "",
    "text": "5.1 Efficiency of multiway analysis of variance.\nWe next consider experiments and designs in which there are multiple factors being manipulated by the experimenter simultaneously. Before jumping into the statistical analysis, let us discuss briefly some examples that will be covered in the sequel.\nConsider the setting of Sharma, Tully, and Cryder (2021) and suppose we want to check the impact of debt and collect a certain number of observations in each group. If we suspected the label had an influence, we could run a one-way analysis of variance for each spending type separately (thus, two one-way ANOVA each with two groups). We could do likewise if we wanted instead to focus on whether the spending was discretionary in nature or not, for each label: together, this would give a total of eight sets of observations. Combining the two factors allows us to halve the number of groups/samples we collect in this simple setting: this highlights the efficiency of running an experiment modifying all of these instances at once, over a series of one-way analysis of variance. This concept extends to higher dimension when we manipulate two or more factors. Factorial designs allow us to study the impact of multiple variables simultaneously with fewer overall observations.\nThe drawback is that as we increase the number of factors, the total number of subgroups increases: with a complete design1 and with factors \\(A\\), \\(B\\), \\(C\\), etc. with \\(n_a\\), \\(n_b\\), \\(n_c\\), \\(\\ldots\\) levels, we have a total of \\(n_a\\times n_b \\times n_c \\times \\cdots\\) combinations and the number of observations needed to efficiently measure the group means increases quickly. This is the curse of dimensionality: the larger the number of experimental treatments manipulated together, the larger the sample size needed. A more efficient approach, which we will cover in later section, relies on measuring multiple observations from the same experimental units, for example by giving multiple tasks (randomly ordered) to participants.\nIntrinsically, the multiway factorial design model description does not change relative to a one-way design: the analysis of variance describes the sample mean for the response in each subgroup,\nConsider a two-way analysis of variance model. This is a linear model with two factors, \\(A\\) and \\(B\\), with respectively \\(n_a\\) and \\(n_b\\) levels. The response \\(Y_{ijk}\\) of the \\(k\\)th measurement in group \\((a_i, b_j)\\) is \\[\n\\underset{\\text{response}\\vphantom{b}}{Y_{ijk}} = \\underset{\\text{subgroup mean}}{\\mu_{ij}} + \\underset{\\text{error term}}{\\varepsilon_{ijk}}\n\\tag{5.1}\\] where\nThis, it turns out, is a special case of linear regression model. We could build contrasts for comparing group averages, but it will more convenient to reparametrize the model so that hypotheses of interest are directly expressed in terms of the parameters.\nFor example, in the Maglio and Polman (2014) study, we could gather observations for each factor combination in a table, where direction is the row and station the column.\nThe \\(i\\)th row mean represents the average response across all levels of \\(B\\), \\(\\mu_{i.} = (\\mu_{i1} + \\cdots + \\mu_{in_b})/n_b\\) and similarly for the average of the \\(j\\)th column, \\(\\mu_{.j} = (\\mu_{1j} + \\cdots + \\mu_{n_aj})/n_a.\\) Finally, the overall average is \\[\\mu = \\frac{\\sum_{i=1}^{n_a} \\sum_{j=1}^{n_b} \\mu_{ij}}{n_an_b}.\\]\nEach subgroup average \\(\\mu_{ij}\\) will be estimated as the sample mean of observations in their group and we would use the above formulae to obtain estimates of the row, column and overall means \\(\\widehat{\\mu}_{i.}\\), \\(\\widehat{\\mu}_{.j}\\) and \\(\\widehat{\\mu}\\). If the sample is balanced, meaning the number of observations is the same, these will be the same as summing over all observations in a row, column or table and then averaging. In general setup, however, we will give equal weight to each subgroup average.\nTable 5.2: Repartition of the sample for Study 1 of Maglio and Polman (2014).\n\n\n\n\n\n\n\nSpadina\nSt. George\nBloor-Yonge\nSherbourne\n\n\n\n\neast\n26\n26\n23\n26\n\n\nwest\n25\n25\n26\n25\nLooking at Table 5.2, we can see that the number of observations is not exactly the same. In general, attrition and non-response can lead to unequal cell sample size, but you should strive to gather roughly equal number of observations. The main consequence is that different decompositions of the variance will lead to different tests, whereas no such ambiguity exists for balanced data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Complete factorial designs</span>"
    ]
  },
  {
    "objectID": "twoway.html#efficiency-of-multiway-analysis-of-variance.",
    "href": "twoway.html#efficiency-of-multiway-analysis-of-variance.",
    "title": "5  Complete factorial designs",
    "section": "",
    "text": "\\(Y_{ijk}\\) is the \\(k\\)th replicate for \\(i\\)th level of factor \\(A\\) and \\(j\\)th level of factor \\(B\\)\n\\(\\mu_{ij}\\) is the average response of measurements in group \\((a_i, b_j)\\)\n\\(\\varepsilon_{ijk}\\) are independent error terms with mean zero and standard deviation \\(\\sigma\\).\n\n\n\n\n\n\nTable 5.1: Conceptual depiction of cell average for the two by two design of Maglio and Polman (2014)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\) station\n\\(B\\) direction\n\\(b_1\\) (east)\n\\(b_2\\) (west)\nrow mean\n\n\n\n\n\\(a_1\\) (Spadina)\n\n\\(\\mu_{11}\\)\n\\(\\mu_{12}\\)\n\\(\\mu_{1.}\\)\n\n\n\\(a_2\\) (St. George)\n\n\\(\\mu_{21}\\)\n\\(\\mu_{22}\\)\n\\(\\mu_{2.}\\)\n\n\n\\(a_3\\) (Bloor-Yonge)\n\n\\(\\mu_{31}\\)\n\\(\\mu_{32}\\)\n\\(\\mu_{3.}\\)\n\n\n\\(a_4\\) (Sherbourne)\n\n\\(\\mu_{41}\\)\n\\(\\mu_{42}\\)\n\\(\\mu_{4.}\\)\n\n\ncolumn mean\n\n\\(\\mu_{.1}\\)\n\\(\\mu_{.2}\\)\n\\(\\mu\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Complete factorial designs</span>"
    ]
  },
  {
    "objectID": "twoway.html#interactions",
    "href": "twoway.html#interactions",
    "title": "5  Complete factorial designs",
    "section": "5.2 Interactions",
    "text": "5.2 Interactions\nTable 5.1 shows the individual mean of each subgroup. From these, we may be interested in looking at the experiment as a single one-way analysis of variance model with eight subgroups, or as a series of one-way analysis of variance with either direction or station as sole factor.\nWe will use particular terminology to refer to these:\n\nsimple effects: difference between levels of one in a fixed combination of others. Simple effects are comparing cell averages within a given row or column.\nmain effects: differences relative to average for each condition of a factor. Main effects are row/column averages.\ninteraction effects: when simple effects differ depending on levels of another factor. Interactions effects are differences relative to the row or column average.\n\nIn other words, an interaction occurs when some experimental factors, when coupled together, have different impacts than the superposition of each. An interaction between two factors occurs when the average effect of one independent variable depends on the level of the other.\nIf there is a significant interaction, the main effects are not of interest since they are misleading. Rather, we will compute the simple effects by making the comparison one at level at the time.\nIn our example of Maglio and Polman (2014), a simple effect would be comparing the distance between Spadina and Sherbourne for east. The main effect for the direction would be the average perceived distance for east and for west. Finally, the interaction would measure how much these differ by station depending on direction.\n\n\n\n\n\n\n\n\nFigure 5.1: Interaction plots (line graphs) for example patterns for means for each of the possible kinds of general outcomes in a 2 by 2 design. Illustration adapted from Figure 10.2 of Crump, Navarro, and Suzuki (2019) by Matthew Crump (CC BY-SA 4.0 license).\n\n\n\n\n\nTo better understand, we consider the average response and suppose we have access to the true population average for each sub-treatment. We can then represent the population using a line graph with the two factors, one being mapped to color and another to the \\(x\\)-axis. Figure 5.1 shows what happens under all possible scenarios with a 2 by 2 design. When there is no overall effect, the mean is constant. If there isn’t a main effect of \\(A\\), the average of the two mean response for \\(a_1\\) and \\(a_2\\) are the same, etc. Interactions are depicted by non-parallel lines.\nIt’s clear from Figure 5.1 that looking only at the average of \\(A\\) alone (the main effect) isn’t instructive when we are in the presence of an interaction: rather, we should be comparing the values of \\(A\\) for \\(b_1\\) separately than those for \\(b_2\\), and vice-versa, using simple effects, otherwise our conclusions may be misleading.\n\nExample 5.3 (Interaction plots for Maglio and Polman (2014)) The hypothesis of interest is the interaction; for the time being, we can simply plot the average per group. Since the summary statistics can hide important information such as the uncertainty, we add 95% confidence intervals for the subgroup averages and superimpose jittered observations to show the spread of the data. Based on Figure 5.2, there appears to be at least an interaction between station and direction of travel, in addition to a main effect for station. Formal hypothesis testing can help check this intuition.\n\n\n\n\n\n\n\n\nFigure 5.2: Interaction plot for Study 1 of Maglio and Polman (2014), showing group averages and 95% confidence intervals for the means. Observations are overlaid on the graph.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Complete factorial designs</span>"
    ]
  },
  {
    "objectID": "twoway.html#model-parametrization",
    "href": "twoway.html#model-parametrization",
    "title": "5  Complete factorial designs",
    "section": "5.3 Model parametrization",
    "text": "5.3 Model parametrization\nThe following section is technical and may be omitted.\nThe model parametrized in terms of subgroup or cell average is okay in Equation 5.1, but it doesn’t help us if we want to check for the presence of main effects and interaction, even if it would be possible to specify the contrasts required to test these hypotheses. We can however express the model in terms of main effects and interactions.\nWe consider the alternative formulation \\[\\begin{align*}\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk},\n\\end{align*}\\] where\n\n\\(\\mu\\) is the average of all subgroup averages, termed overall mean.\n\\(\\alpha_i = \\mu_{i.} - \\mu\\) is the mean of level \\(A_i\\) minus the overall mean.\n\\(\\beta_j  = \\mu_{.j} - \\mu\\) is the mean of level \\(B_j\\) minus overall mean.\n\\((\\alpha\\beta)_{ij} = \\mu_{ij} - \\mu_{i.} - \\mu_{.j} + \\mu\\) is the interaction term for \\(A_i\\) and \\(B_j\\) which encodes the effect of both variable not already captured by the main effects.\n\nA rapid calculation shows that there are more coefficients than the number of cells and subgroups (\\(n_an_b\\) cells overall) in our table. The model is overparametrized: to get away with this, we impose constraints to remove redundancies. The idea is that if we know \\(n_a-1\\) of the mean for factor \\(A\\) and the global average is a combination of these, we can deduce the value for the last row mean. The model formulation in terms of difference from the global average or main effect ensures that we can test for main effects for factor \\(A\\) by setting \\(\\mathscr{H}_0: \\alpha_1 = \\cdots = \\alpha_{n_a-1}=0\\). The \\(1 +  n_a + n_b\\) sum to zero constraints, \\[\\sum_{i=1}^{n_a} \\alpha_i=0, \\quad \\sum_{j=1}^{n_b} \\beta_j=0, \\quad  \\sum_{j=1}^{n_b} (\\alpha\\beta)_{ij}=0, \\quad \\sum_{i=1}^{n_a} (\\alpha\\beta)_{ij}=0,\\] restore identifiability.\nThe redundancy in information, due to the fact main effects are expressible as row and column averages, and the overall mean as the average of all observations, will arise again when we consider degrees of freedom for tests.\nTo be continued…\n\nExample 5.4 (Testing for Psychological ownership of borrowed money) Sharma, Tully, and Cryder (2021) first proceeded with the test for the interaction. Since there are one global average and two main effect (additional difference in average for both factors debttype and purchase), the interaction involves one degree of freedom since we go from a model with three parameters describing the mean to one that has a different average for each of the four subgroups.\nThe reason why this is first test to carry out is that if the effect of one factor depends on the level of the other, as shown in Figure 5.1, then we need to compare the label of debt type separately for each type of purchase and vice-versa using simple effects. If the interaction on the contrary isn’t significant, then we could look at main effects by pooling observations and averaging across either of the two factors, resulting in marginal comparisons.\nFitting the model including the interaction between factors ensures that we keep the additivity assumption and that our conclusions aren’t misleading: the price to pay is additional mean parameters to be estimated, which isn’t an issue if you collect enough data, but can be critical when data collection is extremely costly and only a few runs are allowed.\nIn R, we include both factors in a formula as response ~ factorA * factorB, the * symbol indicating that both are allowed to interact, as a shorthand for factorA + factorB + factorA:factorB; in the main effect model, we would use instead + to reflect that the effects of both factors add up.\n\n# Analysing Supplementary Study 5\n# of Sharma, Tully, and Cryder (2021)\ndata(STC21_SS5, package = \"hecedsm\")\nmod &lt;- aov(likelihood ~ purchase*debttype, \n           data = STC21_SS5)\nmodel.tables(mod, type = \"means\")\n\nTables of means\nGrand mean\n         \n4.879747 \n\n purchase \n    discretionary    need\n            4.182   5.579\nrep       751.000 750.000\n\n debttype \n     credit    loan\n      5.127   4.631\nrep 753.000 748.000\n\n purchase:debttype \n               debttype\npurchase        credit loan \n  discretionary   4.5    3.8\n  rep           392.0  359.0\n  need            5.7    5.4\n  rep           361.0  389.0\n\n# Analysis of variance reveals \n# non-significant interaction\n# of purchase and type\ncar::Anova(mod, type = 2)\n\nAnova Table (Type II tests)\n\nResponse: likelihood\n                   Sum Sq   Df F value    Pr(&gt;F)    \npurchase            752.3    1 98.2066 &lt; 2.2e-16 ***\ndebttype             92.2    1 12.0363 0.0005365 ***\npurchase:debttype    13.7    1  1.7852 0.1817132    \nResiduals         11467.4 1497                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Main effects\nemmeans::emmeans(mod, \n                 specs = \"debttype\",\n                 contr = \"pairwise\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n$emmeans\n debttype emmean    SE   df lower.CL upper.CL\n credit     5.12 0.101 1497     4.93     5.32\n loan       4.63 0.101 1497     4.43     4.83\n\nResults are averaged over the levels of: purchase \nConfidence level used: 0.95 \n\n$contrasts\n contrast      estimate    SE   df t.ratio p.value\n credit - loan    0.496 0.143 1497   3.469  0.0005\n\nResults are averaged over the levels of: purchase \n\n# Pairwise comparisons within levels of purchase\n# Simple effect\nemmeans::emmeans(mod, \n                 specs = c(\"purchase\", \"debttype\"),\n                 by = \"purchase\",\n                 contr = \"pairwise\")\n\n$emmeans\npurchase = discretionary:\n debttype emmean    SE   df lower.CL upper.CL\n credit     4.51 0.140 1497     4.24     4.78\n loan       3.82 0.146 1497     3.54     4.11\n\npurchase = need:\n debttype emmean    SE   df lower.CL upper.CL\n credit     5.74 0.146 1497     5.45     6.02\n loan       5.43 0.140 1497     5.16     5.71\n\nConfidence level used: 0.95 \n\n$contrasts\npurchase = discretionary:\n contrast      estimate    SE   df t.ratio p.value\n credit - loan    0.687 0.202 1497   3.398  0.0007\n\npurchase = need:\n contrast      estimate    SE   df t.ratio p.value\n credit - loan    0.305 0.202 1497   1.508  0.1318\n\n\nIn the analysis of variance table, we focus exclusively on the last line with the sum of squares for purchase:debttype. The \\(F\\) statistic is 1.79; using the \\(\\mathsf{F}\\) (1, 1497) distribution as benchmark, we obtain a \\(p\\)-value of 0.18 so there is no evidence the effect of purchase depends on debt type.\nWe can thus pool data and look at the effect of debt type (loan or credit) overall by combining the results for all purchase types, one of the planned comparison reported in the Supplementary material. To do this in R with the emmeans package, we use the emmeans function and we quote the factor of interest (i.e., the one we want to keep) in specs. By default, this will compute the estimate marginal means: the contr = \"pairwise\" indicates that we want the difference between the two, which gives us the contrasts.\nTo get the simple effects, we give both variables in specs as factors for which to compute subgroup means, then set additionally the by command to specify which variable we want separate results for. We get the difference in average between credit and loan labels for each purchase type along with the \\(t\\) statistics for the marginal contrast and the \\(p\\)-value. The simple effects suggest that the label has an impact on perception only for discretionary expenses rather than needed ones, which runs counter-intuitively with the lack of interaction.\n\n\nMaglio and Polman (2014) considered the relative perception of distance from Bay station in Toronto. We modify the data so that we consider station distance in direction of travel (rather than station names). The categorical variable stdist has labels \\((-2, -1, +1, +2\\)) for stations Spadina, St. Georges, Bloor-Yonge, Sherbourne in direction East, and opposite signs in the other direction: see Figure 5.3 for the map. We are interested in knowing whether two stations behind (stdist\\(=-2\\)) is perceived the same as two stations ahead (stdist\\(=+2\\)).\n\n\n\n\n\n\n\n\nFigure 5.3: Simplified depiction of the Toronto metro stations used in the experiment, based on work by Craftwerker on Wikipedia, distributed under CC-BY-SA 4.0.\n\n\n\n\n\n\n# Set up parametrization to sum-to-zero for categorical factors\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nlibrary(emmeans)\nmod &lt;- lm(distance ~ stdist * direction, data = MP14_S1) \ncar::Anova(mod, type = 2)\n\n\n\n\n\nTable 5.3: Analysis of variance (type II effects) for the Maglio and Polman (2014) reformated data.\n\n\n\n\n\n\nterm\nsum of squares\ndf\nstat\np-value\n\n\n\n\nstdist\n121.87\n3\n37.86\n&lt;0.001\n\n\ndirection\n0.38\n1\n0.35\n0.55\n\n\nstdist:direction\n5.70\n3\n1.77\n0.15\n\n\nResiduals\n208.15\n194\n\nNA\n\n\n\n\n\n\n\n\nWe look at the analysis of variance in Table 5.3 to see what the perception of distance is. The \\(F\\)-tests suggest that there is no interaction, and no effect of direction of travel although there is an uninteresting main effect of station distance (of course, two station apart is considered further from Bay than one station apart).\nSince there is no interaction, we can collapse the data to a one-way ANOVA with a single factor (station distance) and consider contrasts. Say we are interested in testing the perception of distance, by looking at average distance of pairs at equal distance \\(\\mu_{-1} = \\mu_{+1}\\) and \\(\\mu_{-2} = \\mu_{+2}\\).\n\n\n\n\n\n\n\n\nFigure 5.4: Interaction plot for the reformated data from Maglio and Polman (2014) with 95% confidence intervals.\n\n\n\n\n\nIf we have categories are in the order \\((-2, -1, +1, +2)\\), the contrast weights are \\((-1, 0, 0, 1)\\) and \\((0, -1, 1, 0)\\) or a multiple thereof; the two contrasts are orthogonal. Table 5.4 shows the result of the hypothesis tests: both are significant, even applying a Bonferroni correction. This supports the hypothesis of Maglio and Polman (2014).\n\n\n\n\nTable 5.4: Contrasts for comparing the perceived distance for stations at the same distance on the network, but in opposition directions to that of travel.\n\n\n\n\n\n\ncontrast\nestimate\nstd. error\nstat\np-value\n\n\n\n\ntwo dist\n-1.12\n0.21\n-5.47\n&lt;0.001\n\n\none dist\n-0.86\n0.21\n-4.13\n&lt;0.001\n\n\n\n\n\n\n\n\n\n\nExample 5.5 (Precision of anchors and subjective adjustments) We consider data from a replication by Chandler (2016) of Study 4a of Janiszewski and Uy (2008). Both studies measured the amount of adjustment when presented with vague or precise range of value for objects, with potential encouragement for adjusting more the value. Chandler (2016) described the effect in the replication report:\n\nJaniszewski and Uy (2008) conceptualized people’s attempt to adjust following presentation of an anchor as movement along a subjective representation scale by a certain number of units. Precise numbers (e.g. $9.99) imply a finer-resolution scale than round numbers (e.g. $10). Consequently, adjustment along a subjectively finer resolution scale will result in less objective adjustment than adjustment by the same number of units along a subjectively coarse resolution scale.\n\nThe experiment is a 2 by 2 factorial design (two-way ANOVA) with anchor (either round or precise) and magnitude (0 for small, 1 for big adjustment) as experimental factors. A total of 120 students were recruited and randomly assigned to one of the four experimental sub-condition, for a total of 30 observations per subgroup (anchor, magnitude). The response variable is majust, the mean adjustment for the price estimate of the item.\n\n# Example of two-way ANOVA with balanced design\ndata(C16, package = \"hecedsm\")\n# Check for balance\nxtabs(formula = ~ anchor + magnitude,\n      data = C16)\n\n         magnitude\nanchor     0  1\n  round   30 30\n  precise 30 30\n\n# Fit two-way ANOVA model\nmod &lt;- aov(madjust ~ anchor * magnitude,\n           data = C16)\n# Analysis of variance table\nsummary(mod)\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nanchor             1  0.777   0.777   6.277   0.0136 *  \nmagnitude          1  8.796   8.796  71.058 1.09e-13 ***\nanchor:magnitude   1  0.002   0.002   0.013   0.9088    \nResiduals        116 14.359   0.124                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe dataset is available from the R package hecedsm as C16. We can see that there are 30 observations in each group in the replication, as advertised. In R, the function aov fits an analysis of variance model for balanced data. Analysis of variance are simple instances of linear regression models, and the main difference between fitting the model using aov and lm is the default parametrization used (aov uses sum-to-zero constraints by default, lm does not). In more general settings (including continuous covariates), we will use lm as a workshorse to fit the model, with an option to set up the contrasts so the output matches our expectations (and needs). The model is fitted as before by specifying the response ~ explanatories: the * notation is a shortcut to specify anchor + magnitude + anchor:magnitude, with the last term separated by a semi-colon : denoting an interaction between two variables. Here, the experimental factors anchor and magnitude are crossed, as it is possible to be in both experimental groups simultaneously.\nThe interaction.plot function in base R allows one to create an interaction (or profile) plot for a two-way design. More generally, we can simply compute the group means for each combination of the experimental conditions, map the mean response to the \\(y\\)-axis of a graph and add the experimental factors to other dimensions (\\(x\\)-axis, panel, color, etc.)\n\nC16 |&gt;\n ggplot(mapping = aes(x = anchor,\n                      y = madjust,\n                      color = magnitude)) +\n  geom_jitter(width = 0.1,\n              alpha = 0.1) +\n  stat_summary(aes(group = magnitude), \n               fun = mean, \n               geom = \"line\") +\n  # Change position of labels\n  labs(y = \"\",\n       subtitle = \"Mean adjustment\") +\n  theme_classic() + # change theme\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIn our example, the interaction plot shows a large main effect for magnitude, a smaller one for anchor and no evidence of interaction — despite the uncertainty associated with the estimation, the lines are very close to being parallel. Overlaying the jitter observations shows there is quite a bit of spread, but with limited overlap. Despite the graphical evidence hinting that the interaction isn’t significant, we will fit the two-way analysis of variance model with the interaction unless we invalidate our statistical inference. The emmip function allows one to return a plot automagically.\n\n# Interaction plot\nemmeans::emmip(mod, \n               magnitude ~ anchor, \n               CIs = TRUE) +\n  theme_minimal()\n\nWarning: `aes_()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`\nℹ The deprecated feature was likely used in the emmeans package.\n  Please report the issue at &lt;https://github.com/rvlenth/emmeans/issues&gt;.\n\n\n\n\n\n\n\n\n\nBecause our dataset is balanced, the marginal means (the summary statistics obtained by grouping the data for a single factor) and the marginal effects (obtained by calculating the average cell means by either row or column) will coincide. There are multiple functions that allow one to obtain estimates means for cells, rows or columns, including functionalities, notably emmeans from the eponymous package and model.tables\n\n# Get grand mean, cell means, etc.\nmodel.tables(mod, type = \"means\")\n\nTables of means\nGrand mean\n            \n0.001155778 \n\n anchor \nanchor\n   round  precise \n 0.08162 -0.07931 \n\n magnitude \nmagnitude\n       0        1 \n-0.26959  0.27190 \n\n anchor:magnitude \n         magnitude\nanchor    0       1      \n  round   -0.1854  0.3487\n  precise -0.3537  0.1951\n\n# Cell means\nemmeans(object = mod, \n        specs = c(\"anchor\", \"magnitude\"),\n        type = \"response\")\n\n anchor  magnitude emmean     SE  df lower.CL upper.CL\n round   0         -0.185 0.0642 116  -0.3127  -0.0582\n precise 0         -0.354 0.0642 116  -0.4810  -0.2265\n round   1          0.349 0.0642 116   0.2215   0.4759\n precise 1          0.195 0.0642 116   0.0679   0.3223\n\nConfidence level used: 0.95 \n\n# Marginal means\nemmeans(object = mod, \n        specs = \"anchor\", \n        type = \"response\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n anchor   emmean     SE  df lower.CL upper.CL\n round    0.0816 0.0454 116 -0.00834   0.1716\n precise -0.0793 0.0454 116 -0.16928   0.0107\n\nResults are averaged over the levels of: magnitude \nConfidence level used: 0.95 \n\n# These match summary statistics\nC16 |&gt;\n  group_by(magnitude) |&gt;\n  summarize(margmean = mean(madjust))\n\n# A tibble: 2 × 2\n  magnitude margmean\n  &lt;fct&gt;        &lt;dbl&gt;\n1 0           -0.270\n2 1            0.272\n\nC16 |&gt;\n  group_by(anchor) |&gt;\n  summarize(margmean = mean(madjust))\n\n# A tibble: 2 × 2\n  anchor  margmean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 round     0.0816\n2 precise  -0.0793\n\n\nSince the data are balanced, we can look at the (default) analysis of variance table produced using anova function2\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: madjust\n                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nanchor             1  0.7770  0.7770  6.2768   0.01362 *  \nmagnitude          1  8.7962  8.7962 71.0584 1.089e-13 ***\nanchor:magnitude   1  0.0016  0.0016  0.0132   0.90879    \nResiduals        116 14.3595  0.1238                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output confirms our intuition that there is not much different from zero, with a strong effect for magnitude of adjustment and a significant, albeit smaller one, for anchor type.\nWhile the conclusions are probably unambiguous due to the large evidence, it would be useful to check the model assumptions. The sample size is just enough to forego normality checks, but the quantile-quantile plot can be useful to detect outliers and extremes. Outside of one potential value much lower than it’s group mean, there is no cause for concern.\n\ncar::qqPlot(mod, id = FALSE)\n\n\n\n\n\n\n\n\nWith 30 observations per group and no appearance of outlier, we need rather to worry about additivity and possibly heterogeneity arising from the treatment. Independence is plausible based on the context.\nThe Tukey-Anscombe plot of residuals against fitted values (the group means) indicate no deviation, but the variance appears to be larger for the two groups with a large adjustment. Because the response takes negative values, we can simply proceed with fitting a two-way analysis in which each of the subgroups has mean \\(\\mu_{ij}\\) and standard deviation \\(\\sigma_{ij}\\): in other words, only the data for each subgroup (anchor, magnitude) are used to estimate the summary statistics of that group.\n\n# Evidence of unequal variance\nggplot(data = data.frame(residuals = resid(mod),\n                         fitted = fitted(mod)),\n       mapping = aes(x = fitted,\n                     y = residuals)) +\n   geom_jitter(width = 0.03, height = 0) +\n  theme_classic()\n\n\n\n\n\n\n\n# Equality of variance - Brown-Forsythe\ncar::leveneTest(mod) \n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)  \ngroup   3  2.8133 0.0424 *\n      116                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGiven the Brown-Forsythe test output, we can try fitting a different variance in each group, as there are enough observations for this. The function gls in the nlme package fits such models; the weight argument being setup with a constant variance (~1) per each combination of the crossed factors anchor * magnitude.\n\nlibrary(nlme)\n# Fit a variance per group\nmod2 &lt;- nlme::gls(\n  model = madjust ~ anchor * magnitude,\n  data = C16,\n  weights = nlme::varIdent(\n    form = ~ 1 | anchor * magnitude))\n\n# Different ANOVA - we use type II here\ncar::Anova(mod2, type = 2)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: madjust\n                 Df   Chisq Pr(&gt;Chisq)    \nanchor            1  6.7708   0.009266 ** \nmagnitude         1 79.9238  &lt; 2.2e-16 ***\nanchor:magnitude  1  0.0132   0.908595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see the unequal std. deviation per group when passing the model with unequal variance and unequal means and computing the estimated marginal means. The package emmeans automatically adjusts for these changes.\n\nemmeans(object = mod2, \n        specs = c(\"anchor\", \"magnitude\"))\n\n anchor  magnitude emmean     SE   df lower.CL upper.CL\n round   0         -0.185 0.0676 29.1  -0.3236  -0.0473\n precise 0         -0.354 0.0422 29.0  -0.4401  -0.2674\n round   1          0.349 0.0796 29.0   0.1859   0.5115\n precise 1          0.195 0.0618 29.2   0.0688   0.3215\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n# Compute pairwise difference for anchor\nmarg_effect &lt;- emmeans(object = mod2, \n        specs = \"anchor\") |&gt; \n  pairs()\n\nNOTE: Results may be misleading due to involvement in interactions\n\nmarg_effect\n\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.161 0.0642 100   2.505  0.0138\n\nResults are averaged over the levels of: magnitude \nDegrees-of-freedom method: satterthwaite \n\n# To get a data frame with data\n# broom::tidy(marg_effect)\n\nWe can then pass the output to car::Anova to print the analysis of variance table. The \\(p\\)-value for the main effect of anchor is 0.014 in the equal variance model. With unequal variance, different tests give different values: the \\(p\\)-value is 0.009 if we use type II effects (the correct choice here), 0.035 with type III effects3 and the emmeans package returns Welch’s test for the pairwise difference with Satterwaite’s degree of freedom approximation if we average over magnitude to account for the difference in variance, this time with a \\(p\\)-value of 0.014. These differences in output are somewhat notable: with borderline statistical significance, they may lead to different conclusions if one blindly dichotomize the results. Clearly stating which test and how the results are obtained is crucial for transparent reporting, as is providing the code and data. Let your readers make their own mind by reporting \\(p\\)-values.\nRegardless of the model, it should be clearly stated that there is some evidence of heterogeneity. We should also report sample size per group, mention the repartition (\\(n=30\\) per group). In the present case, we can give information about the main effects and stop here, but giving an indication about the size of the adjustment (by reporting estimated marginal means) is useful. Note that emmeans gives a (here spurious) warning about the main effects (row or column average) since there is a potential interaction — as we all but ruled out the latter, we proceed nevertheless.\n\nemm_marg &lt;- emmeans::emmeans(\n  object = mod2,\n  specs = \"anchor\"\n)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\nThere are many different options to get the same results with emmeans. The specs indicates the list of factors which we want to keep, whereas by gives the one we want to have separate analysis for. In formula, we could get the simple effects for anchor by level of magnitude using ~ anchor | magnitude, or set specs = anchor and by = magnitude. We can pass the result to pairs to obtain pairwise differences.\n\n# Simple effects for anchor\nemm_simple &lt;- emmeans::emmeans(\n  object = mod,\n  specs = \"anchor\",\n  by = \"magnitude\"\n)\n# Compute pairwise differences within each magnitude\npairs(emm_simple)\n\nmagnitude = 0:\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.168 0.0908 116   1.853  0.0665\n\nmagnitude = 1:\n contrast        estimate     SE  df t.ratio p.value\n round - precise    0.154 0.0908 116   1.690  0.0936\n\n\nBy default, emmeans will compute adjustments for pairwise difference using Tukey’s honest significant difference method if there are more than one pairwise comparison. The software cannot easily guess the degrees of freedom, the number of tests, etc. There are also tests which are not of interest: for example, one probably wouldn’t want to compute the difference between the adjustment for (small magnitude and round) versus (large magnitude and precise). If we were interested in looking at all pairwise differences, we could keep all of the cells means.\n\nemmeans(object = mod2, \n        specs = c(\"magnitude\", \"anchor\"), \n        contr = \"pairwise\")\n\n$emmeans\n magnitude anchor  emmean     SE   df lower.CL upper.CL\n 0         round   -0.185 0.0676 29.1  -0.3236  -0.0473\n 1         round    0.349 0.0796 29.0   0.1859   0.5115\n 0         precise -0.354 0.0422 29.0  -0.4401  -0.2674\n 1         precise  0.195 0.0618 29.2   0.0688   0.3215\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n$contrasts\n contrast                                estimate     SE   df t.ratio p.value\n magnitude0 round - magnitude1 round       -0.534 0.1040 56.7  -5.115  &lt;.0001\n magnitude0 round - magnitude0 precise      0.168 0.0797 48.7   2.112  0.1636\n magnitude0 round - magnitude1 precise     -0.381 0.0916 57.6  -4.156  0.0006\n magnitude1 round - magnitude0 precise      0.702 0.0901 44.2   7.795  &lt;.0001\n magnitude1 round - magnitude1 precise      0.154 0.1010 54.6   1.524  0.4306\n magnitude0 precise - magnitude1 precise   -0.549 0.0748 51.5  -7.333  &lt;.0001\n\nDegrees-of-freedom method: satterthwaite \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nNotice now the mention about Tukey’s effect. When there is heterogeneity of variance or unbalanced effects, the actual method employed is called Games-Howell correction.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nFactorial designs are more efficient than running repeatedly one-way analysis of variance with the same sample size per group.\nInteractions occur when the effect of a variable depends on the levels of the others.\nInteraction plots (group average per group) can help capture this difference, but beware of overinterpretation in small samples.\nIf there is an interaction, we consider differences and contrasts for each level of the other factor (simple effects).\nIf there is no interaction, we can pool observations and look at main effects.\n\n\n\n\n\n\n\nChandler, J. J. 2016. “Replication of Janiszewski & Uy (2008, Psychological Science, Study 4b).” https://osf.io/aaudl.\n\n\nCrump, M. J. C., D. J. Navarro, and J. Suzuki. 2019. Answering Questions with Data: Introductory Statistics for Psychology Students. https://doi.org/10.17605/OSF.IO/JZE52.\n\n\nMaglio, Sam J., and Evan Polman. 2014. “Spatial Orientation Shrinks and Expands Psychological Distance.” Psychological Science 25 (7): 1345–52. https://doi.org/10.1177/0956797614530571.\n\n\nSharma, Eesha, Stephanie Tully, and Cynthia Cryder. 2021. “Psychological Ownership of (Borrowed) Money.” Journal of Marketing Research 58 (3): 497–514. https://doi.org/10.1177/0022243721993816.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Complete factorial designs</span>"
    ]
  },
  {
    "objectID": "twoway.html#footnotes",
    "href": "twoway.html#footnotes",
    "title": "5  Complete factorial designs",
    "section": "",
    "text": "By complete design, it is meant that we gather observations for each subcategory.↩︎\nIn general, for unbalanced data, one would use car::Anova with type = 2 or type = 3 effects. The former (i.e., Type 2) is preferable because it respects the marginality principle.↩︎\nThe type 3 effects compare the model with interactions and main effects to one that includes the interaction, but removes the main effects. Not of interest in the present context.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Complete factorial designs</span>"
    ]
  },
  {
    "objectID": "repeated.html",
    "href": "repeated.html",
    "title": "6  Repeated measures",
    "section": "",
    "text": "6.1 Repeated measures\nSo far, all experiments we have considered can be classified as between-subject designs, meaning that each experimental unit was assigned to a single experimental (sub)-condition. In many instances, it may be possible to randomly assign multiple conditions to each experimental unit. For example, an individual coming to a lab to perform tasks in a virtual reality environment may be assigned to all treatments. There is an obvious benefit to doing so, as the participants can act as their own control group, leading to greater comparability among treatment conditions.\nFor example, consider a study performed at Tech3Lab that looks at the reaction time for people texting or talking on a cellphone while walking. We may wish to determine whether disengagement is slower for people texting, yet we may also postulate that some elderly people have slower reflexes.\nBy including multiple conditions, we can filter out the effect due to subject: this leads to increased precision of effect sizes and increased power (as we will see, hypothesis tests are based on within-subject variability). Together, this translates into the need to gather fewer observations or participants to detect a given effect in the population and thus experiments are cheaper to run.\nThere are of course drawbacks to gathering repeated measures from individuals. Because subjects are confronted with multiple tasks, there may be carryover effects (when one task influences the response of the subsequent ones, for example becoming better as manipulations go on), period effects (fatigue, a decrease in acuity), and permanent changes in the subject condition after a treatment or attrition (loss of subjects over time).\nTo minimize potential biases, there are multiple strategies one can use. Tasks are normally presented in random order among subjects to avoid confounding, or using a balanced crossover design and include the period and carryover effect in the statistical model via control variables so as to better isolate the treatment effect. The experimenter should also allow enough time between treatment conditions to reduce or eliminate period or carryover effects and plan tasks accordingly.\nIf each subject is assigned to an experimental condition only once, one good way to do this is via counterbalancing. We proceed as follows: first, enumerate all possible orders of the condition and then assign participants as equally as possible between conditions. For example, with a single within-factor design with three conditions \\(A, B, C\\), we have six possible orderings (either \\(ABC\\), \\(ACB\\), \\(BAC\\), \\(BCA\\), \\(CAB\\) or \\(CBA\\)). Much like other forms of randomization, this helps us remove confounding effects and let’s us estimate what is the average effect of task ordering on the response.\nThere are multiple approaches to handling repeated measures. The first option is to take averages over experimental condition per subject and treat them as additional blocking factors, but it may be necessary to adjust the resulting statistics. The second approach consists in fitting a multivariate model for the response and explicitly account for the correlation, otherwise the null distribution commonly used are off and so are the conclusions, as illustrated with the absurd comic displayed in Figure 13.1.\nWe introduce the concept of repeated measure and within-subject ANOVA with an example.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Repeated measures</span>"
    ]
  },
  {
    "objectID": "repeated.html#repeated-measures",
    "href": "repeated.html#repeated-measures",
    "title": "6  Repeated measures",
    "section": "",
    "text": "Example 6.1 (Happy fakes) We consider an experiment conducted in a graduate course at HEC, Information Technologies and Neuroscience, in which PhD students gathered electroencephalography (EEG) data. The project focused on human perception of deepfake image created by a generative adversarial network: Amirabdolahian and Ali-Adeeb (2021) expected the attitude towards real and computer generated image of people smiling to change.\nThe response variable is the amplitude of a brain signal measured at 170 ms after the participant has been exposed to different faces. Repeated measures were collected on 9 participants given in the database AA21, who were expected to look at 120 faces. Not all participants completed the full trial, as can be checked by looking at the cross-tabs of the counts\n\ndata(AA21, package = \"hecedsm\")\nxtabs(~stimulus + id, data = AA21)\n\n        id\nstimulus  1  2  3  4  5  6  7  8  9 10 11 12\n    real 30 32 34 32 38 29 36 36 40 30 39 33\n    GAN1 32 31 40 33 38 29 39 31 39 28 35 34\n    GAN2 31 33 37 34 38 29 34 36 40 33 35 32\n\n\nThe experimental manipulation is encoded in the stimuli, with levels control (real) for real facial images, whereas the others were generated using a generative adversarial network (GAN) with be slightly smiling (GAN1) or extremely smiling (GAN2); the latter looks more fake. While the presentation order was randomized, the order of presentation of the faces within each type is recorded using the epoch variable: this allows us to measure the fatigue effect.\nSince our research question is whether images generated from generative adversarial networks trigger different reactions, we will be looking at pairwise differences with the control.\n\n\n\n\n\n\n\n\n\n\n\n(a) real\n\n\n\n\n\n\n\n\n\n\n\n(b) slightly modified\n\n\n\n\n\n\n\n\n\n\n\n(c) extremely modified\n\n\n\n\n\n\n\nFigure 6.2: Examples of faces presented in Amirabdolahian and Ali-Adeeb (2021).\n\n\n\nWe could first grouping the data and compute the average for each experimental condition stimulus per participant and set id as blocking factor. The analysis of variance table obtained from aov would be correct, but would fail to account for correlation.\nThe one-way analysis of variance with \\(n_s\\) subjects, each of which was exposed to the \\(n_a\\) experimental conditions, can be written \\[\\begin{align*}\\underset{\\text{response}\\vphantom{l}}{Y_{ij}} = \\underset{\\text{global mean}}{\\mu_{\\vphantom{j}}} + \\underset{\\text{mean difference}}{\\alpha_j} + \\underset{\\text{subject difference}}{s_{i\\vphantom{j}}} + \\underset{\\text{error}\\vphantom{l}}{\\varepsilon_{ij}}\n\\end{align*}\\]\n\n# Compute mean for each subject + \n# experimental condition subgroup\nAA21_m &lt;- AA21 |&gt;\n  dplyr::group_by(id, stimulus) |&gt;\n  dplyr::summarize(latency = mean(latency))\n# Use aov for balanced sample\nfixedmod &lt;- aov(\n  latency ~ stimulus + Error(id/stimulus), \n  data = AA21_m)\n# Print ANOVA table\nsummary(fixedmod)\n\n\nError: id\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 11  187.8   17.07               \n\nError: id:stimulus\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nstimulus   2   1.94  0.9704   0.496  0.615\nResiduals 22  43.03  1.9557               \n\n\nSince the design is balanced after averaging, we can use aov in R: we need to specify the subject identifier within Error term. This approach has a drawback, as variance components can be negative if the variability due to subject is negligible. While aov is fast, it only works for simple balanced designs.\n\n\n6.1.1 Contrasts\nWith balanced data, the estimated marginal means coincide with the row averages. If we have a single replication or the average for each subject/condition, we could create a new column with the contrast and then fit a model with an intercept-only (global mean) to check whether the latter is zero. With 12 participants, we should thus expect our test statistic to have 11 degrees of freedom, since one unit is spent on estimating the mean parameter and we have 12 participants.\nUnfortunately, the emmeans package analysis for object fitted using aov will be incorrect: this can be seen by passing a contrast vector and inspecting the degrees of freedom. The afex package includes functionalities that are tailored for within-subject and between-subjects and has an interface with emmeans.\n\nafexmod &lt;- afex::aov_ez(\n  id = \"id\",           # subject id\n  dv = \"latency\",      # response variable\n  within = \"stimulus\", # within-subject factor\n  data = AA21,\n  fun_aggregate = mean)\n\nThe afex package has different functions for computing the within-subjects design and the aov_ez specification, which allow people to list within and between-subjects factor separately with subject identifiers may be easier to understand. It also has an argument, fun_aggregate, to automatically average replications.\n\n# Set up contrast vector\ncont_vec &lt;- list(\n  \"real vs GAN\" = c(1, -0.5, -0.5))\nlibrary(emmeans)\n# Correct output\nafexmod |&gt;\n  emmeans::emmeans(\n    spec = \"stimulus\", \n    contr = cont_vec)\n\n$emmeans\n stimulus emmean    SE df lower.CL upper.CL\n real      -10.8 0.942 11    -12.8    -8.70\n GAN1      -10.8 0.651 11    -12.3    -9.40\n GAN2      -10.3 0.662 11    -11.8    -8.85\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate    SE df t.ratio p.value\n real vs GAN   -0.202 0.552 11  -0.366  0.7213\n\n# Incorrect output - \n# note the wrong degrees of freedom\nfixedmod |&gt; \n  emmeans::emmeans(\n    spec = \"stimulus\", \n    contr = cont_vec)\n\nNote: re-fitting model with sum-to-zero contrasts\n\n\n$emmeans\n stimulus emmean    SE   df lower.CL upper.CL\n real      -10.8 0.763 16.2    -12.4    -9.15\n GAN1      -10.8 0.763 16.2    -12.4    -9.21\n GAN2      -10.3 0.763 16.2    -11.9    -8.69\n\nWarning: EMMs are biased unless design is perfectly balanced \nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate    SE df t.ratio p.value\n real vs GAN   -0.202 0.494 22  -0.409  0.6867\n\n\n\n\n6.1.2 Sphericity assumption\nThe validity of the \\(F\\) statistic null distribution relies on the model having the correct structure.\nIn repeated-measure analysis of variance, we assume again that each measurement has the same variance. We equally require the correlation between measurements of the same subject to be the same, an assumption that corresponds to the so-called compound symmetry model.1\nWhat if the within-subject measurements have unequal variance or the correlation between those responses differs?\nSince we care only about differences in treatment, can get away with a weaker assumption than compound symmetry (equicorrelation) by relying instead on sphericity, which holds if the variance of the difference between treatment is constant. Sphericity is not a relevant concept when there is only two measurements (as there is a single correlation); we could check this by comparing the fit of a model with an unstructured covariance (difference variances for each and correlations for each pair of variable)\nThe most popular approach to handling correlation in tests is a two-stage approach: first, check for sphericity (using, e.g., Mauchly’s test of sphericity). If the null hypothesis of sphericity is rejected, one can use a correction for the \\(F\\) statistic by modifying the parameters of the Fisher \\(\\mathsf{F}\\) null distribution used as benchmark.\nAn idea due to Box is to correct the degrees of freedom of the \\(\\mathsf{F}(\\nu_1, \\nu_2)\\) distribution by multiplying them by a common factor \\(\\epsilon&lt;1\\) and use \\(\\mathsf{F}(\\epsilon\\nu_1, \\epsilon\\nu_2)\\) as null distribution instead to benchmark our statistics and determine how extreme our observed one is. Since the \\(F\\) statistic is a ratio of variances, the \\(\\epsilon\\) terms would cancel. Using the scaled \\(\\mathsf{F}\\) distribution leads to larger \\(p\\)-values, thus accounting for the correlation.\nThere are three widely used corrections: Greenhouse–Geisser, Huynh–Feldt and Box correction, which divides by \\(\\nu_1\\) both degrees of freedom and gives a very conservative option. The Huynh–Feldt method is reported to be more powerful so should be preferred, but the estimated value of \\(\\epsilon\\) can be larger than 1.\nUsing the afex functions, we get the result for Mauchly’s test of sphericity and the \\(p\\) values from using either correction method\n\nsummary(afexmod)\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df  F value    Pr(&gt;F)    \n(Intercept) 4073.1      1  187.814     11 238.5554 8.373e-09 ***\nstimulus       1.9      2   43.026     22   0.4962    0.6155    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n         Test statistic p-value\nstimulus        0.67814 0.14341\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n          GG eps Pr(&gt;F[GG])\nstimulus 0.75651     0.5667\n\n            HF eps Pr(&gt;F[HF])\nstimulus 0.8514944  0.5872648\n\n\n\nExample 6.2 (Enjoyment from waiting) We consider Experiment 3 from Hatano et al. (2022). The data consist in a two by two mixed analysis of variance. The authors studied engagement and enjoyment from waiting tasks, and “potential effects of time interval on the underestimation of task motivation by manipulating the time for the waiting task”. The waiting time was randomly assigned to either short (3 minutes) or long (20 minutes) with equal probability, but participants were either told that there was a 70% chance of being assigned to the short session, or 30% chance. We first load the data from the package and inspect the content.\n\n\ntibble [126 × 4] (S3: tbl_df/tbl/data.frame)\n $ id        : Factor w/ 63 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 4 4 5 5 ...\n $ waiting   : Factor w/ 2 levels \"long\",\"short\": 2 2 1 1 1 1 2 2 1 1 ...\n $ ratingtype: Factor w/ 2 levels \"experience\",\"prediction\": 2 1 2 1 2 1 2 1 2 1 ...\n $ imscore   : num [1:126] 2.33 3.33 1.25 1.92 1 ...\n\n\nwaiting\n long short \n   32    31 \n\n\nFrom this, we can see that there each student is assigned to a single waiting time, but that they have both rating types. Since there are 63 students, the study is unbalanced but by a single person; this may be due to exclusion criteria. We use the afex package (analysis of factorial design) with the aov_ez function to fit the model in R. We need to specify the identifier of the subjects (id), the response variable (dv) and both between- (between) and within-subject (within) factors. Each of those names must be quoted.\n\n\nAnova Table (Type 3 tests)\n\nResponse: imscore\n                   num Df den Df     MSE       F      ges    Pr(&gt;F)    \nwaiting                 1     61 2.48926 11.2551 0.126246   0.00137 ** \nratingtype              1     61 0.68953 38.4330 0.120236 5.388e-08 ***\nwaiting:ratingtype      1     61 0.68953  0.0819 0.000291   0.77575    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output includes the \\(F\\)-tests for the two main effects and the interaction and global effect sizes \\(\\widehat{\\eta}^2\\) (ges). There is no output for tests of sphericity, since there are only two measurements per person and thus a single mean different within-subject (so the test to check equality doesn’t make sense with a single number). We could however compare variance between groups using Levene’s test. Note that the degrees of freedom for the denominator of the test are based on the number of participants, here 63.\n\n\n\n\n\nHatano, A., C. Ogulmus, H. Shigemasu, and K. Murayama. 2022. “Thinking about Thinking: People Underestimate How Enjoyable and Engaging Just Waiting Is.” Journal of Experimental Psychology: General 151 (12): 3213–29. https://doi.org/10.1037/xge0001255.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Repeated measures</span>"
    ]
  },
  {
    "objectID": "repeated.html#footnotes",
    "href": "repeated.html#footnotes",
    "title": "6  Repeated measures",
    "section": "",
    "text": "Note that, with two measurements, there is a single correlation parameter to estimate and this assumption is irrelevant.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Repeated measures</span>"
    ]
  },
  {
    "objectID": "multiway.html",
    "href": "multiway.html",
    "title": "7  Multiway factorial designs",
    "section": "",
    "text": "This chapter focuses on models with a mix of within-subject and between-subject factors. As the number of factor increases, so does the number of categories: this explains why \\(2^p\\) designs, where each factor has two options, are typically employed.\nAny multiway ANOVA with two or more factor can be collapsed into a single one-way ANOVA: this is notably useful when there is a control group which is not related to the factor levels, as no manipulation takes place. The use of contrasts becomes critical since we can write any test for main effects, interactions using the latter through weighting.\n\nExample 7.1 (Perceptions of cultural appropriation by ideology) We consider a three-way ANOVA from Lin et al. (2024). Their Study 4 focused on cultural appropriation for soul food recipe cookbook from Chef Dax, who was either black (or not), manipulating the description of the way he obtained the recipes (by peeking without permission in kitchens, by asking permission or no mention for control). Authors postulated that the perception of appropriation would vary by political ideology (liberal or conservative). The study results in a 3 by 2 by 2 three-way between-subject ANOVA.\nFor the \\(K\\)-way ANOVA, we always start with estimating the full model with all \\(K\\)-way interaction (provided there are enough data to estimate the latter, which implies there are repetitions). If the latter is significant, we can fix one or more factor levels and compare the others.\n\n\n\n\nTable 7.1: Analysis of variance table (type II decomposition) for the data from Study 4 of Lin et al. (2024).\n\n\n\n\n\n\nterm\nsum of squares\ndf\nstat\np-value\n\n\n\n\npolitideo\n48.49\n1\n21.35\n&lt;0.001\n\n\nchefdax\n473.72\n1\n208.61\n&lt;0.001\n\n\nbrandaction\n34.24\n2\n7.54\n&lt;0.001\n\n\npolitideo:chefdax\n65.00\n1\n28.63\n&lt;0.001\n\n\npolitideo:brandaction\n1.56\n2\n0.34\n0.71\n\n\nchefdax:brandaction\n0.62\n2\n0.14\n0.87\n\n\npolitideo:chefdax:brandaction\n0.66\n2\n0.15\n0.86\n\n\nResiduals\n1587.33\n699\n\nNA\n\n\n\n\n\n\n\n\nIf we consider Table 7.1, we find that there is no three-way interaction and, omitting the latter and focusing on lower-level, a single two-way interaction between political ideology and the race of Chef Dax. We cannot interpret the \\(p\\)-value for the main effect of brandaction, but we could look at the marginal means.\nBased on the data, we will collapse data to a one-way ANOVA comparing the three levels of brandaction and a 2 by 2 two-way ANOVA for the other two factors. The results are obtained by averaging over the missing factor.\nWe are interested in comparing the perception between the race of Chef Dax (black or not, as Southern Soul food cooking is more likely to be associated with cultural appropriation if Chef Dax is not black. We proceed with emmeans by computing the marginal means separately for each of the four subcategories, but compare the race of Chef Dax separately for liberals and conservatives due to the presence of the interaction.\n\ndata(LKUK24_S4, package = \"hecedsm\")\nlibrary(emmeans)\nmod &lt;- lm(appropriation ~ politideo * chefdax * brandaction,\n   data = LKUK24_S4)\n# Marginal means for political ideology/Chef Dax\nemm_racebypolit &lt;- emmeans(mod, specs = \"chefdax\", by = \"politideo\")\nemm_racebypolit |&gt; pairs() #shortcut for contrast(\"pairwise\")\n\npolitideo = conservative:\n contrast          estimate    SE  df t.ratio p.value\n not black - black     0.71 0.206 699   3.438  0.0006\n\npolitideo = liberal:\n contrast          estimate    SE  df t.ratio p.value\n not black - black     2.03 0.135 699  14.998  &lt;.0001\n\nResults are averaged over the levels of: brandaction \n\n\nWe see that the liberals are much more likely to view Chef Dax cookbook as an instance of cultural appropriation if he is not black; there is limited evidence of any difference between conservatives and liberal when Chef Dax is black. Both differences are statistically significative, but the differences (and thus evidence of an effect) is much stronger for left-leaning respondents.\nWe can look next at the brand action: we expect participants will view peeking less favorably than if Chef Dax asked for permission to publish the recipes. It’s tricky to know the effect of the control, as we are not bringing the point to the attention of participants in this instance.\n\n# Marginal mean for brandaction\nemm_brand &lt;- emmeans(mod, specs = c(\"brandaction\")) \nemm_brand\n\n brandaction emmean    SE  df lower.CL upper.CL\n peeking       2.56 0.108 699     2.35     2.77\n permission    2.29 0.105 699     2.09     2.50\n control       2.07 0.108 699     1.86     2.28\n\nResults are averaged over the levels of: politideo, chefdax \nConfidence level used: 0.95 \n\n# Joint test for the main effect of brandaction\nemm_brand |&gt; pairs() |&gt; joint_tests()\n\n model term df1 df2 F.ratio p.value\n contrast     2 699   5.091  0.0064\n\n\nA joint \\(F\\)-test, obtained by collapsing everything to a one-way ANOVA, shows that there are indeed differences. However, note that the averages of the three actions are much smaller than for race.\n\n\nExample 7.2 (Visual acuity) We consider a model with both within-subject and between-subject factors. Data for a study on visual acuity of participants. The data represent the number of words correctly detected at different font size; interest is in effect of illusory contraction on detection. The mixed analysis of variance includes the experimental factors adaptation (2 levels, within), fontsize (4 levels, within), position (5 levels, within) and visual acuity (2 levels, between). There are a total of 1760 measurements for 44 participants in LBJ17_S1A, balanced. The within-subject factors give a total of 40 measurements (\\(2 \\times 4 \\times 5\\)) per participant; all of these factors are crossed and we can estimate interactions for them. The subjects are nested within visual acuity groups, The participants were dichotomized in two groups based on their visual acuity, obtained from preliminary checks, using a median split.\nTo fit the model, we rely on the aov_ez function from afex. By default, the latter includes all interactions.\n\nLBJ_mod &lt;- afex::aov_ez(\n  id = \"id\",     # subject id\n  dv = \"nerror\", # response\n  between = \"acuity\",\n  within = c(\"adaptation\",\n             \"fontsize\", \n             \"position\"),\n  data = hecedsm::LBJ17_S1A)\nanova_tbl &lt;- anova(LBJ_mod,  # model\n      correction = \"none\", # no correction for sphericity\n      es = \"pes\") \n#partial eta-square for effect sizes (es)\n\n\n\n\n\nTable 7.2: Analysis of variance for the four-way model with partial effect sizes (partial eta-square)\n\n\n\n\n\n\n\ndf1\ndf2\nF\npes\np-value\n\n\n\n\nacuity\n1\n42\n30.8\n0.42\n&lt;0.001\n\n\nadaptation\n1\n42\n7.8\n0.16\n0.008\n\n\nacuity:adaptation\n1\n42\n12.7\n0.23\n&lt;0.001\n\n\nfontsize\n3\n126\n1705.7\n0.98\n&lt;0.001\n\n\nacuity:fontsize\n3\n126\n10.0\n0.19\n&lt;0.001\n\n\nposition\n4\n168\n9.4\n0.18\n&lt;0.001\n\n\nacuity:position\n4\n168\n4.2\n0.09\n0.003\n\n\nadaptation:fontsize\n3\n126\n3.3\n0.07\n0.023\n\n\nacuity:adaptation:fontsize\n3\n126\n7.0\n0.14\n&lt;0.001\n\n\nadaptation:position\n4\n168\n0.6\n0.01\n0.662\n\n\nacuity:adaptation:position\n4\n168\n0.9\n0.02\n0.464\n\n\nfontsize:position\n12\n504\n9.1\n0.18\n&lt;0.001\n\n\nacuity:fontsize:position\n12\n504\n2.7\n0.06\n0.002\n\n\nadaptation:fontsize:position\n12\n504\n0.5\n0.01\n0.907\n\n\nacuity:adaptation:fontsize:position\n12\n504\n1.2\n0.03\n0.295\n\n\n\n\n\n\n\n\n\n\nThis is the most complicated model we tested so far: there are four experimental factor being manipulated at once, and all interactions of order two, three and four are included!\nThe fourth order interaction isn’t statistically significant: this means that we can legitimately marginalize over and look at each of the four three-way ANOVA designs in turn. We can also see that the third order interaction adaptation:fontsize:position and acuity:adaptation:position are not really meaningful.\nThe following paragraph is technical and can be skipped. One difficult bit with designs including both within-subject and between-subject factors is the degrees of freedom and the correct sum of square terms to use to calculate the \\(F\\) statistics for each hypothesis of interest. The correct setup is to use the next sum of square (and the associated degrees of freedom) from this. For any main effect or interaction, we count the number of instances of this particular (e.g., 10 for the interaction between position and adaptation). We subtract the number of mean parameter used to estimate means and differences in mean (1 global mean, 4 means for position, 1 for adaptation), which gives \\(4=10-6\\) degrees of freedom. Next, this term is compared to the mean square which contains only subject (here via acuity levels, since subjects are nested within acuity) and the corresponding variables; the correct mean square is for acuity:adaptation:position. In the balanced design setting, this can be formalized using Hasse diagram (Oehlert 2000).\nWe can produce an interaction plot to see what comes out: since we can’t draw in four dimensions, we map visual acuity and adaptation level to panels with different colours for the position. The figure looks different from the paper, seemingly because their \\(y\\)-axis is flipped.\n\n\n\n\n\n\n\n\nFigure 7.1: Interaction plot for visual acuity levels.\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\nA multiway analysis of variance can be treated as a one-way analysis of variance by collapsing categories; however, only specific contrasts will be of interest.\nThe number of observations increases quickly with the dimension as we increase the number of factors considered.\n\n\n\n\n\n\n\nLin, Jason D, Nicole You Jeung Kim, Esther Uduehi, and Anat Keinan. 2024. “Culture for Sale: Unpacking Consumer Perceptions of Cultural Appropriation.” Journal of Consumer Research. https://doi.org/10.1093/jcr/ucad076.\n\n\nOehlert, Gary. 2000. A First Course in Design and Analysis of Experiments. W. H. Freeman. http://users.stat.umn.edu/~gary/Book.html.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Multiway factorial designs</span>"
    ]
  },
  {
    "objectID": "manova.html",
    "href": "manova.html",
    "title": "8  Multivariate analysis of variance",
    "section": "",
    "text": "8.1 Data format\nMultivariate analysis of variance (MANOVA) leads to procedures that are analogous to univariate analysis of variance, but we now need to estimate correlation and variance parameters for each measurement separately and there are multiple potential statistics that can be defined for testing effects. While we can benefit from the correlation and find differences that wouldn’t be detected from univariate models, the additional parameters to estimate lead to a loss of power. Finally, the most popular method nowadays for handling repeated measures is to fit a mixed model, with random effects accounting to subject-specific characteristics. By doing so, we assume that the levels of a factor (here the subject identifiers) form a random sample from a large population. These models can be difficult to fit and one needs to take great care in specifying the model.\nThe second paradigm for modelling is to specify that the response from each subject is in fact a multivariate object: we can combine all measurements from a given individual in a vector \\(\\boldsymbol{Y}\\). In the example with the happy fakes, this would be the tuple of measurements for (real, GAN1, GAN2).\nThe multivariate analysis of variance model is designed by assuming observations follow a (multivariate) normal distribution with mean vector \\(\\boldsymbol{\\mu}_j\\) in group \\(j\\) and common covariance matrix \\(\\boldsymbol{\\Sigma}\\) and comparing means between groups. As in univariate analysis of variance, the multivariate normal assumption holds approximately by virtue of the central limit theorem in large samples, but the convergence is slower and larger numbers are needed to ensure this is valid.\nThe difference with the univariate approach is now that we will compare a global mean vector \\(\\boldsymbol{\\mu}\\) between comparisons. In the one-way analysis of variance model with an experimental factor having \\(K\\) levels and a balanced sample \\(n_g\\) observations per group and \\(n=n_gK\\) total observations, we assume that each group has average \\(\\boldsymbol{\\mu}_k\\) \\((k=1, \\ldots, K)\\), which we can estimate using only the observations from that group. Under the null hypothesis, all groups have the same mean, so the estimator is the overall mean \\(\\boldsymbol{\\mu}\\) combining all \\(n\\) observations.\nThe statistic is obtained by decomposing the total variance around the global mean into components due to the different factors and the leftover variability. Because these equivalent to the sum of square decomposition results in multiple matrices, there are multiple ways of constructing test statistics. Wilk’s \\(\\Lambda\\) is the most popular choice. Another common choice, which leads to a statistic giving lower power but which is also more robust to departure from model assumptions is Pillai’s trace.\nThe MANOVA model assumes that the covariance matrices are the same within each experimental condition. We can use Box’s \\(M\\) statistic to test the normality hypothesis.\nWith repeated measures, it is sometimes convenient to store measurements associated to each experimental condition in different columns of a data frame or spreadsheet, with lines containing participants identifiers. Such data are said to be in wide format, since there are multiple measurements in each row. While this format is suitable for storate, many statistical routines will instead expect data to be in long format, for which there is a single measurement per line. Figure 8.1 illustrates the difference between the two formats.\nFigure 8.1: Long versus wide-format for data tables (illustration by Garrick Aden-Buie).\nIdeally, a data base in long format with repeated measures would also include a column giving the order in which the treatments were assigned to participants. This is necessary in order to test whether there are fatigue or crossover effects, for example by plotting the residuals after accounting for treatment subject by subject, ordered over time. We could also perform formal tests by including time trends in the model and checking whether the slope is significant.\nOverall, the biggest difference with within-subject designs is that observations are correlated whereas we assumed measurements were independent until now. This needs to be explicitly accounted for, as correlation has an important impact on testing as discussed Section 3.4.4: failing to account for correlation leads to \\(p\\)-values that are much too low. To see why, think about a stupid setting under which we duplicate every observation in the database: the estimated marginal means will be the same, but the variance will be halved despite the fact there is no additional information. Intuitively, correlation reduces the amount of information provided by each individual: if we have repeated measures from participants, we expect the effective sample size to be anywhere between the total number of subjects and the total number of observations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate analysis of variance</span>"
    ]
  },
  {
    "objectID": "manova.html#mathematical-complement",
    "href": "manova.html#mathematical-complement",
    "title": "8  Multivariate analysis of variance",
    "section": "8.2 Mathematical complement",
    "text": "8.2 Mathematical complement\nThis section is technical and can be omitted. Analogous to the univariate case, we can decompose the variance estimator in terms of within, between and total variance. Let \\(\\boldsymbol{Y}_{ik}\\) denote the response vector for the \\(i\\)th observation of group \\(k\\); then, we can decompose the variance as \\[\\begin{align*} &\n\\underset{\\text{total variance}}{\\sum_{k=1}^K \\sum_{i=1}^{n_g} (\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}})(\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}})^\\top} \\\\\\qquad &= \\underset{\\text{within variance}}{\\sum_{k=1}^K \\sum_{i=1}^{n_g} (\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}}_k)(\\boldsymbol{Y}_{ik} - \\widehat{\\boldsymbol{\\mu}}_k)^\\top} + \\underset{\\text{between variance}}{\\sum_{k=1}^K n_g(\\boldsymbol{\\mu}_{k} - \\widehat{\\boldsymbol{\\mu}})(\\widehat{\\boldsymbol{\\mu}}_k - \\widehat{\\boldsymbol{\\mu}})^\\top}\n\\end{align*}\\] defining covariance matrix estimators. If we write \\(\\widehat{\\boldsymbol{\\Sigma}}_T,\\) \\(\\widehat{\\boldsymbol{\\Sigma}}_W\\), and \\(\\widehat{\\boldsymbol{\\Sigma}}_B\\) for respectively the total, within and between variance estimators, we can build a statistic from these ingredients to see how much variability is induced by centering using a common vector. When \\(K&gt;2\\), there are multiple statistics that be constructed, including\n\nWilk’s \\(\\Lambda\\): \\(|\\widehat{\\boldsymbol{\\Sigma}}_W|/|\\widehat{\\boldsymbol{\\Sigma}}_W + \\widehat{\\boldsymbol{\\Sigma}}_B|\\)\nRoy’s maximum root: the largest eigenvalue of \\(\\widehat{\\boldsymbol{\\Sigma}}_W^{-1}\\widehat{\\boldsymbol{\\Sigma}}_B\\)\nLawley–Hotelling trace: \\(\\mathrm{tr}(\\widehat{\\boldsymbol{\\Sigma}}_W^{-1}\\widehat{\\boldsymbol{\\Sigma}}_B)\\)\nPillai’s trace: \\(\\mathrm{tr}\\left\\{\\widehat{\\boldsymbol{\\Sigma}}_B(\\widehat{\\boldsymbol{\\Sigma}}_W +\\widehat{\\boldsymbol{\\Sigma}}_B)^{-1}\\right\\}\\).\n\nAll four criteria lead to equivalent statistics and the same \\(p\\)-values if \\(K=2\\).\nWith a two-way balanced MANOVA, we can perform a similar decomposition for each factor or interaction, with \\[\\widehat{\\boldsymbol{\\Sigma}}_T = \\widehat{\\boldsymbol{\\Sigma}}_A + \\widehat{\\boldsymbol{\\Sigma}}_B + \\widehat{\\boldsymbol{\\Sigma}}_{AB} + \\widehat{\\boldsymbol{\\Sigma}}_W.\\]\nWilk’s \\(\\Lambda\\) is based on taking the ratio of the determinant of the within-variance and that of the sum of effect-variance plus within-variance, e.g., \\(|\\widehat{\\boldsymbol{\\Sigma}}_{AB} + \\widehat{\\boldsymbol{\\Sigma}}_W|\\) for the interaction term.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate analysis of variance</span>"
    ]
  },
  {
    "objectID": "manova.html#model-fitting",
    "href": "manova.html#model-fitting",
    "title": "8  Multivariate analysis of variance",
    "section": "8.3 Model fitting",
    "text": "8.3 Model fitting\nWe can treat the within-subject responses as a vector of observations and estimate the model using using multivariate linear regression. Contrary to the univariate counterpart, the model explicitly models the correlation between observations from the same subject.\nIn order to fit a model with a multivariate response, we first need to pivot the data into wider format so as to have a matrix with rows for the number of subjects and \\(M\\) columns for the number of response variables.\nOnce the data are in a suitable format, we fit the multivariate model with the lm function using the sum-to-zero constraints, here imposed globally by changing the contrasts option. Syntax-wise, the only difference with the univariate case is that the response on the left of the tilde sign (~) is now a matrix composed by binding together the vectors with the different responses.\n\nExample 8.1 (A multivariate take on “Happy fakes”) We use the data from Amirabdolahian and Ali-Adeeb (2021), but this time treating the averaged repeated measures for the different stimulus as a multivariate response. We first pivot the data to wide format, then fit the multivariate linear model.\n\ndata(AA21, package = \"hecedsm\")\n# Compute mean per subject\nAA21_m &lt;- AA21 |&gt;\n  dplyr::group_by(id, stimulus) |&gt;\n  dplyr::summarize(latency = mean(latency))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\n# Pivot to wide format (one individual, multiple measurements per line)\nAA21_mw &lt;- AA21_m |&gt;\n  tidyr::pivot_wider(names_from = stimulus, # within-subject factor labels\n                     values_from = latency) # response measurements \n# Model with each variable with a different mean\n# Specify all columns with column bind \n# left of the ~, following \noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nmlm &lt;- lm(cbind(real, GAN1, GAN2) ~ 1,\n          data = AA21_mw)\n\nSince the within-subject factor stimulus disappeared when we consider the multivariate response, we only specify a global mean vector \\(\\boldsymbol{\\mu}\\) via ~1. In general, we would add the between-subject factors to the right-hand side of the equation. Our hypothesis of equal mean translates into the hypothesis \\(\\boldsymbol{\\mu} = \\mu\\boldsymbol{1}_3\\), which can be imposed using a call to anova. The output returns the statistic and \\(p\\)-values including corrections for sphericity.\nWe can also use emmeans to set up post-hoc contrasts. Since we have no variable, we need to set in specs the repeated measure variable appearing on the left hand side of the formula; the latter is labelled rep.meas by default.\n\n# Test the multivariate model against\n# equal mean (X = ~1)\nanova(mlm, X = ~1, test = \"Spherical\")\n\nAnalysis of Variance Table\n\n\nContrasts orthogonal to\n~1\n\nGreenhouse-Geisser epsilon: 0.7565\nHuynh-Feldt epsilon:        0.8515\n\n            Df      F num Df den Df  Pr(&gt;F)  G-G Pr  H-F Pr\n(Intercept)  1 0.4962      2     22 0.61549 0.56666 0.58726\nResiduals   11                                             \n\n# Follow-up contrast comparisons\nlibrary(emmeans)\nemm_mlm &lt;- emmeans(mlm, specs = \"rep.meas\") \nemm_mlm |&gt; contrast(method = list(c(1,-0.5,-0.5)))\n\n contrast         estimate    SE df t.ratio p.value\n c(1, -0.5, -0.5)   -0.202 0.552 11  -0.366  0.7213\n\n\nWe can check that the output is the same in this case as the within-subject analysis of variance model fitted previously with the afex package.\n\n\nExample 8.2 (Teaching to read) We consider a between-subject repeated measure multivariate analysis of variance model with the Baumann, Seifert-Kessell, and Jones (1992). The data are balanced by experimental condition and they include the results of three tests performed after the intervention: an error detection task, an expanded comprehension monitoring questionnaire and a cloze test. Note that the scale of the tests are different (16, 18 and 56).\nWe could obtain the estimated covariance matrix of the fitted model by extracting the residuals \\(Y_{ik} - \\widehat{\\mu}_k\\) and computing the empirical covariance. The results shows a strong dependence between tests 1 and 3 (correlation of 0.39), but much weaker dependence with test 2.\nLet us compute the multivariate analysis of variance model\n\ndata(BSJ92, package = \"hecedsm\")\n# Force sum-to-zero parametrization\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n# Fit MANOVA model\nmmod &lt;- lm(\n  cbind(posttest1, posttest2, posttest3) ~ group,\n   data = BSJ92)\n# Calculate multivariate test\nmtest &lt;- car::Anova(mmod, test = \"Wilks\")\n# mtest\n# Get all statistics and univariate tests\nsummary(car::Anova(mmod), univariate = TRUE)\n\n\nType II MANOVA Tests:\n\nSum of squares and products for error:\n          posttest1  posttest2 posttest3\nposttest1 640.50000   30.77273  498.3182\nposttest2  30.77273  356.40909 -104.3636\nposttest3 498.31818 -104.36364 2511.6818\n\n------------------------------------------\n \nTerm: group \n\nSum of squares and products for the hypothesis:\n           posttest1 posttest2 posttest3\nposttest1 108.121212  6.666667 190.60606\nposttest2   6.666667 95.121212  56.65152\nposttest3 190.606061 56.651515 357.30303\n\nMultivariate Tests: group\n                 Df test stat approx F num Df den Df     Pr(&gt;F)    \nPillai            2 0.4082468 5.300509      6    124 6.7654e-05 ***\nWilks             2 0.6320200 5.243287      6    122 7.7744e-05 ***\nHotelling-Lawley  2 0.5185169 5.185169      6    120 8.9490e-05 ***\nRoy               2 0.3184494 6.581288      3     62 0.00062058 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Type II Sums of Squares\n          df posttest1 posttest2 posttest3\ngroup      2    108.12    95.121     357.3\nresiduals 63    640.50   356.409    2511.7\n\n F-tests\n      posttest1 posttest2 posttest3\ngroup      5.32      8.41      4.48\n\n p-values\n      posttest1  posttest2  posttest3 \ngroup 0.00734676 0.00058043 0.01515115\n\n\nBy default, we get Pillai’s trace statistic. Here, there is clear evidence of differences between groups of observations regardless of the statistic being used.\nWe can compute effect size as before by passing the table, for example using eta_squared(mtest) to get the effect size of the multivariate test, or simple the model to get the individual variable effect sizes.\nHaving found a difference, one could in principle investigate for which component of the response they are by performing univariate analysis of variance and accounting for multiple testing using, e.g., Bonferroni’s correction. A more fruitful avenue if you are trying to discriminate is to use descriptive discriminant analysis as a follow-up, which computes the best fitting hyperplanes that separate groups.\n\nMASS::lda(group ~ posttest1 + posttest2 + posttest3,\n          data = BSJ92)\n\nThis amounts to compute the weights \\(\\boldsymbol{w}\\) such, that, computing \\(\\boldsymbol{w}^\\top\\boldsymbol{Y}\\) creating a composite score by adding up weighted components that leads to maximal separation between groups. Figure 8.2 shows the new coordinates.\n\n\n\n\n\n\n\n\nFigure 8.2: Scatterplot of observations projected onto the linear discriminants for the post-experiment tests, by group.\n\n\n\n\n\nLinear discriminant analysis is a topic on it’s own that is beyond the scope of the course.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate analysis of variance</span>"
    ]
  },
  {
    "objectID": "manova.html#model-assumptions",
    "href": "manova.html#model-assumptions",
    "title": "8  Multivariate analysis of variance",
    "section": "8.4 Model assumptions",
    "text": "8.4 Model assumptions\nIn addition to the usual model assumptions (independence of measurements from different subjects, equal variance, additivity, etc.), the MANOVA model adds two hypothesis that altogether determine how reliable our \\(p\\)-values and conclusions are.\nThe first assumption is that of multivariate normality of the response. The central limit theorem can be applied to a multivariate response, but the sample size needed overall to reliably estimate the correlation and variance is larger than in the univariate setting. This hypothesis can be tested using the Shapiro-Wilk normality test (null hypothesis is that of normality) by passing the residuals of the multivariate model. Such a test can lead to rejection of the null hypothesis when specific variables are far from normal, or when the dependence structure isn’t the one exhibited by a multivariate normal model. With decent sample sizes (say \\(n=50\\) per group), this assumption isn’t as important as others.\n\n# Shapiro-Wilk normality test\n# Must transpose the residuals \n# to get a 3 by n matrix\nmvnormtest::mshapiro.test(U = t(resid(mmod)))\n\n\n    Shapiro-Wilk normality test\n\ndata:  Z\nW = 0.96464, p-value = 0.05678\n\n\nThe second assumption is that the covariance matrix is the same for all individuals, regardless of their experimental group assignment. We could try checking whether a covariance model in each group: under multivariate normal assumption, this leads to a test statistic called Box’s \\(M\\) test. Unfortunately, this test is quite sensitive to departures from the multivariate normal assumption and, if the \\(p\\)-value is small, it may have to do more with the normality than the heterogeneity.\n\nwith(BSJ92, \n     biotools::boxM(\n       data = cbind(posttest1, posttest2, posttest3),\n       grouping = group))\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  cbind(posttest1, posttest2, posttest3)\nChi-Sq (approx.) = 15.325, df = 12, p-value = 0.2241\n\n\nIn our example, there is limited evidence against any of those model assumptions. We should of course also check the assumptions of the analysis of variance model for each of postest1, posttest2 and posttest3 in turn; such a check is left as an exercice to the reader.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate analysis of variance</span>"
    ]
  },
  {
    "objectID": "manova.html#power-and-effect-size",
    "href": "manova.html#power-and-effect-size",
    "title": "8  Multivariate analysis of variance",
    "section": "8.5 Power and effect size",
    "text": "8.5 Power and effect size\nSince all of the multivariate statistics can be transformed for a comparison with a univariate \\(\\mathsf{F}\\) distribution, we can estimate partial effect size as before. The package effectsize offers a measure of partial \\(\\widehat{\\eta}^2\\) for the multivariate tests.1\nPower calculations are beyond the reach of ordinary software as one needs to specify the variance of each observation, their correlation and their mean. Simulation is an obvious way for this kind of design to obtain answers, but the free G\\({}^{*}\\)Power software (Faul et al. 2007) also offers some tools. See also Läuter (1978) for pairwise comparisons: to achieve a power of 80%, we need the following number of replicates per group \\(j=1, \\ldots, J\\), which shows that the number increases rapidly with the dimension of the response vector \\(p\\). As usual, smaller effect sizes are more difficult to detect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 groups\n\n\n4 groups\n\n\n5 groups\n\n\n\neffect size \\ p\n2\n4\n6\n8\n2\n4\n6\n8\n2\n4\n6\n8\n\n\n\n\nvery large\n13\n16\n18\n21\n14\n18\n21\n23\n16\n21\n24\n27\n\n\nlarge\n26\n33\n38\n42\n29\n37\n44\n48\n34\n44\n52\n58\n\n\nmedium\n44\n56\n66\n72\n50\n64\n74\n84\n60\n76\n90\n100\n\n\nsmall\n98\n125\n145\n160\n115\n145\n165\n185\n135\n170\n200\n230\n\n\n\n\n\n\n\n\nExample 8.3 (Disclosure formats for companies) The data presented in this example vignette is inspired by a study from Anandarajan, Viger, and Curatola (2002), who looked at the impact of communication means through different disclosure format on the perceived risk of organization on the brink of bankruptcy in accountancy. There is a single between-subject factor for the disclore format, and three measures of the performance, ratings for the interest rate premium assessed, for the ability to service debt and that to improve profitability. We first load the data from the package and inspect the content.\n\ndata(AVC02, package = \"hecedsm\")\nstr(AVC02)\n\ntibble [132 × 4] (S3: tbl_df/tbl/data.frame)\n $ format       : Factor w/ 3 levels \"integrated note\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ prime        : num [1:132] 0.5 1.5 0.5 0.25 1.5 1.25 2 0.5 1.5 0.5 ...\n $ debt         : int [1:132] 3 4 1 4 3 3 3 5 2 2 ...\n $ profitability: int [1:132] 3 2 2 5 2 2 2 3 2 3 ...\n\nxtabs(~ format, data = AVC02)\n\nformat\n        integrated note        stand-alone note modified auditor report \n                     40                      45                      47 \n\n\nThe data are unbalanced by condition. In general, we need them to be roughly balanced to maximize power. The manova function will not be usable unless data are balanced, and we need to enforce sum-to-zero constraints to get sensible outputs. After this is done, we can fit the multivariate linear model with lm by binding columns on the left of the ~ sign to gather the response vectors.\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nmodel &lt;- lm(cbind(prime, debt, profitability) ~ format, \n            data = AVC02)\n\nWe can check the residual correlation matrix to see if there was a strong dependence between our measurements. The benefit of MANOVA is to be able to leverage this correlation, if any.\n\ncor(resid(model))\n\n\n\n              prime  debt profitability\nprime          1.00 -0.40         -0.54\ndebt          -0.40  1.00          0.65\nprofitability -0.54  0.65          1.00\n\n\nWe can look at the global mean for each variable and the estimated mean differences for all groups, including the reference which is omitted. It’s easy to see that the mean differences sum to zero.\n\ndummy.coef(model)\n\nFull coefficients are \n                                                               \n(Intercept):             prime        1.280511                 \n                          debt        2.881521                 \n                 profitability        2.597695                 \nformat:                        integrated note stand-alone note\n                         prime    -0.099261229     -0.013844563\n                          debt     0.068479117     -0.059298660\n                 profitability     0.127304965      0.002304965\n                                       \n(Intercept):                           \n                                       \n                                       \nformat:         modified auditor report\n                            0.113105792\n                           -0.009180457\n                           -0.129609929\n\n\nNext, we compute the multivariate analysis of variance table and the follow-up with the univariate functions. By default, we can add a multiplicity correction for the tests, using Holm-Bonferonni with option 'holm'. For the MANOVA test, there are multiple statistics to pick from, including Pillai, Wilks, Hotelling-Lawley and Roy. The default is Pillai, which is more robust to departures from the model hypothesis, but Wilks is also popular choice among practitioners.\n\ncar::Manova(model, test = \"Pillai\")\n\n\nType II MANOVA Tests: Pillai test statistic\n       Df test stat approx F num Df den Df Pr(&gt;F)\nformat  2   0.02581  0.55782      6    256 0.7637\n\n\nWe can compute effect sizes overall for the MANOVA statistic using the correspondance with the \\(F\\) distribution, and also the individual effect size variable per variable. Here, the values returned are partial \\(\\widehat{\\eta}^2\\) measures.\n\neffectsize::eta_squared(car::Manova(model))\n\n# Effect Size for ANOVA (Type II)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nformat    |           0.01 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n# Since it's a one-way between-subject MANOVA, no partial measure\neffectsize::eta_squared(model, partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nResponse      | Parameter |     Eta2 |       95% CI\n---------------------------------------------------\nprime         |    format |     0.01 | [0.00, 1.00]\ndebt          |    format | 2.23e-03 | [0.00, 1.00]\nprofitability |    format |     0.02 | [0.00, 1.00]\n\n\nWe can continue with descriptive discriminant analysis for the post-hoc comparisons. To fit the model using the lda function from the MASS package, we swap the role of the experimental factor and responses in the formula. The output shows the weights for the linear combinations.\n\nMASS::lda(format ~ prime + debt + profitability,\n          data = AVC02)\n\nCall:\nlda(format ~ prime + debt + profitability, data = AVC02)\n\nPrior probabilities of groups:\n        integrated note        stand-alone note modified auditor report \n              0.3030303               0.3409091               0.3560606 \n\nGroup means:\n                           prime     debt profitability\nintegrated note         1.181250 2.950000      2.725000\nstand-alone note        1.266667 2.822222      2.600000\nmodified auditor report 1.393617 2.872340      2.468085\n\nCoefficients of linear discriminants:\n                     LD1         LD2\nprime          0.5171202  0.16521149\ndebt           0.6529450  0.97286350\nprofitability -1.2016530 -0.04412713\n\nProportion of trace:\n   LD1    LD2 \n0.9257 0.0743 \n\n\nInterpretation of these is beyond the scope of the course, but you can find information about linear discriminant analysis in good textbooks (e.g., Chapter 12 of Mardia, Kent, and Taylor 2024). The next step before writing about any of our conclusions is to check the model assumptions. As before, we could check for each variable in turn whether the variance are the same in each group. Here, we rather check equality of covariance matrix. The test has typically limited power, but unfortunately is very sensitive to departure from the multivariate normality assumption, so sometimes rejections are simply due to false positive.\n\nwith(AVC02,\nbiotools::boxM(cbind(prime, debt, profitability),\n               grouping = format))\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  cbind(prime, debt, profitability)\nChi-Sq (approx.) = 21.274, df = 12, p-value = 0.0465\n\n\nWe get a smallish \\(p\\)-value, and therefore weak evidence against equality of covariance matrices. The data were generated from normal distribution, so the small \\(p\\)-value is likely an artifact of the rounding of the Likert scale. We can test the normality assumption using univariate quantile-quantile plots or tests of normality, including Shapiro-Wilks.\n\n\n\n\n\n\n\n\n\nWe see severe rounding impacts for debt and profitability. There is little to be done about this, but the sample size are large enough that this shouldn’t be too much a concern. We can also test the multivariate normality assumption. The latter supposes that observations in each group have the same mean. To get this, we detrend using multivariate linear model by subtracting the mean of each group. Thus, our input is the matrix of residuals, which must be transposed for the function to work.\n\nmvnormtest::mshapiro.test(U = t(resid(model)))\n\n\n    Shapiro-Wilk normality test\n\ndata:  Z\nW = 0.96982, p-value = 0.004899\n\n\nThere is (unsurprisingly) strong evidence against multivariate normality, but it matters less due to sample size. This is a consequence of the discrete univariate measurements, which explain rejection of the null (for data to be multivariate normal, each of the response must be univariate normal and the dependence structure must also match. Since model assumptions are doubtful, we recommend using Pilai’s trace as test statistic for the MANOVA.\n\n\n\n\n\nAnandarajan, Asokan, Chantal Viger, and Anthony P. Curatola. 2002. “An Experimental Investigation of Alternative Going-Concern Reporting Formats: A Canadian Experience.” Canadian Accounting Perspectives 1 (2): 141–62. https://doi.org/10.1506/5947-NQTC-C3Y5-H46N.\n\n\nBaumann, James F., Nancy Seifert-Kessell, and Leah A. Jones. 1992. “Effect of Think-Aloud Instruction on Elementary Students’ Comprehension Monitoring Abilities.” Journal of Reading Behavior 24 (2): 143–72. https://doi.org/10.1080/10862969209547770.\n\n\nFaul, Franz, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner. 2007. “G*Power 3: A Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences.” Behavior Research Methods 39 (2): 175–91. https://doi.org/10.3758/BF03193146.\n\n\nLäuter, J. 1978. “Sample Size Requirements for the \\(T^2\\) Test of MANOVA (Tables for One-Way Classification).” Biometrical Journal 20 (4): 389–406. https://doi.org/10.1002/bimj.4710200410.\n\n\nMardia, Kanti V., John T. Kent, and Charles C. Taylor. 2024. Multivariate Analysis. Edited by Wiley. 2nd ed. Hoboken, NJ.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate analysis of variance</span>"
    ]
  },
  {
    "objectID": "manova.html#footnotes",
    "href": "manova.html#footnotes",
    "title": "8  Multivariate analysis of variance",
    "section": "",
    "text": "I must confess I haven’t checked whether the output is sensical.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Multivariate analysis of variance</span>"
    ]
  },
  {
    "objectID": "ancova.html",
    "href": "ancova.html",
    "title": "9  Linear regression models",
    "section": "",
    "text": "9.1 Linear models for factorial designs\nIf factors measure a continuous quantity, we may alternatively consider a statistical model that describes a curve or a straight line, rather than only determining the mean at the levels measured in an experiment. These types of experiments are rather common in engineering.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression models</span>"
    ]
  },
  {
    "objectID": "ancova.html#linear-models-for-factorial-designs",
    "href": "ancova.html#linear-models-for-factorial-designs",
    "title": "9  Linear regression models",
    "section": "",
    "text": "Example 9.1 (Is additional paper wrapping viewed as more eco-friendly?) Sokolova, Krishna, and Döring (2023) consider consumer bias when assessing how eco-friendly packages are. Items such as cereal are packaged in plastic bags, which themselves are covered in a box. They conjecture (and find) that, paradoxically, consumers tend to view the packaging as being more eco-friendly when the amount of cardboard or paper surrounding the box is larger, relative to the sole plastic package. We consider in the sequel the data from Study 2A, which measures the perceived environmental friendliness (PEF, variable pef) as a function of the proportion of paper wrapping (either none, half of the area of the plastic, equal or twice).\nThe linear model we can envision measures the effect of pef as a linear function of proportion, with \\[\\begin{align*}\n\\mathsf{E}(\\texttt{pef} \\mid \\texttt{proportion}) = \\beta_0 + \\beta_1 \\texttt{proportion}\n\\end{align*}\\] and with homoscedastic observations. More general models would include polynomials (up to degree \\(K-1\\) for a factor with \\(K\\) levels).\nIf we fit against the simple linear regression model, we can extract the estimated coefficients and the \\(p\\)-values for the \\(t\\)-test for \\(\\beta_0\\) and \\(\\beta_1\\). The test for the intercept is of no interest since data are measured on a scale from 1 to 7, so the mean response when proportion=0 cannot be zero. The coefficient for proportion suggests a trend of 0.5 point per unit ratio, and this is significantly different from zero, indicating that the pef score changes with the paper to plastic ratio.\n\ndata(SKD23_S2A, package = \"hecedsm\") # load data\nlinmod &lt;- lm(pef ~ proportion, data = SKD23_S2A)\n# fit simple linear regression\ncoef(linmod) # extract intercept and slope\n\n(Intercept)  proportion \n  2.4073939   0.5264168 \n\n\nLet \\(\\mu_{0}, \\mu_{0.5}, \\mu_{1}, \\mu_2\\) denote the true mean of the PEF score as a function of the proportion of paper. There are several tests that could be of interest here, but we focus on contrasts performed by authors and an hypothesis test of linearity as a function of the proportion of plastic. For the latter, we could compare the linear regression model (in which the PEF score increases linearly with the proportion of paper to plastic) against the ANOVA which allows each of the four groups to have different means.\nIf we use \\(\\boldsymbol{\\alpha} \\in \\mathbb{R}^4\\) to denote the parameter vector of the analysis of variance model using the treatment parametrization and \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^2\\) for the simple linear regression model, then we have \\[\\begin{align*}\n\\mu_0 &= \\beta_0=\\alpha_0 \\\\\n\\mu_{0.5} &= \\beta_0 + 0.5 \\beta_1 = \\alpha_0 + \\alpha_1\\\\\n\\mu_1 &= \\beta_0 + \\beta_1 = \\alpha_0 + \\alpha_2 \\\\\n\\mu_2 &= \\beta_0 + 2 \\beta_1= \\alpha_0 + \\alpha_3.\n\\end{align*}\\] The test comparing the simple linear regression with the analysis of variance imposes two simultaneous restrictions, with \\(\\mathscr{H}_0: \\alpha_3 = 2\\alpha_2= 4\\alpha_1\\), so the null distribution is \\(\\mathsf{Fisher}(2, 798)\\) or roughly \\(\\chi^2_2\\).\n\nanovamod &lt;- lm(pef ~ factor(proportion), # one-way ANOVA\n               data = SKD23_S2A) \n# Compare simple linear regression with ANOVA\nanova(linmod, anovamod) # is the change in PEF linear?\n\nAnalysis of Variance Table\n\nModel 1: pef ~ proportion\nModel 2: pef ~ factor(proportion)\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    800 1372.7                                  \n2    798 1343.4  2    29.262 8.6909 0.0001845 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Specifying the weights \n# these contrasts encode the mean (so don't sum to zero)\ncar::linearHypothesis(model = anovamod, \n   hypothesis = rbind(c(0, -2, 1, 0), \n                      c(0, 0, -2, 1)))\n\n\nLinear hypothesis test:\n- 2 factor(proportion)0.5  + factor(proportion)1 = 0\n- 2 factor(proportion)1  + factor(proportion)2 = 0\n\nModel 1: restricted model\nModel 2: pef ~ factor(proportion)\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    800 1372.7                                  \n2    798 1343.4  2    29.262 8.6909 0.0001845 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see from the output that the \\(F\\) tests and the \\(p\\)-values are identical, whether we impose the constraints manually or simply feed the two nested models to the anova method.\nThe authors were interested in comparing none with other choices: we are interested in pairwise differences, but only relative to the reference \\(\\mu_{0}\\): \\[\\begin{align*}\n\\mu_0 = \\mu_{0.5}  & \\iff 1\\mu_0 - 1\\mu_{0.5} + 0\\mu_{1} + 0 \\mu_{2} = 0\\\\\n\\mu_0 = \\mu_{1} & \\iff 1\\mu_0 + 0\\mu_{0.5} -1\\mu_{1} + 0 \\mu_{2} = 0\\\\\n\\mu_0 = \\mu_{2} & \\iff 1\\mu_0 + 0\\mu_{0.5} + 0\\mu_{1} -1 \\mu_{2} = 0\n\\end{align*}\\] so contrast vectors \\((1, -1, 0, 0)\\), \\((1, 0, -1, 0)\\) and \\((1, 0, 0, -1)\\) for the marginal means would allow one to test the hypothesis.\n\nmargmean &lt;- anovamod |&gt;  \n  emmeans::emmeans(specs = \"proportion\") # group means\ncontrastlist &lt;- list( # specify contrast vectors\n   refvshalf = c(1, -1, 0, 0),\n   refvsone =  c(1, 0, -1, 0),\n   refvstwo =  c(1, 0, 0, -1))\n# compute contrasts relative to reference\nmargmean |&gt; emmeans::contrast(method = contrastlist)\n\n contrast  estimate    SE  df t.ratio p.value\n refvshalf   -0.749 0.131 798  -5.713  &lt;.0001\n refvsone    -0.901 0.131 798  -6.893  &lt;.0001\n refvstwo    -1.182 0.129 798  -9.196  &lt;.0001\n\n\nThe group averages are reported in Table 9.1, match those reported by the authors in the paper. They suggest an increased perceived environmental friendliness as the amount of paper used in the wrapping increases. We could fit a simple regression model to assess the average change, treating the proportion as a continuous explanatory variable. The estimated slope for the change in PEF score, which ranges from 1 to 7 in increments of 0.25, is 0.53 point per ratio of paper/plastic. There is however strong evidence, given the data, that the change isn’t quite linear, as the fit of the linear regression model is significantly worse than the corresponding linear model.\n\n\n\n\nTable 9.1: Estimated group averages of PEF per proportion with standard errors\n\n\n\n\n\n\nproportion\nmarg. mean\nstd. err.\ndof\nlower (CI)\nupper (CI)\n\n\n\n\n0.0\n2.16\n0.093\n798\n1.98\n2.3439\n\n\n0.5\n2.91\n0.093\n798\n2.73\n3.0926\n\n\n1.0\n3.06\n0.092\n798\n2.88\n3.2441\n\n\n2.0\n3.34\n0.089\n798\n3.17\n3.5193\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.2: Estimated contrasts for differences of PEF to no paper.\n\n\n\n\n\n\ncontrast\nestimate\nstd. err.\ndof\nstat\np-value\n\n\n\n\nrefvshalf\n-0.75\n0.13\n798\n-5.71\n0\n\n\nrefvsone\n-0.90\n0.13\n798\n-6.89\n0\n\n\nrefvstwo\n-1.18\n0.13\n798\n-9.20\n0\n\n\n\n\n\n\n\n\n\n\nAll differences reported in Table 9.2 are significant and positive, in line with the researcher’s hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression models</span>"
    ]
  },
  {
    "objectID": "ancova.html#analysis-of-covariance",
    "href": "ancova.html#analysis-of-covariance",
    "title": "9  Linear regression models",
    "section": "9.2 Analysis of covariance",
    "text": "9.2 Analysis of covariance\nThe previous chapter dealt with factorial experiments in which all experimental factors are of interest. It is possible to use measurements concomitant to the data collection (for example, value to a test before we complete the group assignment for the manipulation) to get a measure of the relative strength of students. The more correlated these measures are with the response, the more we can explain the data. We then proceed with the random assignment of our experimental units to different conditions.\nIncluding covariates should in principle increase power and our ability to detect real differences due to experimental manipulations, provided the variables used as control are correlated with the response. Generally, they are not needed for valid inference, which is guaranteed by randomization, and shouldn’t be used to assign treatment. Such designs are meant to reduce the error.\nWe can include continuous covariates to the analysis of variance, whose slope governs the relationship with the response. The strict inclusion isn’t necessary to draw valid causal conclusion, but adding the term helps again reduce the residual variability. Such a design was historically called analysis of covariance, although as analysis of variance models, they are nothing but linear regression models. The ANCOVA model assumes that slopes are parallel: if rather there is an interaction term, then the experimental manipulation induces changes that depend on the value of the continuous explanatory. This case is termed moderation in the management literature.\nIn an analysis of covariance, we include a linear component for a (continuous) covariate, with the purpose again to reduce residual error and increase power. A prime example is prior/post experiment measurements, whereby we monitor the change in outcome due to the manipulation. This post by Solomon Kurz [link] nicely illustrates the added benefits of using covariates when there is strong correlation between your response and the latter\nIn such setting, it may seem logical to take the difference in post and prior score as response: this is showcased in Example 9.3 and Baumann, Seifert-Kessell, and Jones (1992), an analysis of which is presented on the course website.\nWhen we add a covariate, we need the latter to have a strong linear correlation for the inclusion to make sense. We can assess graphically whether the relationship is linear, and whether the slopes for each experimental condition are the same.1\n\n\n\n\n\n\n\n\nFigure 9.1: Simulated data from two groups with an analysis of covariance model.\n\n\n\n\n\nThe left panel of Figure 9.1 shows the ideal situation for an analysis of covariate: the relationship between response and covariate is linear with strong correlation, with the same slope and overlapping support. Since the slopes are the same, we can compare the difference in average (the vertical difference between slopes at any level of the covariate) because the latter is constant, so this depiction is useful. By contrast, the right-hand panel of Figure 9.1 shows an interaction between the covariate and the experimental groups, different slopes: there, the effect of the experimental condition increases with the level of the covariate. One may also note that the lack of overlap in the support, the set of values taken by the covariate, for the two experimental conditions, makes comparison hazardous at best in the right-hand panel.\nFigure 9.2 shows that, due to the strong correlation, the variability of the measurements is smaller on the right-hand panel (corresponding to the analysis of covariance model) than for the centred response on the left-hand panel; note that the \\(y\\)-axes have different scales.\n\n\n\n\n\n\n\n\nFigure 9.2: Response after subtracting mean (left) and after detrending (right).\n\n\n\n\n\nWe present two examples of analysis of covariance, showing how the inclusion of covariates helps disentangle differences between experimental conditions.\n\nExample 9.2 (Inconsistency of product description and image in online retailing) Lee and Choi (2019) measured the impact of discrepancies between descriptions and visual depiction of items in online retail. They performed an experiment in which participants were presented with descriptions of a product (a set of six toothbrushes) that was either consistent or inconsistent with the description. The authors postulated that a discrepancy could lead to lower appreciation score, measured using three Likert scales. They also suspected that the familiarity with the product brand should impact ratings, and controlled for the latter using another question.\nOne way to account for familiarity when comparing the mean is to use a linear regression with familiarity as another explanatory variable. The expected value of the product evaluation is \\[\n\\mathsf{E}(\\texttt{prodeval}) = \\beta_0 + \\beta_1 \\texttt{familiarity} + \\beta_2 \\texttt{consistency},\n\\tag{9.1}\\] where \\(\\texttt{familiarity}\\) is the score from 1 to 7 and \\(\\texttt{consistency}\\) is a binary indicator equal to one if the output is inconsistent and zero otherwise. The coefficient \\(\\beta_2\\) thus measures the difference between product evaluation rating for consistent vs inconsistent displays, for the same familiarity score.\nWe can look at coefficient (standard error) estimates \\(\\widehat{\\beta}_2 = -0.64 (0.302)\\). No difference between groups would mean \\(\\beta_2=0\\) and we can build a test statistic by looking at the standardized regression coefficient \\(t = \\widehat{\\beta}_2/\\mathsf{se}(\\widehat{\\beta}_2)\\). The result output is \\(b = -0.64\\), 95% CI \\([-1.24, -0.04]\\), \\(t(93) = -2.12\\), \\(p = .037\\). We reject the null hypothesis of equal product evaluation for both display at level 5%: there is evidence that there is a small difference, with people giving on average a score that is 0.64 points smaller (on a scale of 1 to 9) when presented with conflicting descriptions and images.\nWe can compare the analysis of variance table obtained by fitting the model with and without \\(\\texttt{familiarity}\\). Table 9.3 shows that the effect of consistency is small and not significant and a two-sample t-test shows no evidence of difference between the average familiarity score in both experimental conditions (\\(p\\)-value of \\(.532\\)). However, we can explain roughly one fifth of the residual variability by the familiarity with the brand (see the sum of squares in Table 9.3): removing the latter leads to a higher signal-to-noise ratio for the impact of consistency, at the expense of a loss of one degree of freedom. Thus, it appears that the manipulation was successful.\n\n\n\n\nTable 9.3: Analysis of variance tables\n\n\n\n\nmodel without familiarity\n\n\nterm\nsum. sq.\ndf\nstat\np-value\n\n\n\n\nconsistency\n7.04\n1\n2.55\n.113\n\n\nResiduals\n259.18\n94\n\n\n\n\n\n\n\n\nmodel with familiarity\n\n\nterm\nsum. sq.\ndf\nstat\np-value\n\n\n\n\nfamiliarity\n55.94\n1\n25.60\n&lt; .001\n\n\nconsistency\n9.80\n1\n4.49\n.037\n\n\nResiduals\n203.24\n93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3: Scatterplot of product evaluation as a function of the familiarity score, split by experimental manipulation.\n\n\n\n\n\nFigure 9.3 shows that people more familiar with the product or brand tend to have a more positive product evaluation, as postulated by the authors. The graph also shows two straight lines corresponding to the fit of a linear model with different intercept and slope for each display group: there is little material difference, one needs to assess formally whether a single linear relationship as the one postulated in Equation 9.1 can adequately characterize the relation in both groups.\nTo this effect, we fit a linear model with different slopes in each group, and compare the fit of the latter with the analysis of covariance model that includes a single slope for both groups: we can then test if the slopes are the same, or alternatively if the difference between the slopes is zero. The t-statistic indicates no difference in slope (\\(p\\)-value of \\(.379\\)), thus the assumption is reasonable. Levene’s test for homogeneity of variance indicates no discernible difference between groups. Thus, it appears there is a difference in perception of product quality due to the manipulation.\n\n\nExample 9.3 (Effect of scientific consensus on false beliefs) We consider Study 3 of Stekelenburg et al. (2021), who studied changes in perception of people holding false beliefs or denying (to some extent) the scientific consensus by presenting them with news article showcasing information about various phenomena. The experimental manipulation consisted in presenting boosting, a form of training to help readers identify and establish whether scientifists were truly expert in the domain of interest, how strong was the consensus, etc.2\nThe third and final experiment of the paper focused on genetically modified organisms: it is a replication of Study 2, but with a control group (since there were no detectable difference between experimental conditions Boost and BoostPlus) and a larger sample size (because Study 2 was underpowered).\nThe data include 854 observations with prior, the negative of the prior belief score of the participant, the post experiment score for the veracity of the claim. Both were measured using a visual scale ranging from -100 (I am 100% certain this is false) to 100 (I am 100% certain this is true), with 0 (I don’t know) in the middle. Only people with negative prior beliefs were recruited to the study. The three experimental conditions were BoostPlus, consensus and a control group. Note that the scores in the data have been negated, meaning that negative posterior scores indicate agreement with the consensus on GMO.\nPreliminary checks suggest that, although the slopes for prior beliefs could plausibly be the same in each group and the data are properly randomized, there is evidence of unequal variance for the changes in score. As such, we fit a model with mean \\[\\begin{align*}\n\\mathsf{E}(\\texttt{post}) &= \\begin{cases}\n\\beta_0 + \\beta_1 \\texttt{prior} + \\alpha_1 &  \\texttt{condition} = \\texttt{BoostPlus}\\\\\n\\beta_0 + \\beta_1 \\texttt{prior} + \\alpha_2 &\\texttt{condition} = \\texttt{consensus}\\\\\n\\beta_0 + \\beta_1 \\texttt{prior} + \\alpha_3 &\\texttt{condition} = \\texttt{control}\n\\end{cases}\n\\end{align*}\\] with \\(\\alpha_1 + \\alpha_2 + \\alpha_3=0\\), using the sum-to-zero parametrization, and with different variance for each experimental condition, \\[\\begin{align*}\n\\mathsf{Va}(\\texttt{post}) = \\begin{cases}\n\\sigma^2_1, &  \\texttt{condition} = \\texttt{BoostPlus},\\\\\n\\sigma^2_2, &  \\texttt{condition} = \\texttt{consensus},\\\\\n\\sigma^2_3, & \\texttt{condition} = \\texttt{control}.\n\\end{cases}\n\\end{align*}\\] Because of the unequal variances, we cannot use multiple testing procedures reserved for analysis of variance and resort instead to Holm–Bonferroni to control the familywise error rate. We here look only at pairwise differences between conditions.3\n\n\n\n\nTable 9.4: Analysis of variance tables\n\n\n\n\nANOVA model (without prior belief)\n\n\nterm\ndf\nstat\np-value\n\n\n\n\ncondition\n2\n42.5\n&lt; .001\n\n\n\n\n\n\nANCOVA model (with prior belief)\n\n\nterm\ndf\nstat\np-value\n\n\n\n\nprior\n1\n289.2\n&lt; .001\n\n\ncondition\n2\n57.0\n&lt; .001\n\n\n\n\n\n\n\n\nRepeating the exercise of comparing the amount of evidence for comparison with and without inclusion of a covariate shows that the value of the test statistic is larger (Table 9.4), indicative of stronger evidence with the analysis of covariance model: the conclusion would be unaffected with such large sample sizes. We of course care very little for the global \\(F\\) test of equality of mean, as the previous study had shown large differences. What is more interesting here is quantifying the change between conditions.\n\n\n\n\nTable 9.5: Pairwise contrasts with p-values adjusted using Holm–Bonferroni\n\n\n\n\nANOVA model (without prior belief score).\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nstd.error\ndf\nstatistic\np.value\n\n\n\n\nconsensus vs control\n-11.98\n4.0\n557.54\n-3.007\n.003\n\n\nconsensus vs BoostPlus\n16.31\n4.7\n546.36\n3.490\n&lt; .001\n\n\nBoostPlus vs control\n-28.29\n4.4\n505.44\n-6.489\n&lt; .001\n\n\n\n\n\n\nANCOVA model (with prior belief score).\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nstd.error\ndf\nstatistic\np.value\n\n\n\n\nconsensus vs control\n-11.84\n3.3\n543.06\n-3.544\n&lt; .001\n\n\nconsensus vs BoostPlus\n17.47\n4.3\n523.60\n4.108\n&lt; .001\n\n\nBoostPlus vs control\n-29.30\n3.9\n458.62\n-7.454\n&lt; .001\n\n\n\n\n\n\n\n\nTable 9.5 shows the pairwise contrasts, which measure two different things: the analysis of variance model compares the average in group, whereas the analysis of covariance (the linear model with prior) uses detrended values and focuses on the change in perception. Because the data are unbalanced and we estimate group mean and variance separately, the degrees of freedom change from one pairwise comparison to the next. Again, using the covariate prior, which is somewhat strongly correlated with post as seen in Figure 9.4, helps decrease background noise.\n\n\n\n\nTable 9.6: Summary statistics of belief as a function of time of measurement and experimental condition.\n\n\n\n\n\n\ntime\ncondition\nmean\nse\n\n\n\n\nprior\nBoostPlus\n57.65\n1.69\n\n\nprior\nconsensus\n56.32\n1.67\n\n\nprior\ncontrol\n56.49\n1.68\n\n\npost\nBoostPlus\n2.62\n3.53\n\n\npost\nconsensus\n18.93\n3.06\n\n\npost\ncontrol\n30.91\n2.56\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.4: Difference between prior and post experiment beliefs on genetically engineered food.\n\n\n\n\n\n\n\n\n\n\n\n\nPitfall\n\n\n\nStekelenburg et al. (2021) split their data to do pairwise comparisons two at the time (thus taking roughly two-third of the data to perform a two sample t-test with each pair). Although it does not impact their conclusion, this approach is conceptually incorrect: if the variance was equal, we would want to use all observations to estimate it (so their approach would be suboptimal, since we would estimate the variance three times with smaller samples).\nOn the contrary, using a model that assumes equal variance when it is not the case leads to distortion: the variance we estimate will be some sort of average of the variability \\(\\sigma_i\\) and \\(\\sigma_j\\) in experimental condition \\(i\\) and \\(j\\), again potentially leading to distortions. With large samples, this may be unconsequential, but illustrates caveats of subsample analyses.\n\n\n\n\n\n\n\n\nPitfall\n\n\n\nFigure 9.5 shows the relationship between prior and posterior score. The data show clear difference between individuals: many start from completely disbelieving of genetically engineered food and change their mind (sometimes drastically), there are many people who do not change idea at all and have similar scores, and many who give a posterior score of zero. This heterogeneity in the data illustrates the danger of only looking at the summary statistics and comparing averages. It does not tell the whole picture! One could investigate whether the strength of religious or political beliefs, or how much participants trust scientists, could explain some of the observed differences.\n\n\n\n\n\n\n\n\n\n\nFigure 9.5: Scatterplot of negated prior and posterior belief score.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression models</span>"
    ]
  },
  {
    "objectID": "ancova.html#moderation-and-interactions",
    "href": "ancova.html#moderation-and-interactions",
    "title": "9  Linear regression models",
    "section": "9.3 Moderation and interactions",
    "text": "9.3 Moderation and interactions\nIn a randomized experiment, we can check the average outcome of a manipulation by comparing groups: assuming random sampling, these conclusions can be broadly generalized to the population of interest from which the sample is drawn. However, it may be that the effect of the treatment depends on other variables: cultural differences, gender or education may change.\nThe causal effect on \\(Y\\) of the experimental manipulation, say \\(X\\) may be a misleading summary if another variable modifies the relation of \\(X \\to Y\\): for example, the perception of gender discrimination or racism may depend on the person background and experience and this may impact the effect of the manipulation. Such variables, say \\(W\\), thus have an interactive effect with the experimental factor \\(X\\), termed moderator in psychology. In the statistical model, inclusion of interaction terms between \\(X\\) and \\(W\\) (typically via product of the so-called moderator variable with the factor encoding the experimental sub-condition) will allow us to estimate those differences.\nIn an analysis of covariance, covariates are included that have an impact on the outcome to filter out variability, but with the assumption that they do not influence the effect of treatment (no interaction). With moderators, we rather include and tests for the interactions. If we have an experimental factor \\(X\\) which is binary or categorical, the resulting model is a simple analysis of variance model (or analysis of covariance) and we can test the significance of the interaction term to assess the moderating effect of \\(W\\).\nIf \\(W\\) is a mean-centered continuous variable and \\(X\\) a categorical variable with \\(k=1, \\ldots, K\\) levels using the sum-to-zero parametrization, the linear model with mean \\[\n\\underset{\\text{average response at $W$ in group $k$}}{\\mathsf{E}\\{Y \\mid \\mathsf{do}(X) = k, W\\}} = \\underset{\\text{intercept for group $k$}}{\\mu + \\alpha_k} + \\underset{\\text{slope for group $k$}}{(\\beta + \\gamma_k)}W\n\\] includes different slopes for \\(W\\) in each experimental group, as well as different intercepts for each group.\n\nExample 9.4 (Gender discrimation) We consider data from a study on gender discrimination (Garcia et al. 2010). Participants were put with a file where a women was turned down promotion in favour of male colleague despite her being clearly more experimented and qualified. The authors manipulated the decision of the participant to this decision, either choosing not to challenge the decision (no protest), a request to reconsider based on individual qualities of the applicants (individual) and a request to reconsider based on abilities of women (collective). All items were measured using scales constructed using items measured using Likert scales ranging from strongly disagree (1) to strongly agree (7).\nThe postulated mediator variable is sexism, the average of 6 Likert scales for the Modern Sexism Scale assessing pervasiveness of gender discrimination. We consider participants’ evaluation of the appropriateness of the response of the fictional character.\nWe fit the linear model with the interaction and display the observed slopes\n\ndata(GSBE10, package = \"hecedsm\")\nlin_moder &lt;- lm(respeval ~ protest*sexism,\n               data = GSBE10)\nsummary(lin_moder) # coefficients\ncar::Anova(lin_moder, type = 3) # tests\n\n\n\n\n\nTable 9.7: Analysis of variance table for the linear moderation model of .\n\n\n\n\n\n\nterm\nsum of squares\ndf\nstat\np-value\n\n\n\n\nprotest\n6.34\n2\n2.45\n.091\n\n\nsexism\n6.59\n1\n5.09\n.026\n\n\nprotest:sexism\n12.49\n2\n4.82\n.010\n\n\nResiduals\n159.22\n123\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause of the interaction, comparing the levels of the experimental factor only makes sense if we fix the value of sexism (since the slopes are not parallel) and won’t necessarily be reliable outside of the range of observed values of sexism. We could look at quantiles and differences at the mean sexism,4 or one standard deviation away.\n\nWe may be interested in the range of values of the predictor \\(W\\) for which the difference between treatments is not statistically significant if we only have a binary treatment. The Johnson–Neyman method (Johnson and Neyman 1936) considers this range, but this leads to multiple testing problems since we probe the model repeatedly. Esarey and Sumner (2018) offer a method that provides control for the false discovery rate.\nTo illustrate the method, we dichotomize the manipulation pooling individual and collective protests, since these are the most similar.\n\nlibrary(interactions)\ndb &lt;- GSBE10 |&gt;\n  dplyr::mutate(\n    protest = as.integer(protest != \"no protest\"))\nlin_moder2 &lt;- lm(respeval ~ protest*sexism, data = db)\njn &lt;- johnson_neyman(\n  model = lin_moder2, # linear model\n  pred = protest, # binary experimental factor\n  modx = sexism, # moderator\n  control.fdr = TRUE,\n  mod.range = range(db$sexism))\njn$plot\n\n\n\n\n\n\n\nFigure 9.6: Johnson–Neyman plot for difference between protest and no protest as a function of sexism.\n\n\n\n\n\nThe cutoff value is 4.20 with control for the false discovery rate and 4.15 without. The interval is not extended beyond the range of value for sexism, as these are not possible given the Likert scale (which starts at 1). In this example, the moderator is not experimentally manipulated, but it could be. More complicated mediation models could include interactions between treatment effects or moderators and covariates, with external variables, leading to moderated mediation. Interactions can be considered for pretty much any statistical model, but the usual assumptions need to hold for inference to be approximately valid.\n\nExample 9.5 (Ghosting) Leckfor et al. (2023) consider the impact of ghosting someone (i.e., not responding to communications and cutting bridges with a person) on personal satisfaction, uncertainty and the need for closure (explanations). In their Study 3, they postulated that the type of rejection someone experienced. The study comprises 545 participants who were asked to reflect about a situation in their own life, in scenarios where\n\nParticipants in the included (control) condition were asked to write about a time when someone “expressed that they wanted to continue and/or maintain a friendship, romantic relationship, or casual dating situation.” Participants in the ghosted condition wrote about a time when someone ended a relationship by “suddenly cutting off all communication without explanation.” Participants in the directly rejected condition wrote about a time when someone “directly communicate[d] that they wanted to end the relationship.” Nearly half (49%) of participants wrote about a friendship, followed by a romantic relationship (29%), and then a casual dating situation (22%).\n\nThe moderator variable is cond, one of control, being ghosted or rejected. This should moderate the effect of the need for closure on need for satisfaction. The authors postulated that individuals would have lower needs of satisfaction after being ghosted.\n\n# Moderation analysis\n# This time, the factor of interest is continuous\n# and the moderator is categorical (K=3 levels)\ndata(LWSH23_S3, package = \"hecedsm\")\nmod &lt;- lm(data = LWSH23_S3, needsatis ~ needclosure * cond)\nanova(mod) # interaction is significant\n# Compute estimated marginal means, but with global weights equal to relative weight of each variable\nemmeans::emmeans(mod, specs = \"needclosure\", \n                 by = \"cond\", \n                 at = list(needclosure = 2:7), \n                 weights = \"prop\")\n# All values are reported for the average of needclosure by default without the at\n\n\n\n\n\nTable 9.8: Linear moderation model coefficients of Study 3 of Leckfor et al. (2023). Least square coefficients, standard errors, Wald tests and \\(p\\)-values for \\(\\beta=0\\) and 95% confidence intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ncoef.\nestimate\nstd. error\nlower CL\nupper CL\n\n\n\n\n(Intercept)\n\\(\\beta_0\\)\n5.0\n0.37\n4.29\n5.74\n\n\nneed for closure\n\\(\\beta_1\\)\n0.2\n0.07\n0.04\n0.33\n\n\ndirectly rejected [cond]\n\\(\\beta_2\\)\n-1.2\n0.53\n-2.20\n-0.12\n\n\nghosted [cond]\n\\(\\beta_3\\)\n-1.5\n0.50\n-2.48\n-0.52\n\n\nneed for closure * directly rejected [cond]\n\\(\\beta_4\\)\n-0.4\n0.11\n-0.61\n-0.19\n\n\nneed for closure * ghosted [cond]\n\\(\\beta_5\\)\n-0.4\n0.10\n-0.62\n-0.21\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.9: Analysis of variance table for the linear moderation model of Study 3 of Leckfor et al. (2023).\n\n\n\n\n\n\nterm\ndf\nsum of squares\nstat\np-value\n\n\n\n\nneedclosure\n1\n3.9\n4.26\n.039\n\n\ncond\n2\n1346.9\n738.61\n&lt; .001\n\n\nneedclosure:cond\n2\n18.6\n10.19\n&lt; .001\n\n\nResiduals\n539\n491.5\n\n\n\n\n\n\n\n\n\n\nSince the interaction is significant, we focus on the different slopes. We can of course simply report the equations of the linear regression for each group. Alternative, emmeans can also return the predictions for different values of needclosure per condition along with standard errors. The estimates will have lower values of needclosure when close to the subgroup averages.\nThe postulated theoretical mean equation is \\[\\begin{align*}\n\\mathsf{E}(\\texttt{needsatisf}) &= \\beta_0 + \\beta_1\\texttt{needclosure} + \\beta_2 \\mathsf{1}_{\\texttt{cond=directly rejected}} +\\beta_3 \\mathsf{1}_{\\texttt{cond=directly rejected}} \\\\&+ \\beta_4 \\texttt{needclosure} \\times \\mathsf{1}_{\\texttt{cond=directly rejected}} +\\beta_5 \\texttt{needclosure} \\times\\mathsf{1}_{\\texttt{cond=directly rejected}}.\n\\end{align*}\\] The estimated model mean \\(\\widehat{\\mathsf{E}}(\\texttt{needsatisf} \\mid \\texttt{needclosure}=x)\\) when needclosure is worth \\(x \\in [1,7]\\) for each cond is thus \\[\\begin{align*}\n\\widehat{Y} & = \\begin{cases}\n\\widehat{\\beta}_0 + \\widehat{\\beta}_1x, & \\texttt{control};\\\\\n(\\widehat{\\beta}_0 + \\widehat{\\beta}_3) + (\\widehat{\\beta}_1 + \\beta_4)x , & \\texttt{directly rejected};\\\\\n(\\widehat{\\beta}_0 + \\widehat{\\beta}_2) + (\\widehat{\\beta}_1 + \\beta_5)x, & \\texttt{ghosted}.\n\\end{cases} \\\\&= \\begin{cases}\n5.015 + 0.181 x, & \\texttt{control};\\\\\n3.86 -0.222x, & \\texttt{directly rejected};\\\\\n3.52 -0.234x, & \\texttt{ghosted}.\n\\end{cases}\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nFigure 9.7: Scatterplot and linear regression slopes for the need for satisfaction as a function of the moderating situation and the need for closure.\n\n\n\n\n\nWe can see clearly in Figure 9.7 that the impact of the need for closure depends on the situation, with downward trends for the ghosted and rejected groups.\nWe can fit this toy model also with the PROCESS macro, which only understand numeric values for factors… The output (omitted) includes model coefficients with confidence intervals, the \\(F\\) test statistic for the ANOVA, and output for the average need for closure.\n\nprocess(data = LWSH23_S3 |&gt;\n          dplyr::mutate(condind = as.integer(cond)), # cast factor to numeric integer levels\n        y = \"needsatis\",  # response variable\n        x = \"needclosure\", # explanatory variable (not manipulated)\n        w = \"condind\", # postulated moderator\n        mcw = 1, # dummy coding for moderator w (so compare to base level, here 'included')\n        model = 1) # number of model in Hayes (simple moderation)\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\nWe can treat factor levels measuring quantities as continuous covariates, and reduce the number of parameters.\nInclusion of continuous covariates may help filtering out unwanted variability.\nThese are typically concomitant variables (measured alongside the response variable).\nThis designs reduce the residual error, leading to an increase in power (more ability to detect differences in average between experimental conditions).\nWe are only interested in differences due to experimental condition (marginal effects).\nIn general, there should be no interaction between covariates/blocking factors and experimental conditions.\nThis hypothesis can be assessed by comparing the models with and without interaction, if there are enough units (e.g., equality of slope for ANCOVA).\nModerators are variables that interact with the experimental factor. We assess their presence by testing for an interaction in a linear regression model.\nIn the presence of a moderator, we need to consider slopes or groups separately for subgroups.\n\n\n\n\n\n\n\nBaumann, James F., Nancy Seifert-Kessell, and Leah A. Jones. 1992. “Effect of Think-Aloud Instruction on Elementary Students’ Comprehension Monitoring Abilities.” Journal of Reading Behavior 24 (2): 143–72. https://doi.org/10.1080/10862969209547770.\n\n\nEsarey, Justin, and Jane Lawrence Sumner. 2018. “Marginal Effects in Interaction Models: Determining and Controlling the False Positive Rate.” Comparative Political Studies 51 (9): 1144–76. https://doi.org/10.1177/0010414017730080.\n\n\nGarcia, Donna M., Michael T. Schmitt, Nyla R. Branscombe, and Naomi Ellemers. 2010. “Women’s Reactions to Ingroup Members Who Protest Discriminatory Treatment: The Importance of Beliefs about Inequality and Response Appropriateness.” European Journal of Social Psychology 40 (5): 733–45. https://doi.org/10.1002/ejsp.644.\n\n\nJohnson, P. O., and J. Neyman. 1936. “Tests of Certain Linear Hypotheses and Their Application to Some Educational Problems.” Statistical Research Memoirs 1: 57–93.\n\n\nLeckfor, Christina M., Natasha R. Wood, Richard B. Slatcher, and Andrew H. Hales. 2023. “From Close to Ghost: Examining the Relationship Between the Need for Closure, Intentions to Ghost, and Reactions to Being Ghosted.” Journal of Social and Personal Relationships 40 (8): 2422–44. https://doi.org/10.1177/02654075221149955.\n\n\nLee, Kiljae, and Jungsil Choi. 2019. “Image-Text Inconsistency Effect on Product Evaluation in Online Retailing.” Journal of Retailing and Consumer Services 49: 279–88. https://doi.org/10.1016/j.jretconser.2019.03.015.\n\n\nSokolova, Tatiana, Aradhna Krishna, and Tim Döring. 2023. “Paper Meets Plastic: The Perceived Environmental Friendliness of Product Packaging.” Journal of Consumer Research 50 (3): 468–91. https://doi.org/10.1093/jcr/ucad008.\n\n\nStekelenburg, Aart van, Gabi Schaap, Harm Veling, and Moniek Buijzen. 2021. “Boosting Understanding and Identification of Scientific Consensus Can Help to Correct False Beliefs.” Psychological Science 32 (10): 1549–65. https://doi.org/10.1177/09567976211007788.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression models</span>"
    ]
  },
  {
    "objectID": "ancova.html#footnotes",
    "href": "ancova.html#footnotes",
    "title": "9  Linear regression models",
    "section": "",
    "text": "If not, this implies that the covariate interacts with the experimental condition.↩︎\nThe article is interesting because lack of planning/changes led them to adapt the design from experiment 1 to 3 until they found something. Without preregistration, it is unlikely such findings would have been publishable.↩︎\nIn Study 2, the interest was comparing manipulation vs control and the Boost vs BoostPlus conditions, two orthogonal contrasts.↩︎\nThis is the default with emmeans↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression models</span>"
    ]
  },
  {
    "objectID": "power_effect.html",
    "href": "power_effect.html",
    "title": "10  Effect sizes and power",
    "section": "",
    "text": "10.1 Effect sizes\nIn social studies, it is common to write a paper containing multiple studies on a similar topic. These may use different designs, with varying sample size. If the studies uses different questionnaires, or change the Likert scale, the results and the mean difference between groups are not directly comparable between experiments.\nWe may also wish replicate a study by using the same material and re-run an experiment. For the replication to be somewhat successful (or at least reliable), one needs to determine beforehand how many participants should be recruited in the study.\nWe could think for an example of comparing statistics or \\(p\\)-values, which are by construction standardized unit less measures, making them comparable across study. Test statistics show how outlying observed differences between experimental conditions relative to a null hypothesis, typically that of no effect (equal mean in each subgroup). However, statistics are usually a function of both the sample size (the number of observations in each experimental condition) and the effect size (how large the standardized differences between groups are), making them unsuitable for describing differences.\nFigure 10.1 shows an example with the sampling distributions of the difference in mean under the null (curve centered at zero) and the true alternative (mean difference of two). The area in white under the curve represents the power, which is larger with larger sample size and coincides with smaller average \\(p\\)-values for the testing procedure.\nOne could argue that, on the surface, every null hypothesis is wrong and that, with a sufficiently large number of observation, all observed differences eventually become “statistically significant”. This has to do with the fact that we become more and more certain of the estimated means of each experimental sub-condition. Statistical significance of a testing procedure does not translate into practical relevance, which itself depends on the scientific question at hand. For example, consider the development of a new drug for commercialization by Health Canada: what is the minimum difference between two treatments that would be large enough to justify commercialization of the new drug? If the effect is small but it leads to many lives saved, would it still be relevant? Such decision involve a trade-off between efficacy of new treatment relative to the status quo, the cost of the drug, the magnitude of the improvement, etc.\nEffect size are summaries to inform about the standardized magnitude of these differences; they are used to combine results of multiple experiments using meta-analysis, or to calculate sample size requirements to replicate an effect in power studies.\nThere are two main classes of effect size: standardized mean differences and ratio (percentages) of explained variance. The latter are used in analysis of variance when there are multiple groups to compare.\nUnfortunately, the literature on effect size is quite large. Researchers often fail to distinguish between estimand (unknown target) and the estimator that is being used, with frequent notational confusion arising due to conflicting standards and definitions. Terms are also overloaded: the same notation may be used to denote an effect size, but it will be calculated differently depending on whether the design is between-subject or within-subject (with repeated correlated measures per participant), or whether there are blocking factors.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Effect sizes and power</span>"
    ]
  },
  {
    "objectID": "power_effect.html#effect-sizes",
    "href": "power_effect.html#effect-sizes",
    "title": "10  Effect sizes and power",
    "section": "",
    "text": "10.1.1 Standardized mean differences\nTo gather intuition, we begin with the task of comparing the means of two groups using a two-sample \\(t\\)-test, with the null hypothesis of equality in means or \\(\\mathscr{H}_0: \\mu_1 = \\mu_2\\). The test statistic is \\[\\begin{align*}\nT =  \\frac{\\widehat{\\mu}_2 - \\widehat{\\mu}_1}{\\widehat{\\sigma}} \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)^{-1/2}\n\\end{align*}\\] where \\(\\widehat{\\sigma}\\) is the pooled sample size estimator. The first term, \\(\\widehat{d}_s = (\\widehat{\\mu}_2 - \\widehat{\\mu}_1)/\\widehat{\\sigma},\\) is termed Cohen’s \\(d\\) (Cohen 1988) and it measures the standardized difference between groups, a form of signal-to-noise ratio. As the sample size gets larger and larger, the sample mean and pooled sample variance become closer and closer to the true population values \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\sigma;\\) at the same time, the statistic \\(T\\) becomes bigger as \\(n\\) becomes larger because of the second term.1\nThe difference \\(d=(\\mu_1-\\mu_2)/\\sigma\\) has an obvious interpretation: a distance of \\(d=a\\) indicates that the means of the two groups are \\(a\\) standard deviation apart. Cohen’s \\(d\\) is sometimes loosely categorized in terms of weak \\((d = 0.2)\\), medium \\((d=0.5)\\) and large \\((d=0.8)\\) effect size; these, much like arbitrary \\(p\\)-value cutoffs, are rules of thumbs. Alongside \\(d\\), there are many commonly reported metrics that are simple transformations of \\(d\\) describing the observed difference. This interactive applet by Kristoffer Magnusson (Magnusson 2021) shows the visual impact of changing the value of \\(d\\) along. There are different estimators of \\(d\\) depending on whether or not the pooled variance estimator is used. Cohen’s \\(d,\\) is upward biased, meaning it gives values that are on average larger than the truth. Hedge’s \\(g\\) (Hedges 1981) offers a bias-correction and should always be preferred as an estimator, but the bias is relatively small and vanishes quickly as the sample size increases.\nFor these different estimators, it is possible to obtain (asymmetric) confidence intervals or tolerance intervals.2\n\nExample 10.1 (Effect sizes for “The Surprise of Reaching Out”) We consider a two-sample \\(t\\)-test for the study of Liu et al. (2023) discussed in Example 2.5. The difference in average response index is 0.371, indicating that the responder have a higher score. The \\(p\\)-value is 0.041, showing a small effect.\nIf we consider the standardized difference \\(d\\), the group means are \\(-0.289\\) standard deviations apart based on Hedge’s \\(g\\), with an associated 95% confidence interval of [\\(-0.567, -0.011\\)]: thus, the difference found is small (using Cohen (1988)’s convention) and there is a large uncertainty surrounding it.\nThere is a \\(42\\)% probability that an observation drawn at random from the responder condition will exceed an observation drawn at random of the initiator group (probability of superiority) and \\(38.6\\)% of the responder observations will exceed the median of the initiator (Cohen’s \\(U_3\\)).\n\ndata(LRMM23_S1, package = \"hecedsm\")\nttest &lt;- t.test(\n  appreciation ~ role, \n  data = LRMM23_S1,\n  var.equal = TRUE)\neffect &lt;- effectsize::hedges_g(\n  appreciation ~ role, \n  data = LRMM23_S1, \n  pooled_sd = TRUE)\n\n\n\n\n10.1.2 Ratio and proportion of variance\nAnother class of effect sizes are obtained by considering either the ratio of the variance due to an effect (say differences in means relative to the overall mean) relative to the background level of noise as measured by the variance.\nOne common measure employed in software is Cohen’s f (Cohen 1988), the square of which for a one-way ANOVA with factor \\(A\\) (equal variance \\(\\sigma^2\\)) with more than two groups, \\[\nf^2 = \\frac{1}{\\sigma^2} \\sum_{j=1}^k \\frac{n_j}{n}(\\mu_j - \\mu)^2 = \\frac{\\sigma^2_{A}}{\\sigma^2},\n\\] a weighted sum of squared difference relative to the overall mean \\(\\mu\\). \\(\\sigma^2_{A}\\) is a measure of the variability that is due to the difference in mean, so standardizing it by the measurement variance gives us a ratio of variance with values higher than one indicating that more variability is explainable, leading to higher effect sizes. If the means of every subgroup is the same, then \\(f=0\\). For \\(k=2\\) groups, Cohen’s \\(f\\) and Cohen’s \\(d\\) are related via \\(f=d/2\\).\nCohen’s \\(f\\) can be directly related to the behaviour of the \\(F\\) statistic under an alternative, as explained in Section 10.2.1. However, since the interpretation isn’t straightforward, we typically consider proportions of variance (rather than ratios of variance).\nTo build such an effect size, we break down the variability that is explained by our experimental manipulation (\\(\\sigma^2_A\\)), here denoted by effect, from the leftover unexplained part, or residual (\\(\\sigma^2_\\text{resid}\\)). In a one-way analysis of variance, \\[\\sigma^2_{\\text{total}} = \\sigma^2_{\\text{resid}} + \\sigma^2_{A}\\] and the percentage of variability explained by the factor \\(A\\) is, so \\[\\eta^2 = \\frac{\\text{explained variability}}{\\text{total variability}}= \\frac{\\sigma^2_{A}}{\\sigma^2_{\\text{resid}} + \\sigma^2_{A}} = \\frac{\\sigma^2_{A}}{\\sigma^2_{\\text{total}}}.\\] Simple arithmetic manipulations reveal that \\(f^2 = \\eta^2/(1-\\eta^2)\\), so we can relate any proportion of variance in terms of ratio and vice-versa.\nSuch an effect size depends on unknown population quantities (the true means of each subgroup, the overall mean and the variance). There are multiple alternative estimators to estimate \\(\\eta^2\\), and researchers are often carefree when reporting as to which is used. To disambiguate, I will put \\(\\hat{\\eta}^2\\) to denote an estimator. To make an analogy, there are many different recipes (estimators) that can lead to a particular cake, but some may lead to a mixing that is on average too wet if they are not well calibrated.\nThe default estimator for \\(\\eta^2\\) is the coefficient of determination of the linear regression, denoted \\(\\widehat{R}^2\\) or \\(\\widehat{\\eta}^2\\). The latter can be reconstructed from the analysis of variance table using the formula \\[\n\\widehat{R}{}^2 = \\frac{F\\nu_1}{F\\nu_1 + \\nu_2}\n\\] where for the one-way ANOVA \\(\\nu_1 = K-1\\) and \\(\\nu_2 = n-K\\) are the degrees of freedom of a design with \\(n\\) observations and \\(K\\) experimental conditions. From this, we can build an estimator of Cohen’s \\(f\\), where for the omnibus test of the one-way ANOVA, we get \\[\n\\widehat{f} = \\sqrt{F\\nu_1/\\nu_2}\n\\]\nUnfortunately, \\(\\widehat{R}{}^2\\) is an upward biased estimator (too large on average), leading to optimistic measures. Another estimator of \\(\\eta^2\\) that is recommended in Keppel and Wickens (2004) for power calculations is \\(\\widehat{\\omega}^2\\), which is \\[\\widehat{\\omega}^2 = \\frac{\\nu_1 (F-1)}{\\nu_1(F-1)+n}.\\] Since the \\(F\\) statistic is approximately 1 on average, this measure removes the mode. Both \\(\\widehat{\\omega}^2\\) and \\(\\widehat{\\epsilon}^2\\) have been reported to be less biased and thus preferable as estimators of the true proportion of variance (Lakens 2013). We can as before get an estimator of Cohen’s \\(f\\) for the variance ratio as \\(\\widetilde{f} = \\sqrt{\\nu_1(F-1)/n}\\).\n\nExample 10.2 (Computing effect size for a between-subject one-way ANOVA) Consider the one-way analysis of variance model for the “Degrees of Reading Power” cloze test, from Baumann, Seifert-Kessell, and Jones (1992). The response records the number of correctly answered items, ranging from 0 to 56.\n\ndata(BSJ92, package = \"hecedsm\")\n# Fit ANOVA model\nmod_post &lt;- lm(posttest3 ~ group, data = BSJ92)\n# Extract in data frame format the ANOVA table\nanova_table &lt;- broom::tidy(anova(mod_post))\nFstat &lt;- anova_table$statistic[1]\ndfs &lt;- anova_table$df\n# Output estimated value of eta-squared\n(eta_sq &lt;- Fstat * dfs[1] / (Fstat * dfs[1] + dfs[2]))\n\n[1] 0.1245399\n\n# Compare with coefficient of determination from regression\nsummary(mod_post)$r.squared\n\n[1] 0.1245399\n\n# Compare with output from R package 'effectsize'\neffectsize::eta_squared(mod_post, partial = FALSE)$Eta2\n\n[1] 0.1245399\n\n# Compare with omega-squared value - the latter is smaller\n(omega_sq &lt;- pmax(0, dfs[1]*(Fstat-1)/ (dfs[1]*(Fstat-1) + nobs(mod_post))))\n\n[1] 0.0954215\n\neffectsize::omega_squared(mod_post)$Omega2\n\n[1] 0.0954215\n\n\nWe can also compute effect size for contrasts. Since these take individual the form of \\(t\\)-tests, we can use emmeans to obtain corresponding effect sizes, which are Cohen’s \\(d\\).\n\nemmeans::emmeans(mod_post, specs = \"group\") |&gt; \n  emmeans::contrast(list(C1 = c(-1, 0.5, 0.5), \n                         C2 = c(0, 1, -1))) |&gt;\n# Specify estimated std. deviation of data and degrees of freedom \n  emmeans::eff_size(sigma = sigma(mod_post), edf = dfs[2])\n\n contrast effect.size  SE df lower.CL upper.CL\n C1 - C2        0.317 0.4 63   -0.482     1.12\n\nsigma used for effect sizes: 6.314 \nConfidence level used: 0.95 \n\n\n\n\n\n10.1.3 Partial effects and variance decomposition\nIn a multiway design with several factors, we may want to estimate the effect of separate factors or interactions. In such cases, we can break down the variability explained by manipulations per effect. The effect size for such models are build by comparing the variance explained by the effect.\nFor example, say we have a completely randomized balanced design with two factors \\(A\\), \\(B\\) and their interaction \\(AB\\). We can decompose the total variance as \\[\\sigma^2_{\\text{total}} = \\sigma^2_A + \\sigma^2_B + \\sigma^2_{AB} + \\sigma^2_{\\text{resid}}.\\] When the design is balanced, these variance terms can be estimated using the mean squared error from the analysis of variance table output. If the design is unbalanced, the sum of square decomposition is not unique and we will get different estimates when using Type II and Type III sum of squares.\nWe can get formula similar to the one-sample case with now what are termed partial effect sizes, e.g., \\[\\widehat{\\omega}^2_{\\langle \\text{effect} \\rangle} = \\frac{\\text{df}_{\\text{effect}}(F_{\\text{effect}}-1)}{\\text{df}_{\\text{effect}}(F_{\\text{effect}}-1) + n},\\] where \\(n\\) is the overall sample size and \\(F_\\text{effect}\\) and the corresponding degrees of freedom could be the statistic associated to the main effects \\(A\\) and \\(B\\), or the interaction term \\(AB\\). In R, the effectsize package reports these estimates with one-sided confidence intervals derived using the pivot method (Steiger 2004).3\nSoftware will typically return estimates of effect size alongside with the designs, but there are small things to keep in mind. One is that the decomposition of the variance is not unique with unbalanced data. The second is that, when using repeated measures and mixed models, the same notation is used to denote different quantities.\nLastly, it is customary to report effect sizes that include the variability of blocking factors and random effects, leading to so-called generalized effect sizes. Include the variance of all blocking factors and interactions (only with the effect!) in the denominator.4\nFor example, if \\(A\\) is the experimental factor whose main effect is of interest, \\(B\\) is a blocking factor and \\(C\\) is another experimental factor, use \\[\\eta_{\\langle A \\rangle}^2 = \\frac{\\sigma^2_A}{\\sigma^2_A + \\sigma^2_B + \\sigma^2_{AB} + \\sigma^2_{\\text{resid}}}.\\] as generalized partial effect. The reason for including blocking factors and random effects is that they would not necessarily be available in a replication. The correct effect size measure to calculate and to report depends on the design, and there are numerous estimators that can be utilized. Since they are related to one another, it is oftentimes possible to compute them directly from the output or convert. The formula highlight the importance of reporting (with enough precision) exactly the values of the test statistic.\n\nExample 10.3 In R, the effectsize package functions, which are displayed prominently in this chapter, have a generalized argument to which the vector of names of blocking factor can be passed. We use the one-way analysis of variance from Example 12.1 for illustrating the calculation. Once again, the output matches the output of the package.\n\nmod_block &lt;- lm(apprec ~ role + dyad, data = LRMM23_S3l)\nanova_tab &lt;- broom::tidy(anova(mod_block))\n# Compute the generalized effect size - variance are estimated\n# based on sum of squared termes in the ANOVA table\nwith(anova_tab, sumsq[1]/sum(sumsq))\n\n[1] 0.08802785\n\n# We can use the 'effectsize' package, specifying the blocking factor\n# through argument 'generalized'\neff &lt;- effectsize::eta_squared(model = mod_block, generalized = \"dyad\")\n# Extract the generalized eta-squared effect size for 'role' for comparison\neff$Eta2_generalized[1]\n\n[1] 0.08802785\n\n\n\n\n\n\n\n\n\nPitfall\n\n\n\nMeasures of effect size are estimated based on data, but unlike summary statistics such as the mean and variance, tend to be very noisy in small samples and the uncertainty remains significant for larger samples. To show this, I simulated datasets for a two sample \\(t\\)-test: there is no effect for the control group, but the true effect for the treatment group is \\(d=0.2\\). With balanced data (\\(n/2\\) observations in each group) power is maximised. Figure 10.2 shows the estimated sample size for \\(B=100\\) replications of the experiment at samples of size \\(n=10, 12, \\ldots, 250\\). The horizontal lines at \\(0\\) represent no effect: the proportion of values which show effects that are of opposite sign to the truth is still significant at \\(n=250\\) observations, and the variability seems to decrease very slowly. For smaller samples, the effect sizes are erratic and, although they are centered at the true value, most of them are severely inflated.\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: Dispersion of estimated effect size for Cohen’s \\(d\\), for data with a true mean dispersion of \\(d=0.2\\), varying the sample size. The full and dashed lines give 95% and 50% confidence intervals for the sampling distribution, respectively.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Effect sizes and power</span>"
    ]
  },
  {
    "objectID": "power_effect.html#power",
    "href": "power_effect.html#power",
    "title": "10  Effect sizes and power",
    "section": "10.2 Power",
    "text": "10.2 Power\nThere are typically two uses to hypothesis test: either we want to show it is not unreasonable to assume the null hypothesis (for example, assuming equal variance), or else we want to show beyond reasonable doubt that a difference or effect is significative: for example, one could wish to demonstrate that a new website design (alternative hypothesis) leads to a significant increase in sales relative to the status quo (null hypothesis).\nOur ability make discoveries depends on the power of the test: the larger the power, the greater our ability to reject the null hypothesis \\(\\mathscr{H}_0\\) when the latter is false.\nThe power of a test is the probability of correctly rejecting the null hypothesis \\(\\mathscr{H}_0\\) when \\(\\mathscr{H}_0\\) is false, i.e., \\[\\begin{align*}\n\\mathsf{Pr}_a(\\text{reject} \\mathscr{H}_0)\n\\end{align*}\\] Whereas the null alternative corresponds to a single value (equality in mean), there are infinitely many alternatives… Depending on the alternative models, it is more or less easy to detect that the null hypothesis is false and reject in favour of an alternative. Power is thus a measure of our ability to detect real effects. Different test statistics can give broadly similar conclusions despite being based on different benchmark. Generally, however, there will be a tradeoff between the number of assumptions we make about our data or model (the fewer, the better) and the ability to draw conclusions when there is truly something going on when the null hypothesis is false.\n\n\n\n\n\n\n\n\nFigure 10.3: Comparison between null distribution (full curve) and a specific alternative for a t-test (dashed line). The power corresponds to the area under the curve of the density of the alternative distribution which is in the rejection area (in white).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.4: Increase in power due to an increase in the mean difference between the null and alternative hypothesis. Power is the area in the rejection region (in white) under the alternative distribution (dashed): the latter is more shifted to the right relative to the null distribution (full line).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.5: Increase of power due to an increase in the sample size or a decrease of standard deviation of the population: the null distribution (full line) is more concentrated. Power is given by the area (white) under the curve of the alternative distribution (dashed). In general, the null distribution changes with the sample size.\n\n\n\n\n\nWe want to choose an experimental design and a test statistic that leads to high power, so that this power is as close as possible to one. Under various assumptions about the distribution of the original data, we can derive optimal tests that are most powerful, but some of the power comes from imposing more structure and these assumptions need not be satisfied in practice.\nMinimally, the power of the test should be \\(\\alpha\\) because we reject the null hypothesis \\(\\alpha\\) fraction of the time even when \\(\\mathscr{H}_0\\) is true. Power depends on many criteria, notably\n\nthe effect size: the bigger the difference between the postulated value for \\(\\theta_0\\) under \\(\\mathscr{H}_0\\) and the observed behaviour, the easier it is to detect departures from \\(\\theta_0\\). (Figure 10.5); it’s easier to spot an elephant in a room than a mouse.\nvariability: the less noisy your data, the easier it is to assess that the observed differences are genuine, as Figure 10.4 shows;\nthe sample size: the more observation, the higher our ability to detect significative differences because the amount of evidence increases as we gather more observations.5 In experimental designs, the power also depends on how many observations are allocated to each group.6\nthe choice of test statistic: there is a plethora of possible statistics to choose from as a summary of the evidence against the null hypothesis. Choosing and designing statistics is usually best left out to statisticians, as there may be tradeoffs. For example, rank-based statistics discard information about the observed values of the response, focusing instead on their relative ranking. The resulting tests are typically less powerful, but they are also less sensible to model assumptions, model misspecification and outliers.\n\nChanging the value of \\(\\alpha\\) also has an impact on the power, since larger values of \\(\\alpha\\) move the cutoff towards the bulk of the distribution. However, it entails a higher percentage of rejection also when the alternative is false. Since the value of \\(\\alpha\\) is fixed beforehand to control the type I error (avoid judicial mistakes), it’s not a parameter we consider.\nThere is an intricate relation between effect size, power and sample size. Journals and grant agencies oftentimes require an estimate of the latter before funding a study, so one needs to ensure that the sample size is large enough to pick-up effects of scientific interest (good signal-to-noise), but also not overly large as to minimize time and money and make an efficient allocation of resources. This is Goldilock’s principle, but having more never hurts.\nIf we run a pilot study to estimate the background level of noise and the estimated effect, or if we wish to perform a replication study, we will come up with a similar question in both cases: how many participants are needed to reliably detect such a difference? Setting a minimum value for the power (at least 80%, but typically 90% or 95% when feasible) ensures that the study is more reliable and ensures a high chance of success of finding an effect of at least the size specified. A power of 80% ensures that, on average, 4 in 5 experiments in which we study a phenomenon with the specified non-null effect size should lead to rejecting the null hypothesis.\nIn order to better understand the interplay between power, effect size and sample size, we consider a theoretical example. The purpose of displaying the formula is to (hopefully) more transparently confirm some of our intuitions about what leads to higher power. There are many things that can influence the power:\n\nthe experimental design: a blocking design or repeated measures tend to filter out some of the unwanted variability in the population, thus increasing power relative to a completely randomized design\nthe background variability \\(\\sigma\\): the noise level is oftentimes intrinsic to the measurement. It depends on the phenomenon under study, but instrumentation and the choice of scale, etc. can have an impact. Running experiments in a controlled environment helps reduce this, but researchers typically have limited control on the variability inherent to each observation.\nthe sample size: as more data are gathered, information accumulates. The precision of measurements (e.g., differences in mean) is normally determined by the group with the smallest sample size, so (approximate) balancing increases power if the variance in each group is the same.\nthe size of the effect: the bigger the effect, the easier it is to accurately detect (it’s easier to spot an elephant than a mouse hiding in a classroom).\nthe level of the test, \\(\\alpha\\): if we increase the rejection region, we technically increase power when we run an experiment under an alternative regime. However, the level is oftentimes prespecified to avoid type I errors. We may consider multiplicity correction within the power function, such as Bonferonni’s method, which is equivalent to reducing \\(\\alpha\\).\n\n\n10.2.1 Power for one-way ANOVA\nTo fix ideas, we consider the one-way analysis of variance model. In the usual setup, we consider \\(K\\) experimental conditions with \\(n_k\\) observations in group \\(k\\), whose population average we denote by \\(\\mu_k\\). We can parametrize the model in terms of the overall sample average, \\[\\begin{align*}\n\\mu = \\frac{1}{n}\\sum_{j=1}^K\\sum_{i=1}^{n_j} \\mu_j = \\frac{1}{n}\\sum_{j=1}^K n_j \\mu_j,\n\\end{align*}\\] where \\(n=n_1 + \\cdots +n_K\\) is the total sample size. The \\(F\\)-statistic of the one-way ANOVA is \\[\\begin{align*}\nF =  \\frac{\\text{between sum of squares}/(K-1)}{\\text{within sum of squares}/(n-K)}\n\\end{align*}\\] The null distribution is \\(F(K-1, n-K)\\). Our interest is in understanding how the F-statistic behaves under an alternative.\nDuring the construction, we stressed out that the denominator is an estimator of \\(\\sigma^2\\) under both the null and alternative. What happens to the numerator? We can write the population average for the between sum of square as \\[\n\\mathsf{E}(\\text{between sum of squares}) = \\sigma^2\\{(K-1) + \\Delta\\}.\n\\] where \\[\n\\Delta = \\dfrac{\\sum_{j=1}^K n_j(\\mu_j - \\mu)^2}{\\sigma^2} = nf^2.\n\\] and where \\(f^2\\) is the square of Cohen’s \\(f\\). Under the null hypothesis, all group means are equal and \\(\\mu_j=\\mu\\) for \\(j=1, \\ldots, K\\) and \\(\\Delta=0\\), but if some groups have different average the displacement will be non-zero. The greater \\(\\Delta\\), the further the mode (peak of the distribution) is from unity and the greater the power.\nCloser examination reveals that \\(\\Delta\\) increases with \\(n_j\\) (sample size) and with the true squared mean difference \\((\\mu_j-\\mu)^2\\) increases effect size represented by the difference in mean, but decreases as the observation variance increases.\nUnder the alternative, the distribution of the \\(F\\) statistic is a noncentral Fisher distribution, denoted \\(\\mathsf{F}(\\nu_1, \\nu_2, \\Delta)\\) with degrees of freedom \\(\\nu_1\\) and \\(\\nu_2\\) and noncentrality parameter \\(\\Delta\\).7 To calculate the power of a test, we need to single out a specific alternative hypothesis.\n\n\n\n\n\n\n\n\nFigure 10.6: Density curves for the null distribution (full line) and true distribution (dashed line) under noncentrality parameter \\(\\Delta=3\\). The area in white under the curve denotes the power under this alternative.\n\n\n\n\n\nThe plot in Figure 10.6 shows the null (full line) distribution and the true distribution (dashed line) for a particular alternative. The noncentral \\(\\mathsf{F}\\) is shifted to the right and right skewed, so the mode (peak) is further away from 1.\nGiven a value of \\(\\Delta=nf^2\\) and information about the effect of interest (degrees of freedom of the effect and the residuals), we can compute the tail probability as follows\n\nCompute the cutoff point: the value under \\(\\mathscr{H}_0\\) that leads to rejection at level \\(\\alpha\\)\nCompute probability below the alternative curve, from the cutoff onwards.\n\n\ncutoff &lt;- qf(p = 1-alpha, df1 = df1, df2 = df2)\npf(q = cutoff,  df1 = df1, df2 = df2, \n    ncp = Delta, lower.tail = FALSE)\n\n\n\n10.2.2 Power calculations\nIn practice, a software will return these quantities and inform us about the power. Note that these results are trustworthy provided the model assumptions are met, otherwise they may be misleading.\nThe most difficult question when trying to estimate sample size for a study is determining which value to use for the effect size. One could opt for a value reported elsewhere for a similar scale to estimate the variability and provide educated guesses for the mean differences. Another option is to run a pilot study and use the resulting estimates to inform about sensible values, perhaps using confidence intervals to see the range of plausible effect sizes. Keep in mind the findings from Figure 10.2.\nReliance on estimated effect sizes reported in the literature is debatable: many such effects are inflated as a result of the file-drawer problem and, as such, can lead to unreasonably high expectations about power.\nThe WebPower package in R offers a comprehensive solution for conducting power studies, as does the free software G*Power. We present a range of examples from a replication study: the following quotes are taken from the Reproducibility Project: Psychology.\n\nExample 10.4 (Power calculation for between subject ANOVA) The following extract of Jesse Chandler’s replication concerns Study 4b by Janiszewski and Uy (2008), which considers a \\(2 \\times 2\\) between-subject ANOVA.\n\nIn Study 4a there are two effects of theoretical interest, a substantial main effect of anchor precision that replicates the first three studies and a small interaction (between precision and motivation within which people can adjust) that is not central to the paper. The main effect of anchor precision (effect size \\(\\eta^2_p=0.55\\)) would require a sample size of \\(10\\) for \\(80\\)% power, \\(12\\) for \\(90\\)% power, and \\(14\\) for \\(95\\)% power. The interaction (effect size \\(\\eta^2_p=0.11\\)) would require a sample size of \\(65\\) for \\(80\\)% power, \\(87\\) for \\(90\\)% power, and \\(107\\) for \\(95\\)% power. There was also a theoretically uninteresting main effect of motivation (people adjust more when told to adjust more).\n\nIn order to replicate, we must first convert estimates of \\(\\eta^2_p\\) to Cohen’s \\(f\\), which is the input accepted by both WebPower and G*Power. We compute the sample size for power 95% for both the main effect and the interaction: in practice, we would pick the smaller of the two (or equivalently the larger resulting sample size estimate) should we wish to replicate both findings.\n\nf &lt;- effectsize::eta2_to_f(0.55) # convert eta-squared to Cohen's f\nng &lt;- 4 # number of groups for the ANOVA\nceiling(WebPower::wp.kanova(ndf = 1, ng = ng, f = f, power = 0.95)$n)\n\n[1] 14\n\nf &lt;- effectsize::eta2_to_f(0.11)\nceiling(WebPower::wp.kanova(ndf = 1, ng = ng, f = f, power = 0.95)$n)\n\n[1] 108\n\n\nWe can see that the numbers match the calculations from the replication (up to rounding).\n\n\nExample 10.5 (Power calculation for mixed design) Repeated measures ANOVA have different characteristics from between-subject design in that measurements are correlated, and we can also provide correction for sphericity. These additional parameters need to specified by users. In WebPower, the wp.rmanova function. We need to specify the number of measurements per person, the number of groups, the value \\(\\epsilon\\) for the sphericity correction, e.g., the output of Greenhouse–Geisser or Huynh–Feldt and the type of effect for between-subject factor, within-subject factor or an interaction between the two.\n\nThe result that is object of this replication is the interaction between item strength (massed vs. spaced presentation) and condition (directed forgetting vs. control). The dependent variable is the proportion of correctly remembered items from the stimulus set (List 1). “(..) The interaction was significant, \\(F(1,94)=4.97\\), \\(p &lt;.05\\), \\(\\mathrm{MSE} =0.029\\), \\(\\eta^2=0.05\\), (…)”. (p. 412). Power analysis (G*Power (Version 3.1): ANOVA: Repeated measures, within-between interaction with a zero correlation between the repeated measures) indicated that sample sizes for \\(80\\)%, \\(90\\)% and \\(95\\)% power were respectively \\(78\\), \\(102\\) and \\(126\\).\n\nWe are thus considering a \\(2 \\times 2\\) within-between design. The estimated effect size is \\(\\widehat{\\eta}^2_p=0.05\\), with Cohen’s \\(f\\), where the value is multiplied by a constant \\(C\\); see the WebPower page which depends on the number of groups, the number of repeated measurements \\(K\\) and their correlation \\(\\rho\\). For the interaction, the correction factor is \\(C = \\sqrt{K/(1-\\rho)}\\): taking \\(K=2\\) and \\(\\rho=0\\), we get a Cohen’s \\(f\\) of 0.23. We calculate the sample size for a power of 90% changing \\(\\rho\\): if we change the correlation in the calculation from zero to \\(0.5\\), we can see that there is a significant decrease in the sample size.\n\nf &lt;- effectsize::eta2_to_f(0.05) # convert eta-squared to Cohen's f\nrho &lt;- 0 # correlation between measurements\nK &lt;- 2L # number of repeated measurements\nround(WebPower::wp.rmanova(type = 2, # interaction \n                           nm = K, # number of measurement per subject\n                           ng = 2, # number of groups for between,\n                           f = f*sqrt(K/(1-rho)), # scaled effect size\n                           power = 0.9)$n) # requested power\n\n[1] 102\n\n\n\n\n\n\n\n\n\n\nFigure 10.7: Screenshot of G*Power for the calculation of the sample size to replicate the interaction in a repeated measures (within-between) analysis of variance.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.8: Sample size requirement for a within-between interaction in a two by two (within-between) ANOVA with an effect size of \\(\\widehat{\\eta}^2_p = 0.05\\) as a function of correlation and power. The staircase pattern is an artefact of rounding up to the nearest integer.\n\n\n\n\n\nWe can see that the more correlated the response, the smaller sample size requirement according to Figure 10.8. Higher power requirement leads to larger data collection efforts.\n\n\nExample 10.6 (Power calculation for a two sample \\(t\\)-test) We consider a two-sample comparison, as these arise frequently from contrasts. The replication wishes to replicate a study whose estimated effect size was a Cohen’s \\(d\\) of \\(\\widehat{d} = 0.451\\). Using a two-tailed test with a balanced sample and \\(\\alpha = 0.05\\) type I error rate, we obtain \\(258\\) participants. Note that some software, as below, report the sample size by group so the total sample size is twice the number reported.\n\n2*ceiling(WebPower::wp.t(\n  type = \"two.sample\",\n  alternative = \"two.sided\",\n  d = 0.451, # Cohen's f\n  power = 0.95)$n) # power requirement\n\n[1] 258\n\n\nThe function also allows us to figure out the effect size one could obtain for given power and fixed sample size, by replacing d with the latter via arguments n1 and n2.\n\n\n\n10.2.3 Power in complex designs\nIn cases where an analytic derivations isn’t possible, we can resort to simulations to approximate the power. For a given alternative, we\n\nsimulate repeatedly samples from the model from the hypothetical alternative world\nwe compute the test statistic for each of these new samples\nwe transform these to the associated p-values based on the postulated null hypothesis.\n\nAt the end, we calculate the proportion of tests that lead to a rejection of the null hypothesis at level \\(\\alpha\\), namely the percentage of p-values smaller than \\(\\alpha\\). We can vary the sample size and see how many observations we need per group to achieve the desired level of power.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Effect sizes and power</span>"
    ]
  },
  {
    "objectID": "power_effect.html#additional-r-examples",
    "href": "power_effect.html#additional-r-examples",
    "title": "10  Effect sizes and power",
    "section": "10.3 Additional R examples",
    "text": "10.3 Additional R examples\nEffect size typically serve three purpose:\n\ninform readers of the magnitude of the effect,\nprovide a standardized quantity that can be combined with others in a meta-analysis, or\nserve as proxy in a power study to estimate the minimum number of observations needed.\n\nIf you report the exact value of the test statistic, the null distribution and (in short) all elements of an analysis of variance table in a complex design, it is possible by using suitable formulae to recover effect sizes, as they are functions of the test statistic summaries, degrees of freedom and correlation between observations (in the case of repeated measures).\nThe effectsize package includes a variety of estimators for standardized difference or ratio of variance. For example, for the latter, we can retrieve Cohen’s \\(f\\) via cohens_f, \\(\\widehat{\\epsilon}^2\\) via epsilon_squared or \\(\\widehat{\\omega}^2\\) via omega_squared. By default, in a design with more than one factor, the partial effects are returned (argument partial = TRUE) — if there is a single factor, these coincide with the total effects and the distinction is immaterial.\nThe effectsize package reports confidence intervals8 calculated using the pivot method described in Steiger (2004). Check the documentation at ?effectsize::effectsize_CIs for more technical details.9\nIn general, confidence intervals for effect sizes are very wide, including a large range of potential values and sometimes zero. This reflects the large uncertainty surrounding their estimation and should not be taken to mean that the estimated effect is null.\n\nExample 10.7 (Effect size and power for a one-way ANOVA) We begin with the result of another one-way ANOVA using data from Baumann, Seifert-Kessell, and Jones (1992). If we consider the global \\(F\\)-test of equality in means, we can report as corresponding effect size the percentage of variance that is explained by the experimental condition, group.\n\nlibrary(effectsize)\ndata(BSJ92, package = \"hecedsm\")\nmod &lt;- aov(posttest2 - pretest2 ~ group,\n           data = BSJ92)\nprint_md(omega_squared(anova(mod), partial = FALSE))\n\n\nEffect Size for ANOVA (Type I)\n\n\nParameter\nOmega2\n95% CI\n\n\n\n\ngroup\n0.16\n[0.03, 1.00]\n\n\n\nOne-sided CIs: upper bound fixed at [1.00].\n\n\nThe estimator employed is \\(\\widehat{\\omega}^2\\) and could be obtained directly using the formula provided in the slides. For a proportion of variance, the number is medium according to Cohen (1988) definition. Using \\(\\widehat{R}^2 \\equiv \\widehat{\\eta}^2\\) as estimator instead would give an estimated proportion of variance of 0.188, a slightly higher number.\nHaving found a significant difference in mean between groups, one could be interested in computing estimated marginal means and contrasts based on the latter. The emmeans function has a method for computing effect size (Cohen’s \\(d\\)) for pairwise differences if provided with the denominator standard deviation \\(\\sigma\\) and the degrees of freedom associated with the latter (i.e., how many observations were left from the total sample size after subtracting the number of subgroup means).\nThe confidence intervals reported by emmeans for \\(t\\)-tests are symmetric and different in nature from the one obtained previously.\nTechnical aside: while it is possible to create a \\(t\\)-statistic for a constrast by dividing the contrast estimator by it’s standard error, the construction of Cohen’s \\(d\\) here for the contrast consisting of, e.g., the pairwise difference between DRTA and TA would take the form \\[\nd_{\\text{DRTA}- \\text{TA}} = \\frac{\\mu_{\\text{DRTA}}- \\mu_{\\text{TA}}}{\\sigma},\n\\] where the denominator stands for the standard deviation of the observations.10\n\n\nExample 10.8 (Sample size for replication studies) Armed with effect sizes and a desired level of power, it is possible to determine the minimum number of observations that would yield such effect. Johnson, Cheung, and Donnellan (2014) performs a replication study of Schnall, Benton, and Harvey (2008) who conjectured that physical cleanliness reduces the severity of moral judgments. The following excerpt from the paper explain how sample size for the replication were calculated.\n\nIn Experiment 2, the critical test of the cleanliness manipulation on ratings of morality was significant, \\(F(1, 41) = 7.81\\), \\(p=0.01\\), \\(d=0.87\\), \\(N=44\\). Assuming \\(\\alpha=0.05\\), the achieved power in this experiment was \\(0.80\\). Our proposed research will attempt to replicate this experiment with a level of power = \\(0.99\\). This will require a minimum of 100 participants (assuming equal sized groups with \\(d=0.87\\)) so we will collect data from 115 participants to ensure a properly powered sample in case of errors.\n\nThe first step is to try and compute the effect size, here Cohen’s \\(d\\), from the reported \\(F\\) statistic to make sure it matches the quoted value.\nThis indeed coincides with the value reported for Cohen’s \\(d\\) estimator. We can then plug-in this value in the power function with the desired power level \\(0.99\\) to find out a minimal number of 50 participants in each group, for a total of 100 if we do a pairwise comparison using a two-sample \\(t\\)-test.\n\n\nTwo-sample t-test\n\n           n    d alpha power\n    49.53039 0.87  0.05  0.99\n\nNOTE: n is number in *each* group\nURL: http://psychstat.org/ttest\n\n\nThe effectsize package includes many functions to convert \\(F\\) and \\(t\\) statistics to effect sizes.11\n\n\nExample 10.9 (Power calculation for a two-way ANOVA with unbalanced data) While software can easily compute effect sizes, the user should not blindly rely on the output, but rather think about various elements using the following guiding principles:\n\nwe are interested in partial effects when there are multiple factors\nthe denominator should consist of the variance of the effect of interest (say factor \\(A\\)), the variance of blocking factors and random effects and that of all interactions associated with them.\n\nConsider next the unbalanced two-way ANOVA example in Study 1 of Maglio and Polman (2014). We pass here directly the output of the model. We use the lm function with the mean-to-zero parametrization, since we have unbalanced data.\nBy default, the variance terms for each factor and interaction are estimated using the anova call. When the data aren’t balanced and you have multiple factors in the mean equation, these are the sequential sum of square estimates (type I). This means that the resulting effect size would depend on the order in which you specify the terms, an unappealing feature. The model can alternatively take as argument the analysis of variance table produced by the Anova function in package car, e.g., car::Anova(..., type = 3). Note that it is of paramount importance to pass the correct arguments and to use the mean-to-zero parametrization in order to get sensible results. The package warns users about this.\n\n\nType 3 ANOVAs only give sensible and informative results when covariates\n  are mean-centered and factors are coded with orthogonal contrasts (such\n  as those produced by `contr.sum`, `contr.poly`, or `contr.helmert`, but\n  *not* by the default `contr.treatment`).\n\n\nThe estimated effect size for the main effect of direction is negative with \\(\\widehat{\\omega}^2_{\\langle \\text{direction}\\rangle}\\): either reporting a negative value or zero. This reflects that the estimated effect is very insignificant.\nEquipped with the estimated effect size, we can now transform our partial \\(\\widehat{\\omega}^2_{\\langle\\text{AB}\\rangle}\\) measure into an estimated Cohen’s \\(f\\) via \\[\\widetilde{f} = \\left( \\frac{\\widehat{\\omega}^2}{1-\\widehat{\\omega}^2}\\right)^{1/2},\\] which is then fed into WebPower package functionality to compute the post-hoc power. Since we are dealing with a two-way ANOVA with 8 subgroups, we set ng=8 and then ndf corresponding to the degrees of freedom of the estimated interaction (here \\((n_a-1)\\times (n_b-1)=3\\), the number of coefficients needed to capture the interaction).\nGiven all but one of the following collection\n\nthe power,\nthe number of groups and degrees of freedom from the design,\nthe effect size and\nthe sample size,\n\nit is possible to deduce the last one assuming a balanced sample. Below, we use the information to compute the so-called post-hoc power. Such terminology is misleading because there is no guarantee that we are under the alternative, and effect sizes are really noisy proxy so the range of potential values for the missing ingredient is oftentimes quite large. Because studies in the literature have inflated effect size, the power measures are more often than not misleading.\n\n\nMultiple way ANOVA analysis\n\n      n ndf ddf         f ng alpha     power\n    202   3 194 0.4764222  8  0.05 0.9999821\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nHere, the interaction is unusually strong (a fifth of the variance is explained by it!) and we have an extremely large post-hoc power estimate. This is rather unsurprising given the way the experiment was set up.\nWe can use the same function to determine how many observations the study would need to minimally achieve a certain power, below of 99% — the number reported must be rounded up to the nearest integer. Depending on the design or function, this number may be the overall sample size or the sample size per group.\n\n\nType 3 ANOVAs only give sensible and informative results when covariates\n  are mean-centered and factors are coded with orthogonal contrasts (such\n  as those produced by `contr.sum`, `contr.poly`, or `contr.helmert`, but\n  *not* by the default `contr.treatment`).\n\n\nMultiple way ANOVA analysis\n\n        n ndf      ddf         f ng alpha power\n    107.8   3 99.80004 0.4764222  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nMultiple way ANOVA analysis\n\n           n ndf      ddf         f ng alpha power\n    97.61394   3 89.61394 0.5017988  8  0.05  0.99\n\nNOTE: Sample size is the total sample size\nURL: http://psychstat.org/kanova\n\n\nThe total sample size using \\(\\widehat{\\omega}^2\\) is 108, whereas using the biased estimator \\(\\widehat{f}\\) directly (itself obtained from \\(\\widehat{\\eta}^2\\)) gives 98: this difference of 10 individuals can have practical implications.\n\n\nExample 10.10 (Sample size calculation for a replication with a mixed design) You can download G*Power to perform these calculations. The following quote is taken from the Reproducibility Project: Psychology\n\nThe result that is object of this replication is the interaction between item strength (massed vs. spaced presentation) and condition (directed forgetting vs. control). The dependent variable is the proportion of correctly remembered items from the stimulus set (List 1). “(..) The interaction was significant, \\(F(1,94)=4.97\\), \\(p &lt;.05\\), \\(\\mathrm{MSE} =0.029\\), \\(\\eta^2=0.05\\), (…)”. (p. 412). Power analysis (G*Power (Version 3.1): ANOVA: Repeated measures, within-between interaction with a zero correlation between the repeated measures) indicated that sample sizes for \\(80\\)%, \\(90\\)% and \\(95\\)% power were respectively \\(78\\), \\(102\\) and \\(126\\).\n\nNot all software accept the same input, so we need to tweak the effect size to get the same answers. The user must specify whether we are looking at a within-subjet, between-subject contrast, or an interaction between the two, as is the case here. The latter in WebPower (for an interaction effect) should be \\(\\sigma_{\\text{effect}}/\\sigma_{\\text{resid}}  \\times C\\), where \\(C = \\sqrt{K/(1-\\rho)}\\), and where \\(K\\) is the number of groups and \\(\\rho\\) the correlation of within-subject measurements. We use the wp.rmanova function, specify the type (2 for interaction), the number of groups \\(K=2\\), the number of repeated measurements \\(2\\). The description uses a correlation \\(\\rho=0\\), which is a worst-case scenario: in practice, more correlation leads to reduced sample size requirements.\n\n\nRepeated-measures ANOVA analysis\n\n           n         f ng nm nscor alpha power\n    76.53314 0.3244428  2  2     1  0.05   0.8\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nRepeated-measures ANOVA analysis\n\n           n         f ng nm nscor alpha power\n    101.7801 0.3244428  2  2     1  0.05   0.9\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nRepeated-measures ANOVA analysis\n\n           n         f ng nm nscor alpha power\n    125.4038 0.3244428  2  2     1  0.05  0.95\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nConsider next a similar calculation for a repeated measure design:\n\nWe aim at testing the two main effects of prediction 1 and prediction 3. Given the \\(2 \\times 3\\) within factors design for both main effects, we calculated \\(\\eta^2_p\\) based on \\(F\\)-Values and degrees of freedom. This procedure resulted in \\(\\eta^2_p=0.427\\) and \\(\\eta^2_p=0.389\\) for the effect of prediction 1 (\\(F(1, 36)=22.88\\)) and prediction 3 (\\(F(1, 36)=26.88\\)), respectively. Accordingly, G*Power (Version 3.1) indicates that a power of \\(80\\)%, \\(90\\)%, and \\(95\\)% is achieved with sample sizes of \\(3\\), \\(4\\), and \\(4\\) participants, respectively, for both effects (assuming a correlation of \\(r=0.5\\) between repeated measures in all power calculations).\n\n\nf &lt;- effectsize::eta2_to_f(0.389) * sqrt(6/(1-0.5))\n\nWe get an error message because the sample size is smaller than the number of measurements, but we can still compute the power for a given sample size.\n\n\nRepeated-measures ANOVA analysis\n\n    n        f ng nm nscor alpha     power\n    3 2.764043  1  6     1  0.05 0.8231387\n    4 2.764043  1  6     1  0.05 0.9631782\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\n\nRepeated-measures ANOVA analysis\n\n    n        f ng nm nscor alpha     power\n    3 2.990386  1  6     1  0.05 0.8835981\n    4 2.990386  1  6     1  0.05 0.9836138\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\n\nThe WebPower package also has a graphical interface online for effect size calculations.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nEffect sizes are used to provide a standardized measure of the strength of a result, independent of the design and the sample size.\nThere are two classes: standardized differences and proportions of variance.\nMultiple estimators exists: report the latter along with the software used to compute confidence intervals.\nThe adequate measure of variability to use for the effect size depends on the design: we normally include the variability of blocking factors and residual variance.\nGiven a design, we can deduce either the sample size, the power or the effect size from the other two metrics. This allows us to compute sample size for a study or replication.\n\n\n\n\n\n\n\nBaumann, James F., Nancy Seifert-Kessell, and Leah A. Jones. 1992. “Effect of Think-Aloud Instruction on Elementary Students’ Comprehension Monitoring Abilities.” Journal of Reading Behavior 24 (2): 143–72. https://doi.org/10.1080/10862969209547770.\n\n\nCohen, J. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. New York: Routledge. https://doi.org/10.4324/9780203771587.\n\n\nHedges, Larry V. 1981. “Distribution Theory for Glass’s Estimator of Effect Size and Related Estimators.” Journal of Educational Statistics 6 (2): 107–28. https://doi.org/10.3102/10769986006002107.\n\n\nJaniszewski, Chris, and Dan Uy. 2008. “Precision of the Anchor Influences the Amount of Adjustment.” Psychological Science 19 (2): 121–27. https://doi.org/10.1111/j.1467-9280.2008.02057.x.\n\n\nJohnson, David J., Felix Cheung, and M. Brent Donnellan. 2014. “Does Cleanliness Influence Moral Judgments?” Social Psychology 45 (3): 209–15. https://doi.org/10.1027/1864-9335/a000186.\n\n\nKeppel, G., and T. D. Wickens. 2004. Design and Analysis: A Researcher’s Handbook. Pearson Prentice Hall.\n\n\nLakens, Daniel. 2013. “Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for \\(t\\)-Tests and ANOVAs.” Frontiers in Psychology 4: 863. https://doi.org/10.3389/fpsyg.2013.00863.\n\n\nLiu, Peggy J., SoYon Rim, Lauren Min, and Kate E. Min. 2023. “The Surprise of Reaching Out: Appreciated More Than We Think.” Journal of Personality and Social Psychology 124 (4): 754–71. https://doi.org/10.1037/pspi0000402.\n\n\nMaglio, Sam J., and Evan Polman. 2014. “Spatial Orientation Shrinks and Expands Psychological Distance.” Psychological Science 25 (7): 1345–52. https://doi.org/10.1177/0956797614530571.\n\n\nMagnusson, Kristoffer. 2021. “Interpreting Cohen’s \\(d\\) Effect Size: An Interactive Visualization.” https://rpsychologist.com/cohend/.\n\n\nSteiger, James H. 2004. “Beyond the \\(F\\) Test: Effect Size Confidence Intervals and Tests of Close Fit in the Analysis of Variance and Contrast Analysis.” Psychological Methods 9: 164–82. https://doi.org/10.1037/1082-989X.9.2.164.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Effect sizes and power</span>"
    ]
  },
  {
    "objectID": "power_effect.html#footnotes",
    "href": "power_effect.html#footnotes",
    "title": "10  Effect sizes and power",
    "section": "",
    "text": "If we consider a balanced sample, \\(n_1 = n_2 = n/2\\) we can rewrite the statistic as \\(T = \\sqrt{n} \\widehat{d}_s/2\\) and the statement that \\(T\\) increases with \\(n\\) on average becomes more obvious.↩︎\nBy using the pivot method, e.g., Steiger (2004), and relating the effect size to the noncentrality parameter of the null distribution, whether \\(\\mathsf{St}\\), \\(\\mathsf{F}\\) or \\(\\chi^2\\).↩︎\nThe confidence intervals are based on the \\(\\mathsf{F}\\) distribution, by changing the non-centrality parameter and inverting the distribution function (pivot method). This yields asymmetric intervals.↩︎\nTypically, there won’t be any interaction with blocking factors, but it there was for some reason, it should be included in the total.↩︎\nSpecifically, the standard error decreases with sample size \\(n\\) at a rate (typically) of \\(n^{-1/2}\\). The null distribution also becomes more concentrated as the sample size increase.↩︎\nWhile the default is to assign an equal number to each subgroup, power may be maximized by specifying different sample size in each group if the variability of the measurement differ in these groups.↩︎\nNote that the \\(F(\\nu_1, \\nu_2)\\) distribution is indistinguishable from \\(\\chi^2(\\nu_1)\\) for \\(\\nu_2\\) large. A similar result holds for tests with \\(\\chi^2\\) null distributions.↩︎\nReally, these are fiducial intervals based on confidence distributions.↩︎\nNote that, when the test statistic representing the proportion of variance explained is strictly positive, like a \\(F\\) or \\(\\chi^2\\) statistic, the corresponding effect size is an estimated percentage of variance returned by, e.g., \\(\\widehat{\\omega}^2\\). To ensure consistency, the confidence intervals are one-sided, giving a lower bound (for the minimum effect size compatible with the data), while the upper bound is set to the maximum value, e.g., 1 for a proportion.↩︎\nIt isn’t always obvious when marginalizing out a one-way ANOVA from a complex design or when we have random effects or blocking factor what the estimated standard deviation should be, so it is left to the user to specify the correct quantity.↩︎\nAs the example of Baumann, Seifert-Kessell, and Jones (1992) showed, however, not all statistics can be meaningfully converted to effect size.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Effect sizes and power</span>"
    ]
  },
  {
    "objectID": "reproducibility_crisis.html",
    "href": "reproducibility_crisis.html",
    "title": "11  Replication crisis",
    "section": "",
    "text": "11.1 Causes of the replication crisis\nIn recent years, many team efforts have performed so-called replications of existing methodological papers to assess the robustness of their findings. Perhaps unsurprisingly, many replications failed to yield anything like what authors used to claim, or found much weaker findings. This chapter examines some of the causes of this lack of replicability.\nWe adopt the terminology of Claerbout and Karrenbach (1992): a study is said to be reproducible if an external person with the same data and enough indications about the procedure (for example, the code and software versions, etc.) can obtain consistent results that match those of a paper. A related scientific matter is replicability, which is the process by which new data are collected to test the same hypothesis, potentially using different methodology. Reproducibility is important because it enhances the credibility of one’s work. Extensions that deal with different analyses leading to the same conclusion are described in The Turing Way and presented in Figure 11.1.\nWhy is reproducibility and replicability important? In a thought provoking paper, Ioannidis (2005) claimed that most research findings are wrong. The abstract of his paper stated\nSince its publication, collaborative efforts have tried to assess the scale of the problem by reanalysing data and trying to replicate the findings of published research. For example, the “Reproducibility [sic] Project: Psychology” (Nosek et al. 2015)\nA large share of findings in the review were not replicable or the effects were much smaller than claimed, as shown by Figure 2 from the study. Such findings show that the peer-review procedure is not foolproof: the “publish-or-perish” mindset in academia is leading many researchers to try and achieve statistical significance at all costs to meet the 5% level criterion, whether involuntarily or not. This problem has many names: \\(p\\)-hacking, harking or to paraphrase a story of Jorge Luis Borges, the garden of forking paths. There are many degrees of freedom in the analysis for researchers to refine their hypothesis after viewing the data, conducting many unplanned comparisons and reporting selected results.\nAnother problem is selective reporting. Because a large emphasis is placed on statistical significance, many studies that find small effects are never published, resulting in a gap. Figure 11.3 from Zwet and Cator (2021) shows \\(z\\)-scores obtained by transforming confidence intervals reported in Barnett and Wren (2019). The authors used data mining techniques to extract confidence intervals from abstracts of nearly one million publication in Medline published between 1976 and 2019. If most experiments yielded no effect and were due to natural variability, the \\(z\\)-scores should be normally distributed, but Figure 11.3 shows a big gap in the bell curve between approximately \\(-2\\) and \\(2\\), indicative of selective reporting. The fact that results that do not lead to \\(p &lt; 0.05\\) are not published is called the file-drawer problem.\nThe ongoing debate surrounding the reproducibility crisis has sparked dramatic changes in the academic landscape: to enhance the quality of studies published, many journal now require authors to provide their code and data, to pre-register their studies, etc. Teams lead effort (e.g., the Experimental Economics Replication Project) try to replicate studies, with mitigated success so far. This inside recollection by a graduate student shows the extent of the problem.\nThis course will place a strong emphasis on identifying and avoiding statistical fallacies and showcasing methods than enhance reproducibility. How can reproducible research enhance your work? For one thing, this workflow facilitates the publication of negative research, forces researchers to think ahead of time (and receive feedback). Reproducible research and data availability also leads to additional citations and increased credibility as a scientist.\nAmong good practices are\nKeeping a logbook and documenting your progress helps your collaborators, reviewers and your future-self understand decisions which may seem unclear and arbitrary in the future, even if they were the result of a careful thought process at the time you made them. Given the pervasiveness of the garden of forking paths, pre-registration helps you prevents harking because it limits selective reporting and unplanned tests, but it is not a panacea. Critics often object to pre-registration claiming that it binds people. This is a misleading claim in my view: pre-registration doesn’t mean that you must stick with the plan exactly, but merely requires you to explain what did not go as planned if anything.\nVersion control keeps records of changes to your file and can help you retrieve former versions if you make mistakes at some point.\nArchival of data helps to avoid unintentional and irreversible manipulations of the original data, examples of which can have large scale consequences as illustrated in Figure 11.4, who report flaws in genetic journals due to the automatic conversion of gene names to dates in Excel. These problems are far from unique. While sensitive data cannot be shared “as is” because of confidentiality issues, in many instances the data can and should be made available with a licence and a DOI to allow people to reuse it, cite and credit your work.\nTo enforce reproducibility, many journals now have policy regarding data, material and code availability. Some journals encourage such, while the trend in recent years has been to enforce. For example, Nature require the following to be reported in all published papers:\nBelow are multiple (non-exclusive) explanations for the lack of replication of study findings.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Replication crisis</span>"
    ]
  },
  {
    "objectID": "reproducibility_crisis.html#causes-of-the-replication-crisis",
    "href": "reproducibility_crisis.html#causes-of-the-replication-crisis",
    "title": "11  Replication crisis",
    "section": "",
    "text": "11.1.1 The garden of forking paths\nThe garden of forking paths, named after a novel of Borges, is a term coined by Andrew Gelman to refer to researchers’ degrees of freedom. With vague hypothesis and data collection rules, it is easy for the researcher to adapt and interpret the conclusions in a way that fits his or her chosen narratives. In the words of Gelman and Loken (2014)\n\nGiven a particular data set, it can seem entirely appropriate to look at the data and construct reasonable rules for data exclusion, coding, and analysis that can lead to statistical significance. In such a case, researchers need to perform only one test, but that test is conditional on the data.\n\nThis user case is not accomodated by classical testing theory. Research hypothesis are often formulated in a vague way, such that different analysis methods, tests may be compatible. Abel et al. (2022) recent preprint found that preregistration alone did not solve this problem, but that publication bias in randomized control trial was alleviated by publication of pre-analysis plans. This is directly related to the garden of forking path.\n\n\n11.1.2 Selective reporting\nAlso known as the file-drawer problem, selective reporting occurs because publication of results that fail to reach statistical significance (sic) are harder to publish. In much the same way as multiple testing, if 20 researchers perform a study but only one of them writes a paper and the result is a fluke, then this indicates. There are widespread indications publication bias, as evidence by the distribution of \\(p\\)-values reported in papers. A recent preprint of a study found the prevalance to be higher in online experiments such as Amazon MTurks.\nP-hacking and the replication crisis has lead many leading statisticians to advocate much more stringent cutoff criterion such as \\(p &lt; 0.001\\) instead of the usual \\(p&lt;0.05\\) criterion as level for the test. The level \\(\\alpha=5\\)% is essentially arbitrary and dates back to Fisher (1926), who wrote\n\nIf one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level.\n\n\n\n\n\n\n\nThinking outside the box\n\n\n\nMethods that pool together results, such as meta-analysis, are sensitive to selective reporting. Why does it matter?\n\n\n\n\n11.1.3 Non-representative samples\nMany researchers opt for convenience samples by using online panels such as Qualtrics, Amazon MTurks, etc. The quality of those observations is at best dubious: ask yourself whether you would answer such as survey for a small amount. Manipulation checks to ensure participants are following, information is not completed by bots, a threshold for the minimal time required to complete the study, etc. are necessary (but not sufficient) conditions to ensure that the data are not rubbish.\nA more important criticism is that the people who answer those surveys are not representative of the population as a whole: sampling bias thus plays an important role in the conclusions and, even if the summary statistics are not too different from the general population, they may exhibit different opinions, levels of skills, etc. than most.\n\n\n\n\n\n\n\n\nFigure 11.5: Sampling bias. Artwork by Jonathan Hey (Sketchplanations) shared under the CC BY-NC 4.0 license.\n\n\n\n\n\nThe same can be said of panels of students recruited in universities classes, who are more young, educated and perhaps may infer through backward induction the purpose of the study and answer accordingly.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Replication crisis</span>"
    ]
  },
  {
    "objectID": "reproducibility_crisis.html#summary",
    "href": "reproducibility_crisis.html#summary",
    "title": "11  Replication crisis",
    "section": "11.2 Summary",
    "text": "11.2 Summary\nOperating in an open-science environment should be seen as an opportunity to make better science, offer more opportunities to increase your impact and increase the likelihood that your work gets published regardless of whether the results turn out to be negative. It is the right thing to do and it increases the quality of research produced, with collateral benefits because it forces researchers to validate their methodology before, to double-check their data and their analysis and to adopt good practice.\nThere are many platforms for preregistering studies and sharing preanalysis plans, scripts and data, with different level of formality. One such is the Research Box.\n\n\n\n\n\n\nYour turn\n\n\n\nReflect on your workflow as applied researcher when designing and undertaking experiments. Which practical aspects could you improve upon to improve the reproducibility of your study?\n\n\n\n\n\n\nBarnett, Adrian Gerard, and Jonathan D Wren. 2019. “Examination of CIs in Health and Medical Journals from 1976 to 2019: An Observational Study.” BMJ Open 9 (11). https://doi.org/10.1136/bmjopen-2019-032506.\n\n\nClaerbout, Jon F., and Martin Karrenbach. 1992. “Electronic Documents Give Reproducible Research a New Meaning.” SEG Technical Program Expanded Abstracts. https://doi.org/https://doi.org/10.1190/1.1822162.\n\n\nFisher, Ronald A. 1926. “The Arrangement of Field Experiments.” Journal of the Ministry of Agriculture 33: 503–15. https://doi.org/10.23637/rothamsted.8v61q.\n\n\nGelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in Science.” American Scientist 102: 460–65.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings Are False.” PLOS Medicine 2 (8). https://doi.org/10.1371/journal.pmed.0020124.\n\n\nNosek, Brian, Johanna Cohoon, Mallory Kidwell, and Jeffrey Spies. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251). https://doi.org/10.1126/science.aac4716.\n\n\nZwet, Erik W. van, and Eric A. Cator. 2021. “The Significance Filter, the Winner’s Curse and the Need to Shrink.” Statistica Neerlandica. https://doi.org/https://doi.org/10.1111/stan.12241.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Replication crisis</span>"
    ]
  },
  {
    "objectID": "mixed.html",
    "href": "mixed.html",
    "title": "12  Introduction to mixed models",
    "section": "",
    "text": "12.1 Fixed vs random effects\nThis chapter considers tools for models with repeated measures from a modern perspective, using random effects for modelling. This class of model, called hierarchical models, multilevel models or mixed models in simple scenarios, give us more flexibility to account for complex scenarios in which there may be different sources of variability.\nFor example, consider a large-scale replication study about teaching methods. We may have multiple labs partaking in a research program and each has unique characteristics. Because of these, we can expect that measurements collected within a lab will be correlated. At the same time, we can have repeated mesures for participants in the study. One can view this setup as a hierarchy, with within-subject factor within subject within lab. In such settings, the old-school approach to analysis of variance becomes difficult, if not impossible; it doesn’t easily account for the heterogeneity in the lab sample size and does not let us estimate the variability within labs.\nWe begin our journey with the same setup as for repeated measures ANOVA by considering one-way within-subject ANOVA model. We assign each participant (subject) in the study to all of the experimental treatments, in random order. If we have one experimental factor \\(A\\) with \\(n_a\\) levels, the model is \\[\\begin{align*}\\underset{\\text{response}\\vphantom{l}}{Y_{ij}} = \\underset{\\text{global mean}}{\\mu_{\\vphantom{j}}} + \\underset{\\text{mean difference}}{\\alpha_j} + \\underset{\\text{random effect for subject}}{S_{i\\vphantom{j}}} + \\underset{\\text{error}\\vphantom{l}}{\\varepsilon_{ij}}.\n\\end{align*}\\] In a random effect model, we assume that the subject effect \\(S_i\\) is a random variable; we take \\(S_i \\sim \\mathsf{Normal}(0, \\sigma^2_s)\\) and the latter is assumed to be independent of the noise \\(\\varepsilon_{ij} \\sim \\mathsf{Normal}(0, \\sigma^2_e)\\). The model parameters that we need to estimate are the global mean \\(\\mu\\), the mean differences \\(\\alpha_1, \\ldots, \\alpha_{n_a}\\), the subject-specific variability \\(\\sigma^2_s\\) and the residual variability \\(\\sigma^2_e\\), with the sum-to-zero constraint \\(\\alpha_1 + \\cdots + \\alpha_{n_a}=0\\).\nInclusion of random effects introduces positive correlation between measurements: specifically, the correlation between two observations from the same subject will be \\(\\rho=\\sigma^2_s/(\\sigma^2_s+\\sigma^2_e)\\) and zero otherwise. This correlation structure is termed compound symmetry, since the correlation between measurements, \\(\\rho\\), is the same regardless of the order of the observations. If there are multiple random effects, the dependence structure will be more complicated.\nIn the repeated measure models, we need to first reduce measurements to a single average per within-subject factor, then fit the model by including the subject as a blocking factor. We are therefore considering subjects as fixed effects by including them as blocking factors, and estimate the mean effect for each subject: the value of \\(\\sigma^2_s\\) is estimated from the mean squared error of the subject term, but this empirical estimate can be negative. By contrast, the mixed model machinery will directly estimate the variance term, which will be constrained to be strictly positive.\nMixed models include, by definition, both random and fixed effects. Fixed effects are model parameters corresponding to overall average or difference in means for the experimental conditions. These are the terms for which we want to perform hypothesis tests and compute contrasts. So far, we have only considered models with fixed effects.\nRandom effects, on the other hand, assumes that the treatments are random samples from a population of interest. If we gathered another sample, we would be looking at a new set of treatments. Random effects model the variability arising from the sampling of that population and focuses on variance and correlation parameters. Addition of random effects does not impact the population mean, but induces variability and correlation within subject. There is no consensual definition, but Gelman (2005) lists a handful:\nIn terms of estimation, fixed effect terms are mean parameters, while all random effects will be obtained from variance and correlation parameters. In the repeated measure approach with fixed effects and blocking, we would estimate the average for each subject despite the fact that this quantity is of no interest. Estimating a mean with only a handful of measurements is a risky business and the estimated effects are sensitive to outliers.\nRandom effects would proceed to directly estimate the variability arising from different subjects. We can still get predictions for the subject-specific effect, but this prediction will be shrunk toward the global mean for that particular treatment category. As we gather more data about the subjects, the predictions will become closer to the fixed effect estimates when the number of observations per subject or group increases, but these prediction can deviate from mean estimates in the case where there are few measurements per subject.\nOehlert (2000) identifies the following step to perform a mixed model\nSources of variations are all factors (including identifiers) which could influence the response.\nWe say to factors are nested (\\(A\\) within \\(B\\)) when one can only coexist within the levels of the other: this has implications, for we cannot have interaction between the two. In between-subject experiments, subjects are nested in between-subject factors and the experimental factors are crossed, meaning we can assign an experimental unit or a subject to each factor combination. Interactions can occur for the between-subject factors, although we need participants in each subcategory to estimate them. In a between-subjects design, subjects are nested within experimental condition, as a subject can only be assigned a single treatment. In a within-subjects designs, experimental factors and subjects are crossed: it is possible to observed all combination of subjects and experimental conditions.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to mixed models</span>"
    ]
  },
  {
    "objectID": "mixed.html#fixed-vs-random-effects",
    "href": "mixed.html#fixed-vs-random-effects",
    "title": "12  Introduction to mixed models",
    "section": "",
    "text": "When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random [Green and Tukey (1960)].\n\n\nEffects are fixed if they are interesting in themselves or random if there is interest in the underlying population (e.g., Searle, Casella and McCulloch [(1992), Section 1.4])\n\n\n\n\n\nIdentify sources of variation\nIdentify whether factors are crossed or nested\nDetermine whether factors should be fixed or random\nFigure out which interactions can exist and whether they can be fitted.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to mixed models</span>"
    ]
  },
  {
    "objectID": "mixed.html#blocking-factors",
    "href": "mixed.html#blocking-factors",
    "title": "12  Introduction to mixed models",
    "section": "12.2 Blocking factors",
    "text": "12.2 Blocking factors\nIn many instances, some of the characteristics of observational units are not of interest: for example, EEG measurements of participants in a lab may differ due to time of the day, to the lab technician, etc. In the old days, it was customary to include these as fixed effect factors in the analysis, but disregard tests. We term explanatory factors (fixed effects) that are used only to control unwanted variability blocking factors: variables that impact the measurement variability, but that are not of direct interest. By filtering their effect out and looking at the residual variability that is unexplained by the blocking factors, block designs reduce the error term, at the cost of including and estimating additional parameters (group averages). Experimental units are typically assigned to blocking factor using stratified sampling to ensure comparisons can be made.\nWe will analyse block designs in the same as we did for multi-way analysis of variance model, with one notable exception. Typically, we will assume that there is no interaction between experimental factor and blocking factors, but we can always check for this assumption. Thus, we will be interested mostly in main effects of the experimental factors.\n\nExample 12.1 (The surprise of reaching out: paired data as blocking factors) We consider paired data from Study 3 of Liu et al. (2023), who looked at the appreciation of people reaching out to them in a unsolicited manner. The data includes the appreciation score of both responder and initiator, along with sociodemographic variables (age and gender).\nWhile a paired \\(t\\)-test is the natural (and arguably simplest way) to compare the difference in appreciation scores, we reformat the data to long format (one response per line), with a categorical variable role indicating the role of the participant and dyad, a dummy number indicating which participants belong to which pair. We then fit an analysis of variance model to the scores with both dyad and role. The \\(F\\)-tests for the main effects indicate that the dyads (66 additional parameters, since there are 67 pairs) filter out significant part of the variability. If we consider estimated marginal means and look at the \\(p\\)-value and the pairwise difference between initiator and respondent, we find exactly the same statistic value -4.6, with \\(66\\) degrees of freedom and \\(p\\)-value for the pairwise difference as the paired \\(t\\)-test.\n\ndata(LRMM23_S3, package = \"hecedsm\")\n# Paired t-test\nttest &lt;- with(LRMM23_S3, t.test(apprec_init, apprec_resp, paired = TRUE))\n\n\n# Cast data to long format - one response/participant per line\nLRMM23_S3l &lt;- LRMM23_S3 |&gt;\n  dplyr::mutate(dyad = factor(dplyr::row_number())) |&gt; \n  tidyr::pivot_longer(\n    cols = !dyad, \n    names_to = c(\".value\", \"role\"),\n    names_sep = \"\\\\_\")\n# Format of the data\nhead(LRMM23_S3l)\n# Treat dyad as a factor and fit two-way ANOVA model\nmod &lt;- lm(apprec ~ role + dyad, data = LRMM23_S3l)\n# Global tests of main effects (balanced data)\naov &lt;- anova(mod) # There seems to be significant variability filtered out by 'id'\n# Compute pairwise difference for main effect of 'role'\nemmeans::emmeans(mod, spec = \"role\") |&gt;\n  emmeans::contrast(\"pairwise\")\n\n\n\n\n\nTable 12.1: First six rows of the Study 3 of Liu et al. (2023) data in long format.\n\n\n\n\n\n\ndyad\nrole\napprec\nage\ngender\n\n\n\n\n1\nresp\n4\n20\nmale\n\n\n1\ninit\n4\n20\nmale\n\n\n2\nresp\n5\n20\nfemale\n\n\n2\ninit\n5\n24\nfemale\n\n\n3\nresp\n5\n21\nfemale\n\n\n3\ninit\n4\n21\nfemale\n\n\n\n\n\n\n\n\n\n\n\n\nTable 12.2: Analysis of variance table for appreciation as a function of role and dyad for Study 3 of Liu et al. (2023).\n\n\n\n\n\n\nterm\nsum of squares\ndf\nstat\np-value\n\n\n\n\nrole\n1\n7.17\n21.20\n&lt;0.001\n\n\ndyad\n66\n51.97\n2.33\n&lt;0.001\n\n\nResiduals\n66\n22.33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 12.3: Pairwise difference contrast for role for Study 3 of Liu et al. (2023).\n\n\n\n\n\n\nterm\ncontrast\nstd. error\ndf\nstat\np-value\n\n\n\n\ninit - resp\n-0.46\n0.1\n66\n-4.6\n&lt;0.001",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to mixed models</span>"
    ]
  },
  {
    "objectID": "mixed.html#random-effect-model",
    "href": "mixed.html#random-effect-model",
    "title": "12  Introduction to mixed models",
    "section": "12.3 Random effect model",
    "text": "12.3 Random effect model\n\nExample 12.2 (Temporal distancing in peace prospects) We consider a three-way mixed design from Study 5 of Halevy and Berson (2022). The research studied how temporal distance impacted the prospects of peace by presenting participants with a fictional scenario in which two tribes in a fictional country, Velvetia, where either at war or peace and asking what they think the outcome would be in the near and distance future. Participants rated the likeliness using a Likert scale ranging from extremely unlikely (1) to extremely likely (7) for the question\n\nThere is currently [war/peace] between the two tribes in Velvetia. Thinking about [next year/in 20 years], how likely is it that there will be [war/peace] in Velvetia?\n\nEach participant was assigned to a single combination of the current state curstate and the predicted outcome predout and answered the question for both temporal horizon, tempdist. A rapid examination shows that we have a complete design: there are participants assigned to each subcategory of the between-subject factors.\n\ndata(HB22_S5, package = \"hecedsm\")\nxtabs(~ curstate + predout, data = HB22_S5)\n\n        predout\ncurstate peace war\n   peace   148 164\n   war     118 124\n\n\n\nThe sources of variation are participants id, between-subject factors predout and curstate and within-subjects factor tempdist.\nBetween subject factors predout and curstate are crossed, whereas tempdist is nested within id.\nid is random, whereas predout, curstate and tempdist are fixed effects\nWe can have interactions between the three experimental factors predout, curstate and tempdist since the database contains instances of both of the former together, and since each person gets two tempdist levels. However, no interaction are possible between id and tempdist (due to lack of replication), or between between-subject factors and identifiers.\n\nWith data in long format, meaning each line contains a single response and characteristics are repeated per row, the software will automatically capture the random effect. We can fit the equivalent of the repeated measure mixed ANOVA using a linear mixed model in R, by simply specifying a random intercept for the participant id. The full model with the three-way interaction includes eight components for the mean and two variant component (residual variance, and subject-specific variance).\n\n# In R, two packages offer linear mixed model fit via (RE)ML\n# lme4 is more modern (and reliable?), but does not allow for unequal variance\nlme4::lmer(likelihood ~ curstate*predout*tempdist + (1 | id),\n           data = HB22_S5)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: likelihood ~ curstate * predout * tempdist + (1 | id)\n   Data: HB22_S5\nREML criterion at convergence: 1933.813\nRandom effects:\n Groups   Name        Std.Dev.\n id       (Intercept) 0.8524  \n Residual             1.1436  \nNumber of obs: 554, groups:  id, 277\nFixed Effects:\n                        (Intercept)                          curstatewar  \n                              5.581                               -2.886  \n                         predoutwar                         tempdist20yr  \n                             -2.337                               -1.068  \n             curstatewar:predoutwar             curstatewar:tempdist20yr  \n                              5.142                                3.508  \n            predoutwar:tempdist20yr  curstatewar:predoutwar:tempdist20yr  \n                              2.775                               -6.554  \n\n#  Note that in `lme4` package, the random effects are specified inside parenthesis\n# nlme offers the possibility to acount for unequal variance per group\n# Random effects are specified in `random` with formula notation, and a | to indicate\nnlme::lme(likelihood ~ curstate*predout*tempdist, \n          random = ~ 1 | id, # random intercept per individual\n          data = HB22_S5)\n\nLinear mixed-effects model fit by REML\n  Data: HB22_S5 \n  Log-restricted-likelihood: -966.9064\n  Fixed: likelihood ~ curstate * predout * tempdist \n                        (Intercept)                         curstatewar \n                           5.581081                           -2.886166 \n                         predoutwar                        tempdist20yr \n                          -2.337179                           -1.067568 \n             curstatewar:predoutwar            curstatewar:tempdist20yr \n                           5.142263                            3.508246 \n            predoutwar:tempdist20yr curstatewar:predoutwar:tempdist20yr \n                           2.774885                           -6.554272 \n\nRandom effects:\n Formula: ~1 | id\n        (Intercept) Residual\nStdDev:   0.8523522 1.143597\n\nNumber of Observations: 554\nNumber of Groups: 277 \n\n\n\n\nExample 12.3 (Happy fakes, remixed) We consider again the experiment of Amirabdolahian and Ali-Adeeb (2021) on smiling fakes and the emotion, this time from a pure mixed model perspective. This means we can simply keep all observations and model them accordingly. To fit the model, identifiers of subjects must be declared as factors (categorical variables). If we have repeated measurements of the within-subject factor, we may consider adding a random effect for the interaction between the subject and the within-subject factor.\n\n\n\n\n\n\n\n\nFigure 12.1: Jittered scatterplot of individual measurements per participant and stimulus type.\n\n\n\n\n\nFigure 12.1 shows the raw measurements, including what are notable outliers that may be due to data acquisition problems or instrumental manipulations. Since the experiment was performed in a non-controlled setting (pandemic) with different apparatus and everyone acting as their own technician, it is unsurprising that the signal-to-noise ratio is quite small. We will exclude here (rather arbitrarily) measurements below a latency of minus 40.\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\nmixedmod &lt;- lmerTest::lmer(\n  latency ~ stimulus + \n    (1 | id) + # random effect for subject\n    (1 | id:stimulus), # interaction id and stimuluss\n  # random effect for interaction \n  data = hecedsm::AA21 |&gt; #remove outliers\n    dplyr::filter(latency &gt; -40))\n# Output parameter estimates\nprint(mixedmod)\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: latency ~ stimulus + (1 | id) + (1 | id:stimulus)\n   Data: dplyr::filter(hecedsm::AA21, latency &gt; -40)\nREML criterion at convergence: 8007.913\nRandom effects:\n Groups      Name        Std.Dev.\n id:stimulus (Intercept) 0.7371  \n id          (Intercept) 2.2679  \n Residual                6.2235  \nNumber of obs: 1227, groups:  id:stimulus, 36; id, 12\nFixed Effects:\n(Intercept)    stimulus1    stimulus2  \n   -10.5374      -0.2529      -0.1394  \n\n\nThe model includes a fixed effect for stimulus, and variance terms for stimulus (nested within id) and subject identifier id. We see that there is quite a bit of heterogeneity between participants and per stimulus participant pair, albeit less so for the interaction. All estimated variance terms are rather large. We can also look globally at the statistical evidence for the difference between the various stimuli.\n\n# Global effect of different faces\n# ANOVA here is type III effects \n# computed from the 'lmerTest' package\nanova(mixedmod)\n\n\n\n\n\nTable 12.4: Type III analysis of variance table with Satterthwaite’s method for the linear mixed model with random effects for individuals and the interaction with stimulus.\n\n\n\n\n\n\nterm\nsum of squares\ndf 1\ndf 2\nstat\np-value\n\n\n\n\nstimulus\n65.62\n2\n23.34\n0.85\n0.44\n\n\n\n\n\n\n\n\nThe global \\(F\\) test of significance for stimulus is based on an approximation that accounts for the correlation between observations; the denominator degrees of freedom for the approximate \\(F\\) statistic are based on Satterthwaite’s method. There is again no evidence of differences between experimental conditions. This is rather unsurprising if we look at the raw data in Figure 12.1.\n\n\nExample 12.4 (Verbalization and memorization) We consider a replication study of Elliott et al. (2021), which studied verbalization and verbalization of kids aged 5, 6, 7 and 10. The replication was performed in 17 different school labs, adapting a protocol of Flavell, Beach, and Chinsky (1966), with an overall sample of 977 child partaking in the experiment.\nEach participant was assigned to three tasks (timing): delayed recall with 15 seconds wait, or immediate, and finally a naming task (point-and-name). The taskorder variable records the order in which these were presented: the order of delayed and immediate was counterbalanced across individuals, with the naming task always occurring last. The response variable is the number of words correctly recalled out of five. The experimenters also recorded the frequency at which students spontaneously verbalized during the task (except the point-and-name task, where they were specifically instructed to do so). The latter would normally be treated as a binary or count response variable, but we treat it as an explanatory hereafter.\nThe timing is a within-subject factor, whereas task order and age are between-subject factors: we are particularly interested in the speech frequency and the improvement over time (pairwise differences and trend).\nTo fit the linear mixed model with a random effect for both children id and lab: since children are nested in lab, we must specify the random effects via (1 | id:lab) + (1 | lab) if id are not unique; otherwise, the first term is equivalent to (1|id).\nWe modify the data to keep only 5 and 6 years old students, since most older kids verbalized during the task and we would have large disbalance (14 ten years old out of 235, and 19 out of 269 seven years old). We also exclude the point-and-name task, since verbalization was part of the instruction. This leaves us with 1419 observations and we can check that there are indeed enough children in each condition to get estimates.\n\ndata(MULTI21_D2, package = \"hecedsm\")\nMULTI21_D2_sub &lt;- MULTI21_D2 |&gt;\n  dplyr::filter(\n    age %in% c(\"5yo\", \"6yo\"),\n    timing != \"point-and-name\") |&gt;\n  dplyr::mutate(\n    verbalization = factor(frequency != \"never\",\n                           labels = c(\"no\", \"yes\")),\n    age = factor(age)) # drop unused age levels\nxtabs(~ age + verbalization, data = MULTI21_D2_sub)\n\n     verbalization\nage    no yes\n  5yo 106 334\n  6yo  56 450\n\n\nGiven that we have multiple students of every age group, we can include two-way and three-way interactions in the \\(2^3\\) design. We also include random effects for the student and the lab.\n\nhmod &lt;- lmerTest::lmer(\n  mcorrect ~ age*timing*verbalization + (1 | id:lab) + (1 | lab), \n  data = MULTI21_D2_sub)\n# Parameter estimates\n#summary(hmod)\n\nWe focus here on selected part of the output from summary() giving the estimated variance terms.\n\n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev.\n#&gt;  id:lab   (Intercept) 0.3587   0.599   \n#&gt;  lab      (Intercept) 0.0625   0.250   \n#&gt;  Residual             0.6823   0.826   \n#&gt; Number of obs: 946, groups:  id:lab, 473; lab, 17\n\nIn this setting, the correlation between observations from the same individuals \\(i\\) in lab \\(j\\) for measurement \\(k\\) and from different individuals in the same labs are, respectively \\[\\begin{align*}\n\\mathsf{cor}(Y_{ijk}, Y_{ijk'}) &= \\frac{\\sigma^2_{\\text{id}} + \\sigma^2_{\\text{lab}}}{\\sigma^2_{\\text{id}} + \\sigma^2_{\\text{lab}} + \\sigma^2_{\\text{res}}}, \\\\\\mathsf{cor}(Y_{ijk}, Y_{i'jk'}) &= \\frac{\\sigma^2_{\\text{lab}}}{\\sigma^2_{\\text{id}} + \\sigma^2_{\\text{lab}} + \\sigma^2_{\\text{res}}}.\n\\end{align*}\\] We can interpret the results as follows: the total variance is the sum of the id, lab and residual variances components give us an all but negligible effect of lab with 7 percent of the total variance, versus 40.5 percent for the children-specific variability. Since there are only 17 labs, and most of the individual specific variability is at the children level, the random effect for lab doesn’t add much to the correlation.\n\nanova(hmod, ddf = \"Kenward-Roger\")\n# Check estimated marginal means for age\nemm &lt;- emmeans::emmeans(hmod, specs = \"age\")\n# Pairwise differences\npairdiff &lt;- emm |&gt; \n    emmeans::contrast(method = \"pairwise\")\n\n\n\n\n\nTable 12.5: Type III analysis of variance table with Kenward-Roger’s method for Elliott et al. (2021).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nsum of squares\ndf 1\ndf 2\nstat\np-value\n\n\n\n\nage\n20.50\n1\n459.37\n30.05\n&lt;0.001\n\n\ntiming\n3.25\n1\n469.00\n4.76\n0.03\n\n\nverbalization\n13.61\n1\n459.45\n19.94\n&lt;0.001\n\n\nage:timing\n0.24\n1\n469.00\n0.36\n0.55\n\n\nage:verbalization\n0.08\n1\n462.08\n0.12\n0.72\n\n\ntiming:verbalization\n2.64\n1\n469.00\n3.88\n0.05\n\n\nage:timing:verbalization\n0.27\n1\n469.00\n0.40\n0.53\n\n\n\n\n\n\n\n\n\n\n\n\nTable 12.6: Estimated marginal means for age for Elliott et al. (2021).\n\n\n\n\n\n\nage\nemmeans\nstd. error\ndf\nstat\np-value\n\n\n\n\n5yo\n1.85\n0.09\n35.83\n20.29\n&lt;0.001\n\n\n6yo\n2.44\n0.11\n60.94\n23.21\n&lt;0.001\n\n\n\n\n\n\n\n\n\n\n\n\nTable 12.7: Pairwise difference for age 5 and 6 for the study of Elliott et al. (2021)\n\n\n\n\n\n\nterm\ncontrast\nstd. error\ndf\nstat\np-value\n\n\n\n\n5yo - 6yo\n-0.59\n0.11\n459.37\n-5.48\n&lt;0.001\n\n\n\n\n\n\n\n\nThe type III ANOVA table shows that there is no evidence of interaction between task order, age and verbalization (no three-interaction) and a very small difference for timing and verbalization. Thus, we could compute the estimated marginal means (95% confidence interval) for age with an estimated correct number of words of 1.85 (1.67, 2.04) words out of 5 for the 5 years olds and 2.44 (2.23, 2.66) words for six years old. Note that, despite the very large number children in the experiment, the degrees of freedom from the Kenward–Roger method are much fewer, respectively 35.83 and 60.94 for five and six years old.\nThe \\(t\\)-test for the pairwise difference of the marginal effect is 0.59 words with standard error 0.11. Judging from the output, the degrees of freedom calculation for the pairwise \\(t\\)-test are erroneous — they seem to be some average between the number of entries for the five years old (440) and six years old (506), but this fails to account for the fact that each kid is featured twice. Given the large magnitude of the ratio, this still amounts to strong result provided the standard error is correct.\nWe can easily see the limited interaction and strong main effects from the interaction plot in Figure 12.2. The confidence intervals are of different width because of the sample imbalance.\n\n\n\n\n\n\n\n\nFigure 12.2: Interaction plot for the recall task for younger children.\n\n\n\n\n\nWhile we have added random effects (so our parameter of interest are the variance terms \\(\\sigma^2_{\\mathrm{id}}\\) and \\(\\sigma^2_{\\mathrm{lab}}\\), we can nevertheless use the model to get predictions of the lab or individual-specific effects, which are conditional means. These are assumed to be normally distributed, and we can draw a quantile-quantile plot to check this assumption. With lme4, we can get the random effect predictions using the ranef function, as follows:\n\n# Predictions of lab-specific\n# Difference in means relative to average\nlibrary(lme4)\nranef(hmod)$lab\ndotplot(lme4::ranef(hmod))[[\"lab\"]] # catterpilar plot\n\n\n\n\nTable 12.8: Predicted mean differences for the lab-specific normal random effects\n\n\n\n\n\n\n\nlab\nprediction\n\n\n\n\nAustria\n\\(0.266\\)\n\n\nBristol\n\\(0.07\\)\n\n\nCardiff\n\\(0.131\\)\n\n\nCosta Rica\n\\(0.022\\)\n\n\nFrankfurt\n\\(-0.214\\)\n\n\nItaly\n\\(0.034\\)\n\n\nLouisiana\n\\(-0.161\\)\n\n\nMissouri\n\\(0.244\\)\n\n\nNebraska\n\\(0.091\\)\n\n\n\n\n\n\n\n\nlab\nprediction\n\n\n\n\nNew Zealand\n\\(-0.133\\)\n\n\nNorway\n\\(-0.137\\)\n\n\nOregon\n\\(0.039\\)\n\n\nSwitzerland\n\\(-0.352\\)\n\n\nTurkey\n\\(-0.397\\)\n\n\nVirginia\n\\(0.287\\)\n\n\nWisconsin\n\\(0.025\\)\n\n\nWitten\n\\(0.187\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.3: Catterpilar plot of predictions (conditional means and conditional variances) for the lab-specific random effects.\n\n\n\n\n\n\n\nExample 12.5 We consider data from a study at Tech3Lab (Labonté-LeMoyne et al. 2020) on standing desks. The description from the abstract reads\n\nThirty-seven young and healthy adults performed eight cognitive tasks in a 2 × 2 × 2 within-subject design of the following independent variables: posture (sitting/standing), task difficulty (easy/hard), and input device (computer mouse/tactile screen) in a counterbalanced order. The database LJLSFBM20 in package hecedsm contains the measurements. There is a total of 296 measurements, or \\(37 \\times 8\\), meaning each participant is assigned to a single task.\n\n\n\ntibble [296 × 14] (S3: tbl_df/tbl/data.frame)\n $ id            : Factor w/ 37 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 2 2 ...\n $ order         : int [1:296] 1 2 3 4 5 6 7 8 1 2 ...\n $ position      : Factor w/ 2 levels \"standing\",\"sitting\": 2 2 2 2 1 1 1 1 1 1 ...\n $ phys_demand   : Factor w/ 2 levels \"mouse\",\"touchpad\": 1 2 2 1 2 2 1 1 2 2 ...\n $ task_diff     : Factor w/ 2 levels \"easy\",\"difficult\": 1 2 1 2 2 1 2 1 2 1 ...\n $ ies           : num [1:296] 2486 24034 4691 11727 10874 ...\n $ central_alpha : num [1:296] 0.3 0.227 0.25 0.224 0.235 ...\n $ parietal_alpha: num [1:296] 0.388 0.348 0.345 0.333 0.298 ...\n $ central_beta  : num [1:296] 0.1103 0.0866 0.0902 0.0863 0.0972 ...\n $ parietal_beta : num [1:296] 0.169 0.159 0.141 0.147 0.141 ...\n $ bmi           : num [1:296] 22.8 22.8 22.8 22.8 22.8 ...\n $ sex           : Factor w/ 2 levels \"man\",\"woman\": 1 1 1 1 1 1 1 1 1 1 ...\n $ attention     : num [1:296] 6.83 6.67 6.83 6.83 6.83 ...\n $ satisfaction  : num [1:296] 70 85 80 76 86 100 60 80 70 85 ...\n\n\nThe id variable is a dummy for the participant, with associated covariate sex and bmi. We also have the three within-subject factors, position, phys_demand and task_diff and the order in which the tasks were given (counterbalanced). There are in total seven response variables: the inverse efficiency score global stimulus (ies), measures of brain activity (central_alpha, parietal_alpha, central_beta, parietal_beta) and two scales obtained from answers to a questionaire, attention and satisfaction.\nThe three manipulated factors are nested within subject and crossed, so we can estimate the three-way and two-way interactions with the experimental factors. The only logical random effect here is for subject, and we cannot have further sources of variability given the lack of replication.\nThe authors are not transparent as to what their model is and earn a failure grade for reproducibility: we have no idea of the specification, as coefficients are not reported. There is clearly eight coefficients corresponding to the average of the subgroups, plus a random effect for subject and sex and body mass index as covariates. It appears from the output of Table 1 that there was an interaction term added for BMI and position (as standing may be more strenuous on overweight participants). Finally, we include the order of the tasks as a covariate to account for potential fatigue effects.\nOur linear mixed model would take the form, here with ies as response\n\nmod &lt;- lmer(ies ~ position*phys_demand*task_diff +\n  (1 | id) + sex + bmi*position + order, \n           data = LJLSFBM20) \n\nGiven the number of response variables and coefficients, it is clear one must account for testing multiplicity. Figuring out the size of the family, \\(m\\), is not trivial: testing interactions and main effects leads to seven tests. We could also be interested in the interaction between body mass index and position if it affects performance, bringing the number of tests to eight and throw the covariate effect for order (up to 11 if we include all controls). Further contrasts may inflate this number (for example, there are 28 pairwise comparisons if the three-way interaction is significant). With seven responses and linear mixed models parametrized in the same way, we have at least 56 tests and potentially up to 350! Controlling the type-I error requires using Bonferroni–Holm type correction, since the different responses are not part of the same models.\n\nanova(mod)\n\n\n\n\n\nTable 12.9: Type III analysis of variance table with Satterthwaite’s method for Labonté-LeMoyne et al. (2020).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nsum of squares\ndf 1\ndf 2\nstat\np-value\n\n\n\n\nposition\n574982.8\n1\n250\n0.04\n0.850\n\n\nphys_demand\n1280635445.5\n1\n250\n80.35\n&lt;0.001\n\n\ntask_diff\n4927991208.4\n1\n250\n309.18\n&lt;0.001\n\n\nsex\n79185574.3\n1\n34\n4.97\n0.033\n\n\nbmi\n10365621.5\n1\n34\n0.65\n0.426\n\n\norder\n105580124.8\n1\n250\n6.62\n0.011\n\n\nposition:phys_demand\n9127600.8\n1\n250\n0.57\n0.450\n\n\nposition:task_diff\n1825943.4\n1\n250\n0.11\n0.735\n\n\nphys_demand:task_diff\n327914741.2\n1\n250\n20.57\n&lt;0.001\n\n\nposition:bmi\n1224777.6\n1\n250\n0.08\n0.782\n\n\nposition:phys_demand:task_diff\n2299195.8\n1\n250\n0.14\n0.704\n\n\n\n\n\n\n\n\nIf we focus on the sole results for ies, there is a significant two-way interaction (and main effects) for phys_demand and task_diff, but not for position. Further investigation would reveal better performance on the easy task and overall with a mouse.\n\nemmeans::emmeans(mod, spec = c(\"phys_demand\",\"task_diff\"))\n\n\n\n\n\nTable 12.10: Estimated marginal means for physical demand and task difficulty for the data of Labonté-LeMoyne et al. (2020).\n\n\n\n\n\n\nphys. demand\ntask diff.\nemmeans\nstd. error\ndf\nstat\np-value\n\n\n\n\nmouse\neasy\n2668.22\n671.60\n79.26\n3.97\n&lt;0.001\n\n\ntouchpad\neasy\n4723.12\n671.55\n79.24\n7.03\n&lt;0.001\n\n\nmouse\ndifficult\n8733.22\n671.62\n79.27\n13.00\n&lt;0.001\n\n\ntouchpad\ndifficult\n14998.42\n671.53\n79.23\n22.33\n&lt;0.001\n\n\n\n\n\n\n\n\nOne may wonder whether there are any effect of spillover, learning or fatigue due to the repeated measures: tasks were much longer than is typical. The coefficient for order is -268.14, so the decrease for the inverse efficiency score global stimulus for each additional task, ceteris paribus, with a \\(p\\)-value of 0.011 suggesting participants improve over time.\n\n\n\n\n\n\n\n\nFigure 12.4: Scatterplots of residuals from the linear mixed model against participant identifier (left) and against order of task (right).\n\n\n\n\n\nLooking at the residuals of the model per participants is also quite insightful. It is clear that measurements from participants 30 and 31 are abnormal, and these correspond to the values we see.\nWe can check the other model assumptions: the quantile-quantile plot of random effects suggests some large outliers, unsurprisingly due to the participants identified. The plot of fitted values vs residuals suggests our model is wholly inadequate: there is a clear trend in the residuals and strong evidence of heterogeneity.\n\nplot(ranef(mod))\nplot(mod)\n\n\n\n\n\n\n\n\n\nFigure 12.5: Normal quantile-quantile plot of predicted random effects (left) and Tukey’s plot of residual vs fitted values (right).\n\n\n\n\n\nWe could fit a more complex model to tackle the heterogeneity issue by including different variance per individual, at the expense of lower power.\n\n\n\n\n\nElliott, Emily M., Candice C. Morey, Angela M. AuBuchon, Nelson Cowan, Chris Jarrold, Eryn J. Adams, Meg Attwood, et al. 2021. “Multilab Direct Replication of Flavell, Beach, and Chinsky (1966): Spontaneous Verbal Rehearsal in a Memory Task as a Function of Age.” Advances in Methods and Practices in Psychological Science 4 (2): 1–20. https://doi.org/10.1177/25152459211018187.\n\n\nFlavell, John H., David R. Beach, and Jack M. Chinsky. 1966. “Spontaneous Verbal Rehearsal in a Memory Task as a Function of Age.” Child Development 37 (2): 283–99.\n\n\nGelman, Andrew. 2005. “Analysis of Variance — Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53. https://doi.org/10.1214/009053604000001048.\n\n\nHalevy, Nir, and Yair Berson. 2022. “Thinking about the Distant Future Promotes the Prospects of Peace: A Construal-Level Perspective on Intergroup Conflict Resolution.” Journal of Conflict Resolution 66 (6): 1119–43. https://doi.org/10.1177/00220027221079402.\n\n\nLabonté-LeMoyne, Elise, Marc-Antoine Jutras, Pierre-Majorique Léger, Sylvain Sénécal, Marc Fredette, Mickael Begon, and Marie-Ève Mathieu. 2020. “Does Reducing Sedentarity with Standing Desks Hinder Cognitive Performance?” Human Factors 62 (4): 603–12. https://doi.org/10.1177/0018720819879310.\n\n\nLiu, Peggy J., SoYon Rim, Lauren Min, and Kate E. Min. 2023. “The Surprise of Reaching Out: Appreciated More Than We Think.” Journal of Personality and Social Psychology 124 (4): 754–71. https://doi.org/10.1037/pspi0000402.\n\n\nOehlert, Gary. 2000. A First Course in Design and Analysis of Experiments. W. H. Freeman. http://users.stat.umn.edu/~gary/Book.html.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to mixed models</span>"
    ]
  },
  {
    "objectID": "causal-inference.html",
    "href": "causal-inference.html",
    "title": "13  Causal inference",
    "section": "",
    "text": "13.1 Basics of causal inference\nStatisticians are famous for repeating ad nauseam that “correlation is not causation”. This statement illustrated on the website Spurious correlations by Tyler Vigen, which shows multiple graphs of absurd relations, many of which are simply artifact of population growth. As second example, consider weather forecasts (of rain) and the number of people carrying umbrellas in the streets. These phenomena are positively correlated, but if I intervene and force everyone around to carry umbrellas, it will impact neither weather forecasts nor the weather itself. While correlation (effectively what is being measured in a linear regression model) contains information about presence of a relationship between two variables, it does not allow one to determine cause from effect (i.e., the direction). This can be determined through experiments, or via logical models.\nThe field of causal inference is concerned with inferring the effect of an action or manipulation (intervention, policy, or treatment) applied to a measurement unit and identifying and quantifying the effect of one variable on other variables. Such action may be conceptual: we can imagine for example looking at student’s success (as measured by their grades) by comparing two policies: giving them timely feedback and encouragement, versus no feedback. In reality, only one of these two scenarios can be realized even if both can conceptually be envisioned as potential outcomes.\nThe content of this chapter is divided as follows. First, we discuss the logical interrelation between variables using directed acyclic graphs and focus on relations between triples defining confounders, colliders and mediators. We then describe how we can retrieve causal effects (in an abstract setting). Finally, we present the linear mediation model popularized by Judd and Kenny (1981) and Baron and Kenny (1986). We focus on the hidden assumptions that can undermine causal claims in linear mediation models.\nIn the first chapter, we stated that randomized experiments were the gold standard of science. This is mostly because, through careful control of conditions (keeping everything else constant) and manipulating experimental factors, we can determine whether the latter cause changes in the response. However, not everything can be studied using experiments and we need more general rules and definitions to study causal inference when randomization isn’t possible.\nDenote by \\(Y_i(j)\\) the potential outcome of individual \\(i\\) assigned to treatment \\(X_j\\), i.e., the value of the response in an hypothetical world where we assigned this observation to a particular group. With a binary treatment \\(X \\in \\{0,1\\}\\), we would be interested by the difference in individual response, \\(Y(1)-Y(0)\\) or the individual response \\(Y(1)/Y(0)\\), depending on the context, and this readily extends to more than two groups. In an experiment, we can manipulate assignment to treatment \\(X\\) and randomize units to each value of the treatment to avoid undue effects from other variables. The potential outcomes framework applies to in between-subject designs because experimental units are assigned to a single treatment, whereas in within-subject designs a single ordering is presented. The fundamental problem of causal inference is that, while we would like to study the impact of every action on our response for each individual, only one of these potential outcomes is observed: at least half of the potential outcomes are unobserved.\nTable 13.1: Potential outcomes \\(Y(0)\\), \\(Y(1)\\) with a binary treatment \\(X\\) for six individuals. Question marks indicate missing data.\n\n\n\n\n\n\n\\(i\\)\n\\(X_i\\)\n\\(Y_i(0)\\)\n\\(Y_i(1)\\)\n\\(Y_i(1)-Y_i(0)\\)\n\n\n\n\n1\n1\n?\n4\n?\n\n\n2\n0\n3\n?\n?\n\n\n3\n1\n?\n6\n?\n\n\n4\n0\n1\n?\n?\n\n\n5\n0\n5\n?\n?\n\n\n6\n1\n?\n7\n?\nWe are effectively comparing the distribution of the outcome variable \\((Y \\mid X)\\) for different values of the experimental treatment set \\(X\\) (more generally, these are referred to as action sets). We talk about causation when for treatment (\\(X=j\\)) and control (\\(X=0\\)), the distributions of \\((Y \\mid X=j)\\) differs from that of \\((Y \\mid X=0)\\), as illustrated in Figure 13.2. We must thus resort to population-level estimates by considering averages and other summaries to draw inferences.\nFigure 13.2: Distribution of response on a seven point Likert scale for treatment and control groups in an hypothetical population.\nRather than look at the individual treatment effect difference, we can focus on averages over the population. The most common measure of causation is the average treatment effect, which is the difference between the population averages of treatment group \\(X=j\\) and the control group \\(X=0\\), \\[\\begin{align*}\n\\textsf{ATE}_j = \\underset{\\substack{\\text{expected response among}\\\\\\text{treatment group $j$}}}{\\mathsf{E}(Y \\mid X=j)} - \\underset{\\substack{\\text{expected response among}\\\\\\text{control group}}}{\\mathsf{E}(Y \\mid X=0)}\n\\end{align*}\\] which is a valid estimator of \\(\\mathsf{E}\\{Y(j) - Y(0)\\}\\) provided that \\(X\\) is randomly allocated.\nWhat assumptions are necessary to draw causal conclusion? In an experimental setting, we need the following:\nThe two last assumptions are sometimes pooled together under the umbrella term stable unit-treatment value assumption (SUTVA), implying that ‘’no other version or treatment assignment would lead to different potential outcome’’ (Imbens and Rubin 2015), which also suggests the effect of treatment is constant.\nIn summary, we can estimate the average treatment effect in experimental designs if we randomize the effect \\(X=j,\\) if our subjects comply with their treatment assignment, and more importantly if we use a random sample which is representative of the population.\nIn many fields, the unconditional effect is not interesting enough to warrant publication free of other explanatory variables. It may be that the effect of the treatment is not the same for everyone and depends on some covariate \\(Z:\\) for example, a study on gender discrimination may reveal different perceptions depending on gender \\(Z,\\) in which case the average treatment effect might not be a sensible measure and we could look at conditional effects. We may also be interested in seeing how different mechanisms and pathways are impacted by treatment and how this affects the response.  VanderWeele (2015) provides an excellent non-technical overview of mediation and interaction.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#basics-of-causal-inference",
    "href": "causal-inference.html#basics-of-causal-inference",
    "title": "13  Causal inference",
    "section": "",
    "text": "conditional ignorability, which states that potential outcomes are independent (denoted with the \\({\\perp\\mkern-10mu\\perp}\\) symbol) of assignment to treatment given a set of explanatories \\(\\boldsymbol{Z}.\\) In notation \\(\\{Y(0), \\ldots, Y(J)\\}\\ {\\perp\\mkern-10mu\\perp}\\ X \\mid \\boldsymbol{Z}\\) (if the latter is an empty set, we talk about ignorability).\nlack of interference: the outcome is unaffected by the treatment assignment of other participants.\nconsistency: given a treatment \\(X\\) taking level \\(j\\), the observed value for the response \\(Y \\mid X=j\\) is equal to the corresponding potential outcome \\(Y(j)\\).\n\n\n\n\n\n\n\n\nConditional ignorability for observational data\n\n\n\nCausal inference is possible in observational settings, but some of the assumptions are unverifiable and typically much stronger than for randomized experiments. Notably, we need to identify a sufficient set of confounders \\(\\boldsymbol{Z}\\), i.e., variables which cause both \\(X\\) and the outcome \\(Y\\) but are not of interest, to incorporate in the model to recover the effect due to \\(X\\). We can then look effect averaging over the distribution of \\(\\boldsymbol{Z}\\). This method is valid as long as there is a positive probability of being assigned to each group of \\(X\\) regardless of the value of the covariates \\(\\boldsymbol{Z}\\), an assumption sometimes known as positivity. The model must also be correctly specified, and there must be no omitted confounders.\n\n\n\n\nExample 13.1 The Agir cohort offers a comprehensive experience for the first three trimesters of the bachelor in administration program. These students take the same exams as regular students, but their class average are higher in the mandatory statistics course. However, since participation in the program is reserved for students who postulated to be part of the cohort, and who must be team players who are autonomous and with a keen liking for inverse pedagogical teaching methods, no conclusion can be drawn. Indeed, it may be that the teaching method is well suited to them but not to general students, or their better grades may be due to the fact that the Agir students are more motivated or stronger in statistics to being with.\nIf we compared the outcome of regular bachelor cohort with Agir students, we may also have violation of “no interference”: if students from different programs share course material or study together, then this lack of compliance may induce differences in response relative to their group assignment.\n\n\n\n13.1.1 Directed acyclic graph\n\nCausal inference requires a logical conceptual model of the interrelation between the variables. We will look at directed acyclic graphs to explain concepts of confounding, collision and mediation and how they can influence our conclusions.\nTo illustrate the relationship between variables, we use diagrams consisting of directed acyclic graph (DAG). A graph is a set of nodes and vertices: each node represents a variable of interest and these are linked with directed edges indicating the nature of the relation (if \\(X\\) causes \\(Y\\), then \\(X \\to Y\\)). The acyclic simply forbids loops: if \\(X\\) causes \\(Y\\), then \\(Y\\) cannot cause \\(X\\) (typically, violations are due to time dependence, which means that we must consider a larger DAG where \\(Y\\) is indexed by time, for example when medical treatment can cause changes in condition, which themselves trigger different treatments.\nDirected acyclic graphs are used to represent the data generating process that causes interdependencies between variables, while abstracting from the statistical description of the model. This depiction of the conceptual model helps to formalize and identify the assumptions of the model. To identify a causal effect of a variable \\(X\\) on some response \\(Y\\), we need to isolate the effect from that of other potential causes. Figure 13.3 shows an example of DAG in a real study; the latter is a simplification or abstraction of a world view, but is rather complicated.\n\n\n\n\n\n\n\n\n\nFigure 13.3: Directed acyclic graph of McQuire et al. (2020) reproduction by Andrew Heiss, licensed under CC BY-NC 4.0.\n\n\n\n\n\nAt a theoretical level, the DAG will help identify which paths and relations to control through conditioning arguments to strip the relation to that of interest. Judea Pearl (e.g., Pearl, Glymour, and Jewell 2016) identifies three potential relations between triples of (sets of) variables:\n\nchains (\\(X \\to Z\\to Y\\)),\nforks (\\(Z \\leftarrow X \\rightarrow Y\\)) and\nreverse forks (\\(Z \\rightarrow X \\leftarrow Y\\)).\n\nThese are represented in Figure 13.4. In the graph, \\(X\\) represents an explanatory variable, typically the experimental factor, \\(Y\\) is the response and \\(Z\\) is another variable whose role depends on the logical flow between variables (collider, confounder or mediator).\n\n\n\n\n\n\n\n\nFigure 13.4: Type of causal relations by Andrew Heiss, licensed under CC BY-NC 4.0.\n\n\n\n\n\nIn an experimental design, confounding effects from the experimental treatment \\(X\\) to the response \\(Y\\) are controlled by randomization or sample selection: all incoming arrows inside \\(X\\) from other variables are removed. If we include additional variables in the model which happen to be colliders, then we won’t recover the causal effect of interest. Addition of mediators will let us filter the effect due to \\(Z\\) from the direct effect of \\(X\\).\nIt is essential to determine via logic or otherwise (experiments can help!) the direction of the relationship, lest we run into trouble. Many statistical models commonly used, including regression models, cannot provide an answer to a problem that is philosophical or conceptual in nature. Indeed, correlation is symmetric and insufficient to infer the direction of the arrows in the directed acyclic graph.\nThe conclusions we will draw from statistical models depend on the nature of the relation. For example, in an observational setting, we could eliminate the effect of a confounding variable by controlling in a regression model, matching or by stratifying for different values of the confounders in order to extract the causal estimate of \\(X\\) on \\(Y\\). Matching consists in creating paired samples (with similar characteristics for explanatories), and comparing the pairs. Stratification consists of analysing subpopulations separately.\n\n\n\n\n\n\nCaution about the kitchen sink approach\n\n\n\nControlling for a collider is counterproductive, as we would get erroneous conclusions: Kowal (2021) reportedly claimed married couples with more children were less happy. As Richard McElreath hinted online, controlling for marriage (through sample selection) is incorrect since unhappy couples tend to divorce, but families with many children are less likely to divorce.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#mediation",
    "href": "causal-inference.html#mediation",
    "title": "13  Causal inference",
    "section": "13.2 Mediation",
    "text": "13.2 Mediation\nWe need to add to our logical causal model represented by a directed acyclic graph data generating mechanisms that prescribes how variables how interrelated. Our ability to establish mediation will depend on the model and a set of assumptions, some of which won’t be verifiable.\nTo define in full generality the treatment and mediation effect, we consider the potential outcome framework. Following Pearl (2014), we use the notation \\(\\text{do}(X=x)\\) is used to denote experimental manipulation of \\(X\\), to distinguish it from correlation in observational data. The resulting potential outcome for individual \\(i\\) is \\(Y_i(x, m)\\), with explanatory or experimental covariate/factor \\(x\\) and mediator \\(m\\). Likewise, \\(M_i(x)\\) is the potential mediator when applying treatment level \\(x\\).1 Value \\(x_0\\) represents the baseline or control value for our experimental manipulation.\n\nDefinition 13.1 (Average total, conditional mediation and direct effects) For simplicity, we focus on binary treatment with \\(X_i \\in \\{0,1\\}\\), with \\(X_i=x_0=0\\) the control and \\(X=1\\) for treatment. Until now, we have considered experiments in which we randomly allocate participants to groups, so we recovered the overall effect of treatment abstracting from other variables. The following definitions are standard, e.g. Robins and Greenland (1992), Pearl (2014), Imai, Keele, and Tingley (2010).\nThe average directed effect measures the flow along \\(X \\rightarrow Y\\), disabling the pathway \\(X \\to M \\to Y\\) by fixing the mediator value: it is \\[\\begin{align*}\n\\mathsf{ADE}(x) &=\n\\mathsf{E}[Y_i \\mid \\text{do}\\{X=1, M=M_i(x)\\}] - \\mathsf{E}[Y_i \\mid \\text{do}\\{X=0, M=M_i(x)\\}] \\\\&= \\mathsf{E}[Y_i(1,M_i(x)) -Y_i(0, M_i(x))\\}\n\\end{align*}\\] This measures the expected change in response when the experimental factor changes from treatment to control, while the mediator is set to a fixed value \\(M_i(x)\\) uniformly over the population. Fixing the mediator, which may or not be feasible experimentally.\nThe average causal mediation effect (also called indirect effect), is the main quantity of interest in mediation analysis. It is obtained by looking at changes in the outcome for a fixed intervention due to changing the values of the mediator to those it would take under the treatment and control group, respectively \\(M_i(1)\\) and \\(M_i(0)\\). \\[\\begin{align*}\n\\mathsf{ACME}(x) &=\n\\mathsf{E}[Y_i \\mid \\text{do}\\{X=x, M=M_i(1)\\}] - \\mathsf{E}[Y_i \\mid \\text{do}\\{X=x, M=M_i(0)\\}] \\\\&= \\mathsf{E}[Y_i\\{x, M_i(1)\\} -Y_i\\{x, M_i(0)\\}]\n\\end{align*}\\]\nThe total effect measures the average overall impact of changes in outcome \\(Y\\) (both through \\(M\\) and directly) when experimentally manipulating \\(X\\), \\[\\begin{align*}\n\\mathsf{TE}_i &= \\mathsf{E}[ Y_i \\mid \\text{do}(X=1)] - \\mathsf{E}[ Y_i \\mid \\text{do}(X=0)]\n\\\\ &= \\mathsf{E}[Y_i\\{1, M(1)\\}] - \\mathsf{E}[Y_i\\{0, M(0)\\}],\n\\end{align*}\\] This is what we retrieve if we randomize treatment assignment and consider the average change in average response. The total effect is obtained as \\(\\mathsf{TE} = \\mathsf{ACME}(X) + \\mathsf{ADE}(1-X)\\) for \\(X=0,1.\\) If there is no interaction between the treatment and the mediator, then the values of the average causal mediation effect and the average direct effect are the same regardless of the treatment assignment \\(x\\).\n\n\nDefinition 13.2 (Sequential ignorability assumption for mediation) To draw valid conclusions and build suitable estimation procedures, we need an assumption which can be decomposed into two components. The first component is: given pre-treatment covariates \\(Z\\), treatment assignment is independent of potential outcomes for mediation and outcome, \\[\\begin{align*}\nY_i(x', m), M_i(x) {\\perp\\mkern-10mu\\perp} X_i \\mid \\boldsymbol{Z}_i = \\boldsymbol{z}.\n\\end{align*}\\] In order words, the values taken by the mediator and by the response exist independently of the treatment assignment and don’t change.2\nThe second component of the sequential ignorability assumption is as follows: given pre-treatment covariates and observed treatment, mediation is independent of potential outcomes, \\[\\begin{align*}\nY_i(x', m) \\perp\\mkern-10mu\\perp  M_i(x) \\mid X_i =x, \\boldsymbol{Z}_i = \\boldsymbol{z}\n\\end{align*}\\] The set of assumptions from Imai, Keele, and Tingley (2010) and Pearl (2014) are equivalent under randomization of treatment assignment, as we consider thereafter.\nWe assume also that any value of \\(X\\) given covariates \\(\\boldsymbol{Z}\\) is possible (which is trivially satisfied when randomizing with equal probability), and that likewise any value of the mediator \\(M\\) is possible for any combination of covariates \\(\\boldsymbol{Z}\\) and treatment assignement \\(X\\).\n\n\n\n\n\n\n\n\n\n\nMeasurement error\n\n\n\nWhen measuring effects in psychology and marketing, it will often be the case that the conceptual causal model includes variables that cannot be directly measured. Proxies, as in Figure 13.5, add an additional layer of complexity and potential sources of confounding.\n\n\n\n\n\n\n\n\n\n\nFigure 13.5: xkcd comic 2652 (Proxy Variable) by Randall Munroe. Alt text: Our work has produced great answers. Now someone just needs to figure out which questions they go with. Cartoon reprinted under the CC BY-NC 2.5 license.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#linear-mediation-model",
    "href": "causal-inference.html#linear-mediation-model",
    "title": "13  Causal inference",
    "section": "13.3 Linear mediation model",
    "text": "13.3 Linear mediation model\nOne of the most popular model in social sciences is the linear mediation model, popularized by Baron and Kenny (1986). Hayes’ PROCESS macros for SAS, SPSS and R have lead to the widespread adoption by researchers (Preacher and Hayes 2004). Bullock, Green, and Ha (2010) list limitations of the approach and provide examples in which the model does not have a meaningful causal interpretation.\nThe linear mediation model assumes that the effect of mediation and treatment is additive and that the response measurement is continuous. Consider an experimental factor \\(X\\) corresponding to treatments with control or reference \\(x_0\\) and postulated mediator variable \\(M\\), assumed continuous. Given uncorrelated unobserved noise variables \\(\\varepsilon_M\\) and \\(\\varepsilon_Y\\), we specify linear regression models, \\[\\begin{align*}\nM \\mid X=x &= c_M + \\alpha x + \\varepsilon_M,\\\\\nY \\mid X=x, M=m &=  c_Y + \\beta x + \\gamma m  + \\varepsilon_Y\n\\end{align*}\\] where we use the contrasts parametrization, so that the reference category for the intercept corresponds to control (group \\(x_0\\)) and \\(x\\) the other category of interest, with \\(\\alpha\\) capturing the difference between \\(x\\) and \\(x_0\\). The model for \\(Y \\mid X, M\\) should include additional covariates \\(\\mathbf{z}\\) to control for confounding between \\(M\\) and \\(Y\\) if the latter is suspected.\nThe average value for \\(M(1) = c_M + \\alpha\\) and that of \\(M(0)=c_M\\). If we substitute the first equation in the second equation, we get that \\[\\begin{align*}\n\\mathsf{ACME}(x) &= \\mathsf{E}[Y\\{x, M(1)\\} -Y\\{x, M(0)\\}] = \\alpha \\gamma\\\\\n\\mathsf{ADE}(x) &= \\mathsf{E}[Y\\{1, M(x)\\} -Y\\{0, M(x)\\}] = \\beta \\\\\n\\mathsf{TE} &= \\mathsf{E}[Y\\{1, M(1)\\} -Y\\{0, M(0)\\}] = \\beta + \\alpha \\gamma\n\\end{align*}\\]\nThe parameters can be interpreted as the direct (\\(\\beta\\)), conditional mediation effect (\\(\\alpha \\gamma\\)) and total (\\(\\beta + \\alpha \\gamma\\)) effects. These parameters can be estimated using structural equation models (SEM), or more typically by running a series of linear regression.\nThe “sequential ignorability” assumption in the linear mediation models boils down to “no unmeasured confounders” in the relations \\(X \\to Y\\), \\(X \\to M\\) and \\(M \\to Y\\): the first two are satisfied in experimental studies due to randomization if we can manipulate \\(X\\), as shown in Figure 13.8. This means \\(\\varepsilon_M\\) and \\(\\varepsilon_Y\\) must be independent of one another and, as a result, error terms should also be uncorrelated.\nIn the linear mediation model, we can estimate the conditional direct effect corresponding to the product of coefficients \\(\\alpha\\gamma\\). Absence of mediation implies the product is zero. Baron and Kenny (1986) recommended using Sobel’s test statistic, of the form \\[\\begin{align*}\nS  &= \\frac{\\widehat{\\alpha}\\widehat{\\gamma} - 0}{\\mathsf{se}(\\widehat{\\alpha}\\widehat{\\gamma})} \\\\&=  \\frac{\\widehat{\\alpha}\\widehat{\\gamma}}{\\sqrt{\\widehat{\\gamma}^2\\mathsf{Va}(\\widehat{\\alpha}) + \\widehat{\\alpha}^2\\mathsf{Va}(\\widehat{\\gamma}) + \\mathsf{Va}(\\widehat{\\gamma})\\mathsf{Va}(\\widehat{\\alpha})}}\n\\end{align*}\\] where \\(\\widehat{\\alpha}\\), \\(\\widehat{\\gamma}\\) and their variance \\(\\mathsf{Va}(\\widehat{\\alpha})\\) and \\(\\mathsf{Va}(\\widehat{\\gamma})\\) can be obtained from the estimated coefficients and standard errors.3 The Sobel’s statistic \\(S\\) is approximately standard normal in large samples, \\(S \\stackrel{\\cdot}{\\sim}\\mathsf{Normal}(0,1)\\), but the approximation is sometimes poor in small samples. Other test statistics are listed in MacKinnon et al. (2002).\n\nExample 13.2 (Null distribution of Sobel statistic) To see this, let’s generate data with a binary treatment and normally distributed mediators and response with no confounding, so that the data generating process matches exactly the formulation of Baron–Kenny. We set \\(\\alpha=2\\), \\(\\beta = 1/2\\) and \\(\\gamma=0\\). This is an instance where the null hypothesis is true (\\(X\\) affects both \\(M\\) and \\(Y\\), but there is no mediation effect).\n\n\n\n\n\n\n\n\nFigure 13.6: Null distribution of Sobel’s statistic against approximate asymptotic normal distribution with 20 observations \\(\\alpha=0\\), \\(\\gamma=0.1\\) and normal random errors.\n\n\n\n\n\nIf we knew exactly the model that generated \\(X\\), \\(M\\), \\(Y\\) and the relations between them, we could simulate multiple datasets like in Figure 13.6 with \\(n=20\\) observations and compare the test statistic we obtained with the simulation-based null distribution with \\(\\alpha\\gamma=0\\). In practice we do not know the model that generated the data and furthermore we have a single dataset at hand.\n\nIn the linear mediation causal model, we can estimate the total causal effect of \\(X\\), labelled \\(\\tau\\), by running the linear regression of \\(Y\\) on \\(X\\) as there is no confounding affecting treatment \\(X\\) in a completely randomized experimental design. This strategy isn’t valid with observational data unless we adjust for confounders.\n\nRemark 13.1 (Historical note on Baron and Kenny). Baron and Kenny (1986) suggested for \\(M\\) and \\(Y\\) continuous breaking down the task in three separate steps:\n\nfit a linear regression of \\(M\\) on \\(X\\) to estimate \\(\\alpha\\)\nfit a linear regression of \\(Y\\) on \\(X\\) and \\(M\\) to estimate \\(\\beta\\) and \\(\\gamma\\).\nfit a linear regression of \\(Y\\) on \\(X\\) to estimate \\(\\tau\\)\n\nIn the “Baron and Kenny (1986) approach”, we test \\(\\mathscr{H}_0: \\alpha=0\\), \\(\\mathscr{H}_0: \\tau=0\\) and \\(\\mathscr{H}_0: \\gamma=0\\) against the two-sided alternative. This approach is somewhat flawed since mediation refers to the relation \\(X \\to M \\to Y\\), so we only need to consider (joint tests of) \\(\\alpha\\gamma\\) (the total effect could be zero because \\(\\beta = -\\alpha\\gamma\\) even if there is mediation). In practice, thus, we only need steps 1) and 2) and the models should include potential confounders of the pair \\((M, Y)\\).\n\n\nDefinition 13.3 (Typology of mediation) The following characterization was proposed by Zhao, Lynch, and Chen (2010):\n\ncomplementary mediation when both direct and indirect effects are of the same sign and non-zero.\ncompetitive mediation when direct and indirect effects are of opposite signs.\nindirect-only mediation when the direct effect of \\(X \\to Y\\) is null, but the effect \\(X \\to M \\to Y\\) isn’t.\n\nPrevious definitions popularized by Baron and Kenny (1986) still found in old papers include “full mediation” for instances where \\(\\beta=0\\) and partial mediation if the direct effect is less than the total effect, meaning \\(\\beta &lt; \\tau\\).\n\n\n13.3.1 Nonparametric bootstrap\nAn alternative for inference to large-sample approximations to the null distribution used as benchmark to establish whether a test statistic is ‘large’ is the bootstrap, a form of simulation-based inference (Efron 1979). The latter is conceptually easy to understand: we generate new datasets by resampling from the ones observed (as if it was the population). Since we want the sample size to be identical and our objective is to get heterogeneity, we sample with replacement: from one bootstrap dataset to the next, we will have multiple copies, or none, of each observation. See Efron and Tibshirani (1993) and Davison and Hinkley (1997) for a more thorough treatment of the bootstrap and alternative sampling schemes for regression models. The nonparametric bootstrap procedure advocated by Preacher and Hayes (2004) consists in repeating the following \\(B\\) times:\n\nsample \\(n\\) observations with replacement, i.e., a tuple (\\(X_i, M_i, Y_i)\\), from the original data .\ncompute the natural indirect effect \\(\\widehat{\\alpha}\\cdot\\widehat{\\gamma}\\) on each simulated sample\n\nThere are different approaches to computing confidence intervals, but the simplest is the percentile method. For a two-sided test at level \\(\\alpha\\), compute the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of the bootstrap statistics \\(\\{\\widehat{\\alpha}_b\\widehat{\\gamma}_b\\}_{b=1}^B\\). For example, if the level is \\(\\alpha=5\\)% and we generate \\(B=1000\\) bootstrap samples, the percentile confidence intervals bounds are the 25th and 975th ordered observations.\nNowadays, the asymptotic approximation (sometimes misnamed delta method4) has fallen out of fashion among practitioners for getting confidence intervals for the ACME (not Sobel’s statistic), who prefer the nonparametric bootstrap coupled with the percentile method. Figure 13.7 shows that the sampling distribution of \\(\\widehat{\\alpha}\\widehat{\\gamma}\\) (unstandardized) is indeed not symmetric, unlike the distribution of Sobel’s statistic.\n\n\n\n\n\n\n\n\nFigure 13.7: Bootstrap distribution of average conditional mediation effect (aka indirect effect) with estimate and percentile 95% confidence intervals (vertical red lines) for the simulated example.\n\n\n\n\n\nThe nonparametric percentile bootstrap confidence interval for \\(\\alpha\\gamma\\) is \\([-0.08, 1.71]\\) and thus we fail to reject the null hypothesis \\(\\mathscr{H}_0: \\alpha \\gamma=0\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#model-assumptions",
    "href": "causal-inference.html#model-assumptions",
    "title": "13  Causal inference",
    "section": "13.4 Model assumptions",
    "text": "13.4 Model assumptions\nWe can unpack the model assumptions for the linear mediation model.\n\nThe no unmeasured confounders assumption. Plainly stated, there are no unobserved confounders and thus the error terms \\(\\varepsilon_M\\) and \\(\\varepsilon_Y\\) are independent. Additionally, when we consider observational data, we must make sure there is hidden confounders affecting either the \\(M \\to X\\) and the \\(X \\to Y\\) relation, as shown in Figure 13.8. We can include covariates in the regression models to palliate to this, but we must only include the minimal set of confounders (and no additional mediator or collider chain).\n\n\n\n\n\n\n\n\n\nFigure 13.8: Directed acyclic graph representing observational settings (left) and experimental settings in which assignment is random given covariates measured pre-treatments (right). The ‘no unmeasured confounder’ assumption postulates such confounders are included (with the correct parametric form) in the regression models.\n\n\n\n\n\nAnother problem would be to claim that variable \\(M\\) is a mediator when in truth part of the effect on the outcome is due to change in another mediator. Figure 13.9 shows an instance with no confounding, but multiple mediators, say \\(M_1\\) and \\(M_2\\): the latter mediates the relation \\(M_1 \\to Y\\). The linear mediation model would capture the total effect of \\(M_1\\), but it would be incorrect to claim that the mediation effect on \\(X\\) is due to \\(M_1\\).\n\n\n\n\n\n\n\n\nFigure 13.9: Directed acyclic graph showing multiple mediators.\n\n\n\n\n\n\nThe linearity assumption implies that the linear models are correctly specified and that the effect of the covariates are linear. This means that, ceteris paribus, the effect of an increase of one unit of \\(M\\) on \\(Y\\) is the same regardless of the value of \\(M\\). It also doesn’t depend on the level of other variables (no interaction). If the model isn’t properly specified, the linear model coefficients will capture the best linear predictor given the design and the coefficients may not be meaningful. There must thus be no interaction between treatment and the mediator; see Definition 13.4 for the relaxation.\n\nThe linearity assumption also implies that the effect is the same for every individual, so there is no treatment heterogeneity or measurement error which could lead to attenuation bias.\nFollowing Bullock, Green, and Ha (2010), we index the regression equations by individual \\(i\\) \\[\\begin{align*}\nM_i\\mid X_i=x &= c_M + \\alpha_i x + \\varepsilon_{M,i},\\\\\nY_i \\mid X_i=x, M_i=m &=  c_Y + \\beta_i x + \\gamma_i m + \\varepsilon_{Y,i}\n\\end{align*}\\] If \\(\\alpha_i\\) differ from one observation to the next, the average estimated by the regression could be positive, negative or null. Even in the latter case, we could have \\(\\gamma_i\\) and \\(\\alpha_i\\) positive for some observation, or both negatives so that they cancel out even if there is complementary mediation.\nThe main benefit of experimental designs is that it remove confounding in the relationship between treatment and other variables. Adding spurious variables could create feedback (if some of those variables are colliders) and lead to inconsistent conclusions. It’s not clear that the mediator can be manipulated experimentally, and even if it could be to estimate the \\(\\gamma\\), one must make sure the relation is the same absent of \\(X\\). For example, we could estimate the indirect effect term by manipulating jointly (if possible) (\\(X, M\\)) but even then the linearity assumption must hold for the estimates to correspond to our linear causal mediation model.\n\n13.4.1 Sensitivity analysis\nThe no-unmeasured confounders assumption should be challenged. One way to assess the robustness of the conclusions to this is to consider correlation between errors, as in Bullock, Green, and Ha (2010). Indeed, one can show that the average parameter in the equation for the response of the linear mediation model satisfies \\[\\begin{align*}\n\\mathsf{E}(\\widehat{\\gamma})= \\gamma + \\frac{\\mathsf{Cov}(\\varepsilon_m, \\varepsilon_y)}{\\mathsf{Va}(\\varepsilon_m)},\n\\end{align*}\\] which introduces bias in the \\(\\mathsf{ACME}(x)\\). Even if the true covariance is unknown, we can vary \\(\\rho=\\mathsf{Cor}(\\varepsilon_m, \\varepsilon_y)\\) to assess the sensitivity of our conclusions to confounding. The medsens function in the R package mediation implements the diagnostic of Imai, Keele, and Tingley (2010) for the linear mediation model.\n\nDefinition 13.4 (Moderated mediation) We consider a more complex setting where the effect of the experimental factor \\(X\\) depends on the mediator, a case termed moderated mediator (Judd and Kenny 1981).\nIn this case, the equation for the response variable becomes \\[\\begin{align*}\n\\mathsf{E}(Y \\mid M=m, X=x, \\boldsymbol{Z} = \\boldsymbol{z}) = c_Y + \\beta x + \\gamma m + \\kappa x m + \\boldsymbol{z}\\boldsymbol{\\omega}\n\\end{align*}\\] Upon substituting the equations for both inside the definition of average causal mediation effect, we find that the latter equals \\[\\begin{align*}\n\\mathsf{ACME}(x) = (\\gamma + \\kappa x)\\{M(1)-M(0)\\} = \\alpha (\\gamma + \\kappa x).\n\\end{align*}\\] and thus the value differs depending on experimental regime (treatment or control), due to the presence of the interaction. Both the average direct and total effects now depend on the theoretical average values of the covariates \\(\\boldsymbol{Z}\\) added to the model to control for confounding.\n\n\nExample 13.3 (Pain as a social glue) We consider an example from Experiment 1 of Bastian, Jetten, and Ferris (2014), who experienced the effect of the effect of shared pain (through a manipulation) on bonding.\n\nThis effect of pain remained when controlling for age (\\(p = .048\\)), gender (\\(p = .052\\)), and group size (\\(p = .050\\)). None of these variables were significantly correlated with experimental condition (\\(ps &gt; .136\\)) or perceived bonding (\\(ps &gt; .925\\)). To determine whether the marginal tendency for the pain tasks to be viewed as more threatening than the control tasks mediated the effect of pain on perceived bonding, we conducted a bootstrap analysis (Preacher & Hayes, 2008) using 5,000 resamples. The results of this analysis revealed that threat was not a significant mediator, indirect effect = \\(-0.11\\), SE = \\(0.09\\), 95% CI = [\\(-0.34, 0.03\\)].\n\nThe response variable \\(Y\\) is bonding, the experimental factor condition and threat, the average ALES subscale about the perception of the physical task, is the postulated mediator.\nWe use the mediation package (Tingley et al. 2014) for the model; the package certainly isn’t needed (nor the PROCESS macros) to run the bootstrap, which we could obtain with a single for-loop. However, it has utilities, notably for checking model assumptions, that are convenient.\n\nlibrary(mediation, quietly = TRUE)\ndata(BJF14_S1, package = \"hecedsm\")\n# Mediation and response models\nMsX &lt;- lm(threat ~ condition + gender + groupsize + age, \n          data = BJF14_S1)\nYsXM &lt;- lm(bonding ~ condition + threat + gender + groupsize + age,\n           data = BJF14_S1)\n\n\n\n\nTable 13.2: Coefficients and standard errors of the linear regression model for Bastian, Jetten, and Ferris (2014)\n\n\n\n\n\n\n\n(a) mediation model\n\n\n\n\n\nterm\ncoef.\nstd. error\n\n\n\n\n(Intercept)\n-0.16\n0.59\n\n\ncondition [pain]\n0.30\n0.12\n\n\ngender [female]\n-0.05\n0.13\n\n\ngroup size\n-0.07\n0.05\n\n\nage\n0.07\n0.02\n\n\n\n\n\n\n\n\n\n\n\n(b) outcome model\n\n\n\n\n\nterm\ncoef.\nstd. error\n\n\n\n\n(Intercept)\n2.66\n1.47\n\n\ncondition [pain]\n0.66\n0.32\n\n\nthreat\n-0.24\n0.36\n\n\ngender [female]\n-0.02\n0.33\n\n\ngroup size\n0.02\n0.13\n\n\nage\n0.03\n0.07\n\n\n\n\n\n\n\n\n\n\n\nBoth of the threat and bonding measures are average of Likert scales. We include the controls in the regression for the response to account for potential confounding between threat level and shared bonding: it is unclear whether the authors used the control covariates or whether these make sense.\nIt is instructive to perform calculations by hand. We write a little function that takes as input the coefficients and variance of the parameters \\(\\alpha\\) and \\(\\gamma\\) from the linear mediation model and returns the Sobel statistic, and the \\(p\\)-value and 95% confidence intervals based on the (large-sample) normal approximation.\n\n# Function to calculate Sobel's statistic for binary treatment\nsobel &lt;- function(alpha, gamma, alpha_var, gamma_var){\n  # Compute point estimate (product of alpha and gamma)\n  pe_sob &lt;- as.numeric(alpha*gamma)\n  # Obtain standard error of alpha_hat*gamma_hat\n  se_sob &lt;- as.numeric(sqrt(gamma^2 * alpha_var + alpha^2 * gamma_var  + gamma_var*alpha_var))\n  # Wald statistic\n  stat &lt;- pe_sob/se_sob\n  data.frame(estimate = pe_sob,\n             se = se_sob,\n             stat = stat, # Sobel test statistic\n             pvalue = 2*pnorm(abs(stat), lower.tail = FALSE), # two-sided p-value\n             lowerCI = pe_sob + qnorm(p = 0.025)*se_sob,\n             upperCI = pe_sob + qnorm(p = 0.975)*se_sob\n  )\n}\n\n# Create a function that returns the statistic for any given data\n# This way, we can pass a bootstrap sample and get the same quantity\nstat &lt;- function(data, peOnly = TRUE){\n  # Fit models\nmod_med &lt;- lm(threat ~ condition + gender + groupsize + age,\n          data = data) # mediator model\nmod_resp &lt;- lm(bonding ~ condition + threat + gender + groupsize + age,\n           data = data) # response model\n# Use 'coef' to extract estimated coefficients\n# and 'vcov' to get the estimated covariance matrix of coefficients\ncoef_alpha &lt;- coef(mod_med)['conditionPain']\ncoef_gamma &lt;- coef(mod_resp)['threat']\nvar_alpha &lt;- vcov(mod_med)['conditionPain','conditionPain']\nvar_gamma &lt;- vcov(mod_resp)['threat','threat']\n  # Compute Sobel's statistic\n  S &lt;- sobel(alpha = coef_alpha,\n             alpha_var = var_alpha,\n             gamma = coef_gamma,\n             gamma_var = var_gamma)\n  if(!isTRUE(peOnly)){\n    return(S)\n  } else{\n    return(as.vector(S[,'estimate']))\n  }\n}\n# Values for the paper - using large-sample approximation\nvals &lt;- stat(data = BJF14_S1, peOnly = FALSE)\n\nNext, we turn to the nonparametric bootstrap to get a comparison of how the results would differ. We fix the seed and provide data sampled with replacement, using a simple for-loop. The number of bootstrap samples should be large enough to avoid additional variability due to resampling.\n\n# Nonparametric bootstrap analysis\nset.seed(80667) # set random seed for reproducibility\nB &lt;- 1000L #number of bootstrap replications\nn &lt;- nrow(BJF14_S1) # number of observations\nresults &lt;- numeric(B) # create container\nfor(b in seq_len(B)){\n  # Sample row indices with replacement at random uniformly\n  bootdata &lt;- BJF14_S1[sample.int(n, n, replace = TRUE),]\n  # Compute the test statistic on the bootstrap sample\n  results[b] &lt;- stat(data = bootdata, peOnly = TRUE)\n}\n# Percentile confidence intervals\nboot_perc_confint &lt;- quantile(results, prob = c(0.025, 0.975))\n# If we have B a multiple of 40, then we can use the sorted observations\n# sort(results)[B*c(0.025,0.975)]\n# Bootstrap p-value for test alpha*gamma=0\nM_over_B &lt;- mean(results &lt; 0)\nboot_pval &lt;- 2*min(c(M_over_B, 1-M_over_B))\n\n\n\n\n\nTable 13.3: Average causal mediation effect for Bastian, Jetten, and Ferris (2014), with \\(p\\)-values and 95% confidence intervals based on the normal approximation (top) and nonparametric bootstrap with percentile confidence intervals (bottom).\n\n\n\n\n\n\nestimate\nstd. error\np-value\nlower CL\nupper CL\n\n\n\n\n-0.071\n0.119\n0.549\n-0.305\n0.162\n\n\n-0.071\n0.126\n0.534\n-0.342\n0.132\n\n\n\n\n\n\n\n\nThe differences between the large-sample approximation and the bootstrap, reported in Table 13.3, are negligible.\nWe can also use software to perform the calculations without coding manually effects.\n\nset.seed(80667)\nlinmed &lt;- mediate(\n  model.m = MsX,\n  model.y = YsXM,\n  sims = 1000L, # number of bootstrap simulations\n  boot = TRUE, # use bootstrap\n  boot.ci.type = \"perc\", # type of bootstrap: percentile\n  mediator = \"threat\", # name of mediator\n  treat = \"condition\", # name of treatment\n  control.value = \"Control\", # name of control (level of 'condition')\n  treat.value = \"Pain\") # name of treatment (level of 'condition')\nsummary(linmed)\n\n\n\n\n\nTable 13.4: Linear causal mediation analysis: parameter estimates, nonparametric bootstrap 95% confidence intervals and p-values with the percentile method based on 5000 bootstrap samples.\n\n\n\n\n\n\n\nestimate\nlower 95% CI\nupper 95% CI\np-value\n\n\n\n\nACME\n-0.071\n-0.363\n0.146\n0.548\n\n\nADE\n0.660\n-0.012\n1.257\n0.056\n\n\ntotal effect\n0.588\n0.012\n1.145\n0.048\n\n\nprop. mediated\n-0.121\n-1.501\n0.615\n0.564\n\n\n\n\n\n\n\n\n\n\nThe first line of Table 13.6 gives the average conditional mediated effect (labelled ACME), the second the average direct effect (ADE) and the third the total effect (ADE + ACME). The point estimate for ACME, \\(\\alpha\\gamma\\) is \\(\\widehat{\\alpha}\\widehat{\\gamma} = -0.07\\). The bootstrap sampling distribution is skewed to the left, a fact reflected by the asymmetric percentile confidence interval.\n\n\n\n\n\n\n\n\nFigure 13.10: Density of the 1000 nonparametric bootstrap estimates of the average conditional mediation effect \\(\\alpha\\gamma\\), with point estimate and 95% percentile bootstrap confidence intervals (vertical dashed lines).\n\n\n\n\n\nThe sequential ignorability assumption cannot be verified, but we can see what impacts violations would have on the coefficients: the expected value of the coefficient \\(\\widehat{\\gamma}\\) is \\(\\gamma + \\mathsf{Cov}(\\varepsilon_M, \\varepsilon_Y)/\\mathsf{Va}(\\varepsilon_M)\\); the second component is a bias term that does not vanish, even when the sample size grows (Bullock, Green, and Ha 2010). The variance of the error of the mediation and response models can be estimated, and we can vary the correlation coefficient, \\(\\rho=\\mathsf{Cor}(\\varepsilon_M, \\varepsilon_Y)\\), to assess the sensitivity of our conclusions if there was confounding.\n\nlinmed_sensitivity &lt;- medsens(linmed)\nsummary(linmed_sensitivity)\n\n\nMediation Sensitivity Analysis for Average Causal Mediation Effect\n\nSensitivity Region\n\n      Rho    ACME 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n[1,] -0.4  0.2533      -0.0236       0.5302         0.16       0.1144\n[2,] -0.3  0.1626      -0.0723       0.3975         0.09       0.0644\n[3,] -0.2  0.0805      -0.1288       0.2898         0.04       0.0286\n[4,] -0.1  0.0034      -0.1969       0.2037         0.01       0.0072\n[5,]  0.0 -0.0714      -0.2788       0.1360         0.00       0.0000\n[6,]  0.1 -0.1462      -0.3748       0.0825         0.01       0.0072\n[7,]  0.2 -0.2233      -0.4850       0.0385         0.04       0.0286\n\nRho at which ACME = 0: -0.1\nR^2_M*R^2_Y* at which ACME = 0: 0.01\nR^2_M~R^2_Y~ at which ACME = 0: 0.0072 \n\nplot(linmed_sensitivity)\n\n\n\n\n\n\n\n\nThe medsens function implements the sensitivity diagnostic presented in Section 5.1 of Imai, Keele, and Yamamoto (2010) for the linear mediation model. By default, the correlation \\(\\rho\\) varies in 0.1 increments. We can see the wide range of the ACME if there was correlation between residuals from the mediation and the response model, highlighting the wide range of values that could be returned: the ACME could go from \\(0.2\\) to \\(-0.288\\) for correlations in the range \\(\\rho \\in [-0.4, 0.4]\\). In this example, nearly any correlation in this range would lead to “insignificant results”, mostly because of the small sample size. In a situation where we had found a significant (sic) result, we could observe how much correlation btween would be needed for this effect to be an artifact of correlation and vanish.\nAccording to the documentation of the medsens function (?medsens), there are two variants of the estimated effect size, either computing the proportion of the total (tilde, R^2_M~R^2_Y) or residual (starred, R^2_M*R^2_Y*) variance from the mediation and outcome models that are due to hypothetical unobserved confounders.\n\n\n\n\n\n\n\nPitfall\n\n\n\nThere are several problems with the description of Bastian, Jetten, and Ferris (2014): while it seems that some covariates (age, gender, group size) were added to regression models, it is unclear whether they could be confounders, whether their effect is linear and in which (if any) model they are included. Stating “bootstrap analysis” is the equivalent of “running a statistical test”, so vague it could mean anything, and the fact the output is random does not help with reproducibility.\n\n\n\nExample 13.4 (Fluency as mediator of product evaluation) Study 2 of Lee and Choi (2019) focus on inconsistency of photos and text descriptions for online descriptions and how this impact the product evaluation.\nThe experimental variable is the consistency of the product description and depiction, with fluency leading to “processing disfluency” that is expected to impact negatively judgment ratings. Familiarity with the product brand and product is included as covariate in both mediator and outcome model (see Table 1 of Lee and Choi (2019)).\n\ndata(LC19_S2, package = \"hecedsm\")\nLC19_S2 &lt;- LC19_S2 |&gt; # transform consistency into \n dplyr::mutate(consistency = ifelse(consistency == \"inconsistent\", 0, 1))\nYsMX &lt;- lm(prodeval ~ fluency + consistency + familiarity,\n           data = LC19_S2)\nMsX &lt;- lm(fluency ~ consistency + familiarity,\n           data = LC19_S2)\n\n\n\n\nTable 13.5: Model coefficients\n\n\n\n\n\n\n\n(a) mediation\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n5.23\n0.27\n\n\nconsistency\n0.48\n0.24\n\n\nfamiliarity\n0.08\n0.06\n\n\n\n\n\n\n\n\n\n\n\n(b) outcome\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n3.04\n0.53\n\n\nfluency\n0.60\n0.09\n\n\nconsistency\n0.29\n0.22\n\n\nfamiliarity\n0.09\n0.05\n\n\n\n\n\n\n\n\n\n\n\nWe can extract the effects directly from the outcome: the natural indirect effect estimate is \\(\\widehat{\\alpha}\\widehat{\\gamma} = 0.48 \\times 0.60\\) and the direct effect is \\(\\widehat{\\beta} = 0.29\\).\nTo get confidence intervals, we can use the mediate package (Tingley et al. 2014). The function requires the parametric model for the mediation and outcome, as well as a series of specification (the number of bootstrap samples, the type of confidence interval, the names of the levels for categorical treatment, etc.)\n\nset.seed(80667)\nlibrary(mediation, quietly = TRUE)\nlinmed &lt;- mediate(\n  model.m = MsX,\n  model.y = YsMX,\n  treat = \"consistency\",\n  mediator = \"fluency\",\n  sims = 5000L,\n  boot = TRUE,\n  boot.ci.type = \"perc\", # percentile method\n## with original factor, we could specify the levels for control and treatment\n#  control.value = \"inconsistent\", \n#  treat.value = \"consistent\"\n)\n\n\n\n\n\n\n\n\n\nFigure 13.11: Causal effects and confidence intervals for the difference between inconsistency (control) and consistency (treatement).\n\n\n\n\n\n\n\n\n\nTable 13.6: Linear causal mediation analysis: parameter estimates, nonparametric bootstrap 95% percentile confidence intervals and p-values based on 5000 nonparametric bootstrap samples.\n\n\n\n\n\n\n\nestimate\nlower 95% CI\nupper 95% CI\np-value\n\n\n\n\nACME\n0.286\n0.015\n0.595\n0.037\n\n\nADE\n0.287\n-0.165\n0.737\n0.218\n\n\ntotal effect\n0.573\n0.047\n1.091\n0.030\n\n\nprop. mediated\n0.499\n-0.051\n1.938\n0.057\n\n\n\n\n\n\n\n\n\n\nUsing the summary method, we can print the table of estimates and confidence intervals. We can see that the results are consistent with those reported in the article. We can also use the medsens function to perform the sensitivity analysis, but this only works for binary (numerical) treatments.\n\nplot(mediation::medsens(x = linmed))\n\n\n\n\n\n\n\nFigure 13.12: Sensitivity analysis showing the variation of the average causal mediation effect as a function of the correlation between outcome and mediation errors, with pointwise 95% confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#general-average-causal-mediation-effect-estimation",
    "href": "causal-inference.html#general-average-causal-mediation-effect-estimation",
    "title": "13  Causal inference",
    "section": "13.5 General average causal mediation effect estimation",
    "text": "13.5 General average causal mediation effect estimation\nThe linear mediation model is popular, but is limited to the case with no interaction, continuous mediators, constant (i.e., linear) treatment effect, and continuous response. It also hides model assumptions.\nWe could also rely on different models and use the definitions of the causal effects to perform simulation-based inference; see Appendix D of Imai, Keele, and Tingley (2010). The average causal mediation effect can be estimated empirically based on the simulated potential outcomes. Using Definition 13.1, the method of Imai, Keele, and Tingley (2010) relies on Monte Carlo simulations.\n\nModelling. Specify models to each relationship\n\na mediator model \\(f_M(x, \\boldsymbol{z}; \\boldsymbol{\\theta}_M)\\) that includes \\(X\\) and potential confounders \\(\\boldsymbol{Z}\\)\nan outcome model \\(f_Y(x, m, \\boldsymbol{z}; \\boldsymbol{\\theta}_Y)\\) with both treatment indicator \\(X\\), mediator \\(M\\)5 and potential confounders \\(\\boldsymbol{Z}\\).\n\nDraw \\(B\\) realizations of the mediation model for each observation \\(i=1, \\ldots, n\\), giving \\(M^{(b)}_i(1)\\) and \\(M^{(b)}_i(0)\\) for \\(b=1, \\ldots, B\\). Use these resulting to obtain one draw from the outcome model for each of\n\n\\(Y_{i}^{(b)}\\{0, M^{(b)}_i(0)\\}\\),\n\\(Y_{i}^{(b)}\\{0, M^{(b)}_i(1)\\}\\),\n\\(Y_{i}^{(b)}\\{1, M^{(b)}_i(0)\\}\\) and\n\\(Y_{i}^{(b)}\\{1, M^{(b)}_i(1)\\}\\).\n\nPlug these quantities in the definitions of \\(\\mathsf{ACME}(x)\\), \\(\\mathsf{ADE}(x)\\) and \\(\\mathsf{TE}\\), averaging over the \\(nM\\) replications, over both observations \\(i=1, \\ldots, n\\) and replications \\(b=1, \\ldots, B\\), e.g., \\[\\begin{align*}\n\\mathsf{TE} = \\frac{1}{nB} \\sum_{b=1}^B \\sum_{i=1}^n \\left[ Y_{i}^{(b)}\\{1, M^{(b)}_i(1)\\} - Y_{i}^{(b)}\\{0, M^{(b)}_i(0)\\}\\right]\n\\end{align*}\\]\nEstimation uncertainty: Either use nonparametric bootstrap or a large-sample approximation to the distribution of model parameters \\(\\boldsymbol{\\theta}_M\\) and \\(\\boldsymbol{Y}\\) to get \\(J\\) copies of parameters, and repeat steps 2. and 3. to get \\(J\\) copies of the causal estimates. Use the \\(J\\) replications to compute a 95% percentile interval and return the sample means over \\(J\\) for the average causal effects.\n\nThe benefit of this method is that it naturally account for nonlinear effects, different natures of the mediator (binary, categorical, etc.) and the response, through suitable models, e.g., a logistic regression for a binary outcome. With repeated measures, one could also consider interactions between individuals and mediator using random effects, and still retrieve an effect. The framework easily handles interactions between \\(X:M\\) added to the outcome model, in which case we get different values of average causal mediation effect and average direct effect for each value of \\(X\\).\n\n\n\n\nBaron, R. M., and D. A. Kenny. 1986. “The Moderator-Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.” Journal of Personality and Social Psychology 51 (6): 1173–82. https://doi.org/10.1037/0022-3514.51.6.1173.\n\n\nBastian, Brock, Jolanda Jetten, and Laura J. Ferris. 2014. “Pain as Social Glue: Shared Pain Increases Cooperation.” Psychological Science 25 (11): 2079–85. https://doi.org/10.1177/0956797614545886.\n\n\nBullock, J. G., Green D. P., and S. E. Ha. 2010. “Yes, but What’s the Mechanism? (Don’t Expect an Easy Answer).” 98 (4): 550–58. https://doi.org/10.1037/a0018933.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Application. New York, NY: Cambridge University Press.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. https://doi.org/10.1214/aos/1176344552.\n\n\nEfron, B., and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Boca Raton, FL: CRC Press.\n\n\nImai, Kosuke, Luke Keele, and Dustin Tingley. 2010. “A General Approach to Causal Mediation Analysis.” Psychological Methods 15 (4): 309–34. https://doi.org/10.1037/a0020761.\n\n\nImai, Kosuke, Luke Keele, and Teppei Yamamoto. 2010. “Identification, Inference and Sensitivity Analysis for Causal Mediation Effects.” Statistical Science 25 (1): 51–71. https://doi.org/10.1214/10-STS321.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge, UK: Cambridge University Press. https://doi.org/10.1017/CBO9781139025751.\n\n\nJudd, Charles M., and David A. Kenny. 1981. “Process Analysis: Estimating Mediation in Treatment Evaluations.” Evaluation Review 5 (5): 602–19. https://doi.org/10.1177/0193841X8100500502.\n\n\nKowal, Agata AND Kochan-Wójcik, Marta AND Groyecka-Bernard. 2021. “When and How Does the Number of Children Affect Marital Satisfaction? An International Survey.” PLOS ONE 16 (4): 1–14. https://doi.org/10.1371/journal.pone.0249516.\n\n\nLee, Kiljae, and Jungsil Choi. 2019. “Image-Text Inconsistency Effect on Product Evaluation in Online Retailing.” Journal of Retailing and Consumer Services 49: 279–88. https://doi.org/10.1016/j.jretconser.2019.03.015.\n\n\nMacKinnon, David P., Chondra M. Lockwood, Jeanne M. Hoffman, Stephen G. West, and Virgil Sheets. 2002. “A Comparison of Methods to Test Mediation and Other Intervening Variable Effects.” Psychological Methods 7 (1): 83–104. https://doi.org/10.1037/1082-989X.7.1.83.\n\n\nMcQuire, Cheryl, R. Daniel, L. Hurt, A. Kemp, and S. Paranjothy. 2020. “The Causal Web of Foetal Alcohol Spectrum Disorders: A Review and Causal Diagram.” European Child & Adolescent Psychiatry 29 (5): 575–94. https://doi.org/10.1007/s00787-018-1264-3.\n\n\nPearl, Judea. 2014. “Interpretation and Identification of Causal Mediation.” Psychological Methods 19 (4): 459–81. https://doi.org/10.1037/a0036434.\n\n\nPearl, Judea, Maria Glymour, and Nicholas Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester, UK: Wiley.\n\n\nPreacher, Kristopher J., and Andrew F. Hayes. 2004. “SPSS and SAS Procedures for Estimating Indirect Effects in Simple Mediation Models.” Behavior Research Methods, Instruments & Computers 36: 717–31. https://doi.org/10.3758/BF03206553.\n\n\nRobins, James M., and Sander Greenland. 1992. “Identifiability and Exchangeability for Direct and Indirect Effects.” Epidemiology 3 (2). https://journals.lww.com/epidem/fulltext/1992/03000/identifiability_and_exchangeability_for_direct_and.13.aspx.\n\n\nTingley, Dustin, Teppei Yamamoto, Kentaro Hirose, Luke Keele, and Kosuke Imai. 2014. “mediation: R Package for Causal Mediation Analysis.” Journal of Statistical Software 59 (5): 1–38. https://doi.org/10.18637/jss.v059.i05.\n\n\nVanderWeele, Tyler. 2015. Explanation in Causal Inference: Methods for Mediation and Interaction. New York, NY: Oxford University Press.\n\n\nZhao, Xinshu, Jr. Lynch John G., and Qimei Chen. 2010. “Reconsidering Baron and Kenny: Myths and Truths about Mediation Analysis.” Journal of Consumer Research 37 (2): 197–206. https://doi.org/10.1086/651257.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#footnotes",
    "href": "causal-inference.html#footnotes",
    "title": "13  Causal inference",
    "section": "",
    "text": "The notation is important to distinguish between association \\(Y \\mid X\\) when observing \\(X\\) from experimental manipulations or interventions, \\(Y \\mid \\mathsf{do}(X)\\) and counterfactuals \\(Y(X)\\).↩︎\nThe dependence on \\(\\boldsymbol{Z}\\) is used for situations where we can perform randomization based on pre-treatment assignment (i.e., we specify a mechanisms that is not equal based, but the probability of assignment is known for each individual).↩︎\nSobel derived the asymptotic variance using a first-order Taylor series expansion assuming both \\(\\alpha\\) and \\(\\gamma\\) are non-zero (hence the tests!)↩︎\nThe latter is the name of the method used to derive the denominator of Sobel’s statistic.↩︎\nOptionally, add their interaction \\(X:M\\) in case of moderated mediation.↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal inference</span>"
    ]
  },
  {
    "objectID": "nonparametric.html",
    "href": "nonparametric.html",
    "title": "14  Nonparametric tests",
    "section": "",
    "text": "14.1 Wilcoxon signed rank test\nIn small samples or in the presence of very skewed outcome responses, often combined with extreme observations, the conclusions drawn from the large-sample approximations for \\(t\\)-tests or analysis of variance models need not hold. This chapter presents nonparametric tests.\nIf our responses are numeric (or at least ordinal, such as those measured by Likert scales), we could subtitute them by their ranks. Ranks give the relative ordering in a sample of size \\(n\\), where rank 1 denotes the smallest observation and rank \\(n\\) the largest. Ranks are not affected by outliers and are more robust (contrary to averages), but discard information. For example, ranking the set of four observations \\((8, 2, 1, 2)\\) gives ranks \\((4, 2.5, 1, 2.5)\\) if we assign the average rank to ties.\nWhen are nonparametric tests used? The answer is that they are robust (meaning their conclusions are less affected) by departure from distributional assumptions (e.g., data are normally distributed) and by outliers. In large samples, the central limit theorem kicks in and the behaviour of most group average is normal. However, in small samples, the quality of the \\(p\\)-value approximate depends more critically on whether the model assumptions hold or not.\nAll of what has been covered so far is part of parametric statistics: we assume summary statistics behave in a particular way and utilize the probabilistic model from which these originate to describe the range of likely outcomes under the null hypothesis. As ranks are simply numbers between \\(1\\) to \\(n\\) (if there are no ties), no matter how data are generated, we can typically assess the repartition of those integers under the null hypothesis. There is no free lunch: while rank-based tests require fewer modelling assumptions, they have lower power than their parametric counterparts if the assumption underlying these tests are validated.\nIn short: the more assumptions you are willing to assume, the more information you can squeeze out of your problem. However, the inference can be fragile so you have to decide on a trade-off between efficiency (keeping all numerical records) and robustness (e.g., keeping only the signs or the ranking of the data).\nThe following list nonparametric tests and their popular parametric equivalent.\nThese can be extended with repeated measurements to more than two groups:\nFor more than 15 observations, the normal, student or Fisher approximation obtained by running the tests from the linear model or ANOVA function yield more or less the same benchmarks for all useful purposes: see J. K. Lindeløv cheatsheet and examples for indications.\nThe most common use of the signed rank test is for paired samples for which the response is measured on a numeric or ordinal scale. Let \\(Y_{ij}\\) denote measurement \\(j\\) of person \\(i\\) and the matching observation \\(Y_{kj}\\). For each pair \\(i=1, \\ldots, n\\), we can compute the difference \\(D_i = Y_{ij}-Y_{ik}\\).1 If we assume there is no difference between the distributions of the values taken, then the distribution of the difference \\(D_j\\) is symmetric around zero under the null hypothesis.2 The statistic tests thus tests whether the median is zero.3\nOnce we have the new differences \\(D_1, \\ldots, D_n\\), we take absolute values and compute their ranks, \\(R_i = \\mathsf{rank}|D_i|\\). The test statistic is formed by computing the sum of the ranks \\(R_i\\) associated with positive differences \\(D_i&gt;0\\). How does this test statistic work as a summary of evidence? If there was no difference we expect roughly half of the centered observations or paired difference to be positive and half to be negative. The sum of positive ranks should be close to the average rank: for a two-sided test, large or small sums are evidence against the null hypothesis that there is no difference between groups.\nIn management sciences, Likert scales are frequently used as response. The drawback of this approach, unless the response is the average of multiple questions, is that there will be ties and potentially zero differences \\(D_j=0\\). There are subtleties associated with testing, since the signed rank assumes that all differences are either positive or negative. The coin package in R deals correctly with such instances, but it is important to specify the treatment of such values.4",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric tests</span>"
    ]
  },
  {
    "objectID": "nonparametric.html#wilcoxon-signed-rank-test",
    "href": "nonparametric.html#wilcoxon-signed-rank-test",
    "title": "14  Nonparametric tests",
    "section": "",
    "text": "Example 14.1 (Smartwatches and other distractions) We consider a within-subject design from Brodeur et al. (2021), who conducted an experiment at Tech3Lab to check distraction while driving from different devices including smartwatches using a virtual reality environment. The authors wanted to investigate whether smartwatches were more distracting than cellphones while driving. Using a simulator, they ran a within-subject design where each participant was assigned to a distraction (phone, using a speaker, texting while driving or smartwatch) while using a driving simulator. The response is the number of road safety violations conducted on the segment. Each task was assigned in a random order. The data can be found in the BRLS21_T3 dataset in package hecedsm.\nA quick inspection reveals that the data are balanced with four tasks and 31 individuals. We can view the within-subject design with a single replication as a complete block design (with id as block) and task as experimental manipulation. The data here are clearly far from normally distributed and there are notable outliers in the upper right tail. While conclusions probably wouldn’t be affected by using an analysis of variance to compare the average time per task, but it may be easier to convince reviewers that the findings are solid by ressorting to nonparametric procedures.\nBoth the Friedman and the Quade tests are obtained by computing ranks within each block (participant) and then performing a two-way analysis of variance. The Friedman test is less powerful than Quade’s with a small number of groups. Both are applicable for block designs with a single factor.\n\ndata(BRLS21_T3, package = \"hecedsm\")\nfriedman &lt;- coin::friedman_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\nquade &lt;- coin::quade_test(\n  nviolation ~ task | id,\n  data = BRLS21_T3)\neff_size &lt;- effectsize::kendalls_w(\n  x = \"nviolation\", \n  groups = \"task\", \n  blocks = \"id\", \n  data = BRLS21_T3)\n\nThe Friedman test is obtained by replacing observations by the rank within each block (so rather than the number of violations per task, we compute the rank among the four tasks). Friedman’s test statistic is \\(18.97\\) and is compared to a benchmark \\(\\chi^2_3\\) distribution, yielding a \\(p\\)-value of \\(3\\times 10^{-4}.\\)\nWe can also obtain effect sizes for the rank test, termed Kendall’s \\(W.\\) A value of 1 indicates complete agreement in the ranking: here, this would occur if the ranking of the number of violations was the same for each participant. The estimated agreement (effect size) is \\(0.2.\\)\nThe test reveals significant differences in the number of road safety violations across tasks. We could therefore perform all pairwise differences using the signed-rank test and adjust \\(p\\)-values to correct for the fact we have performed six hypothesis tests.\nTo do this, we modify the data and map them to wide-format (each line corresponds to an individual). We can then feed the data to compute differences, here for phone vs watch. We could proceed likewise for the five other pairwise comparisons and then adjust \\(p\\)-values.\n\nsmartwatch &lt;- tidyr::pivot_wider(\n  data = BRLS21_T3,\n  names_from = task,\n  values_from = nviolation)\ncoin::wilcoxsign_test(phone ~ watch,\n                      data = smartwatch)\n\n\n    Asymptotic Wilcoxon-Pratt Signed-Rank Test\n\ndata:  y by x (pos, neg) \n     stratified by block\nZ = 0.35399, p-value = 0.7233\nalternative hypothesis: true mu is not equal to 0\n\n\nYou can think of the test as performing a paired \\(t\\)-test for the 31 signed ranks \\(R_i =\\mathsf{sign}(D_i) \\mathsf{rank}(|D_i|)\\) and testing whether the mean is zero. The \\(p\\)-value obtained by doing this after discarding zeros is \\(0.73\\), which is pretty much the same as the more complicated approximation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric tests</span>"
    ]
  },
  {
    "objectID": "nonparametric.html#wilcoxon-rank-sum-test-and-kruskalwallis-test",
    "href": "nonparametric.html#wilcoxon-rank-sum-test-and-kruskalwallis-test",
    "title": "14  Nonparametric tests",
    "section": "14.2 Wilcoxon rank sum test and Kruskal–Wallis test",
    "text": "14.2 Wilcoxon rank sum test and Kruskal–Wallis test\nThese testing procedures are the nonparametric analog of the one-way analysis in a between-subject design. One could be interested in computing the differences between experimental conditions (pairwise) or overall if there are \\(K \\geq 2\\) experimental conditions. To this effect, we simply pool all observations, rank them and compare the average rank in each group. We can track what should be the repartition of data if there was no difference between groups (all ranks should be somehow uniformly distributed among the \\(K\\) groups). If there are groups with larger averages than others, than this is evidence.\nIn the two-sample case, we may also be interested in providing an estimator of the difference between condition. To this effect, we can compute the average of pairwise differences between observations of each pair of groups: those are called Walsh’s averages. The Hodges–Lehmann estimate of location is simply the median of Walsh’s averages and we can use the Walsh’s averages themselves to obtain a confidence interval.\n\nExample 14.2 (Virtual communications) Brucks and Levav (2022) measure the attention of participants based on condition using an eyetracker. We compare the time spend looking at the partner by experimental condition (face-to-face or videoconferencing). The authors used a Kruskal–Wallis test, but this is equivalent to Wilcoxon’s rank-sum test.\n\ndata(BL22_E, package = \"hecedsm\")\nmww &lt;- coin::wilcox_test(\n  partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nwelch &lt;- t.test(partner_time ~ cond, \n  data = BL22_E, \n  conf.int = TRUE)\nmww\n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  partner_time by cond (f2f, video)\nZ = -6.4637, p-value = 1.022e-10\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n -50.694 -25.908\nsample estimates:\ndifference in location \n               -37.808 \n\n\nThe output of the test includes, in addition to the \\(p\\)-value for the null hypothesis that both median time are the same, a confidence interval for the time difference (in seconds). The Hodges–Lehmann estimate of location is \\(-37.81\\) seconds, with a 95% confidence interval for the difference of \\([-50.69, -25.91]\\) seconds.\nThese can be compared with the usual Welch’s two-sample \\(t\\)-test with unequal variance. The estimated mean difference is \\(-39.69\\) seconds for face-to-face vs group video, with a 95% confidence interval of \\([-52.93, -26.45]\\).\nIn either case, it’s clear that the videoconferencing translates into longer time spent gazing at the partner than in-person meetings.\n\n\n\n\n\nBrodeur, Mathieu, Perrine Ruer, Pierre-Majorique Léger, and Sylvain Sénécal. 2021. “Smartwatches Are More Distracting Than Mobile Phones While Driving: Results from an Experimental Study.” Accident Analysis & Prevention 149: 105846. https://doi.org/10.1016/j.aap.2020.105846.\n\n\nBrucks, Melanie S., and Jonathan Levav. 2022. “Virtual Communication Curbs Creative Idea Generation.” Nature 605 (7908): 108–12. https://doi.org/10.1038/s41586-022-04643-y.\n\n\nPratt, John W. 1959. “Remarks on Zeros and Ties in the Wilcoxon Signed Rank Procedures.” Journal of the American Statistical Association 54 (287): 655–67. https://doi.org/10.1080/01621459.1959.10501526.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric tests</span>"
    ]
  },
  {
    "objectID": "nonparametric.html#footnotes",
    "href": "nonparametric.html#footnotes",
    "title": "14  Nonparametric tests",
    "section": "",
    "text": "With one sample, we postulate a median \\(\\mu_0\\) and set \\(D_i = Y_{i} - \\mu_0\\).↩︎\nWe could subtract likewise \\(\\mu_0\\) from the paired difference if we assume the distributions are \\(\\mu_0\\) units apart.↩︎\nWhen using ranks, we cannot talk about the mean of the distribution, but rather about quantiles.↩︎\nFor example, are zero difference discarded prior to ranking, as suggested by Wilcoxon, or kept for the ranking and discarded after, as proposed by Pratt (1959)? We also need to deal with ties, as the distribution of numbers changes with ties. If this seems complicated to you, well it is… so much that the default implementation in R is unreliable. Charles Geyer illustrate the problems with the zero fudge, but the point is quite technical. His notes make a clear case that you can’t trust default software, even if it’s been sitting around for a long time.↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric tests</span>"
    ]
  },
  {
    "objectID": "count.html",
    "href": "count.html",
    "title": "15  Count data",
    "section": "",
    "text": "Many experiments can have binary outcomes. If we manipulate one or more experimental factors, we can aggregate these results by subcategories. This leads to contingency tables, which contain the total count per factor combination. These data can be understood in the same way as other ANOVA models, but with the aggregated responses as counts rather than averages. The following section is meant as both an introduction to the topic, showcasing examples of tests and their application.\nWith count data, we typically model observations as arising from a Poisson distribution, which takes values in \\(0, 1, \\ldots\\). The Poisson distribution has mean and variance \\(\\mu&gt;0\\) and, to ensure the fitted average are positive, regression models typically consider the natural logarithm.\nTo make things concrete, consider a two-way between-subject design with crossed factors having \\(I\\) and \\(J\\) categories, respectively. The mean equation would be\n\\[\\begin{align*}\n\\ln \\mu_{ij} = \\alpha + \\beta_i + \\gamma_j + (\\beta\\gamma)_{ij}, \\qquad i=1,\\ldots, I; j =1, \\ldots, J\n\\end{align*}\\] where \\(\\beta\\), \\(\\gamma\\), \\((\\beta\\gamma)\\)’s are subject to sum-to-zero categories. One major difference with regular ANOVA is that there will be at most \\(IJ\\) counts (one for each cell). If we fit a model with an interaction, we will overfit the data and predict back the entries. This is possible for the Poisson distribution because the variance is fully determined by the mean.\nThere are direct analog to the tests we normally consider: we can compare the full model with all interactions (termed saturated model) with simpler alternatives. The deviance statistic assesses whether the simpler model (null hypothesis) provides adequate fit, relative to the full model (alternative hypothesis).\nWe can compare different models in the same way than for multi-way between-subject designs. We fit two competing models (one simpler null model, say without interaction) and the alternative which includes the same specification, plus additional terms. We then compare the log-likelihood, a measure of fit, and form a statistic. As for ANOVA, the alternative models fit better (no matter what), but we can assess whether this improvement, due to including \\(\\nu\\) additional parameters, could be due to chance alone. The likelihood ratio statistic assess this change, and our large-sample benchmark. we compare it to a chi-square random variable with \\(\\nu\\) degrees of freedom, denoted \\(\\chi^2_\\nu\\).\nIn contingency tables, we can use Pearson’s \\(\\chi^2\\) test to compare observed counts to postulated expected counts, comparing the statistic to a chi-square distribution.\nThe statistic is of the form \\[P =\\sum_{i=1}^I \\sum_{j=1}^J \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\] where \\(E_{ij}\\) is the expected count under the null hypothesis, and \\(O_{ij}\\) are the observed entries. The degrees of freedom are \\(N\\) (here \\(IJ=N\\), the number of cells) minus the number of parameters under the null hypothesis. The large-sample approximation is adequate provided that the expected counts in each subgroup, \\(E_{ij}\\)’s, are larger than 5.\nFor contingency tables, the usual effect size is Cramér’s \\(V\\), which is a transformation of the chi-square statistic to remove the dependence on sample size and varies from \\(0\\) (no association) to \\(1\\) (fully determined data). The estimator is typically biased upward, so slightly different estimators (recipe) are available, including many with small-sample corrections.\n\nExample 15.1 (The importance of selling format) We revisit data from Duke and Amir (2023) on selling formats, which compare integrated purchases (e.g., proposing to choose first the item, then choosing the combination) or having a menu at checkout for the quantity (sequential). We look at whether the person purchased the item and aggregate counts in a \\(2 \\times 2\\) table. We are again testing for an interaction versus only main effects.\nIn the paper, all participants were included for this test, but other were excluded at a latter stage. The database DA23_E2 only includes the 325 participants who were present at all stages.\n\ndata(DA23_E2, package = \"hecedsm\")\ntabs &lt;- with(DA23_E2, table(purchased, format))\ntabs\n\n         format\npurchased quantity-integrated quantity-sequential\n        0                 100                 133\n        1                  66                  26\n\n# Chi-square test for interaction\nchisq.test(tabs)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tabs\nX-squared = 20.786, df = 1, p-value = 5.135e-06\n\n\nThe integrated quantity lead to a higher proportion of sales, and this difference is statistically significant at level 1%. The effect size (estimated with a small-sample correction of \\(V=0.25\\) indicates a moderate effect, even if the \\(p\\)-value is small since we have 39.8% who bought for quantity-integrated, versus 16.4% for sequential.\n\n\nExample 15.2 (Spontaneous verbal rehersal of memory tasks) We consider Elliott et al. (2021) multi-lab replication of Flavell, Beach, and Chinsky (1966) study on spontaneous verbalization of children when asked to identify pictures of objects. We pool data from all labs and study the counts as a function of age and frequency. We are interested in assessing whether there is an interaction between the two. Using Pearson \\(\\chi^2\\) test, we can fit the model in which counts in each cell \\(E_{ij}\\), which corresponds to the average predicted with a model with main effect only (overall average + row \\(i\\) and column \\(j\\) estimated deviation, from the main model).\n\ndata(MULTI21_D1, package = \"hecedsm\")\ncontingency &lt;- xtabs(\n  count ~ age + frequency, \n  data = MULTI21_D1)\n# Use chi-square directly - the correction is not applied\n# to get the same result as with the Poisson regression model\nchisqtest &lt;- chisq.test(contingency, correct = FALSE)\nchisqtest\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency\nX-squared = 87.467, df = 6, p-value &lt; 2.2e-16\n\n# Effect size (with adjustment for small sample)\neffectsize::cramers_v(chisqtest)$Cramers_v_adjusted\n\n[1] 0.2043891\n\n# Aggregate data into long format\nMULTI21_D1_long &lt;- MULTI21_D1 |&gt; \n  dplyr::group_by(age, frequency) |&gt; \n  dplyr::summarize(total = sum(count))\nmod_main &lt;- glm(total ~ age + frequency, \n    family = poisson,\n    data = MULTI21_D1_long)\nPearsonX2 &lt;- sum(residuals(mod_main, type = \"pearson\")^2)\n# p-value for Pearson chi-square test\npval_score &lt;- pchisq(PearsonX2, df = mod_main$df.residual, lower.tail = FALSE)\n# p-value for likelihood ratio test\npval_lrt &lt;- pchisq(deviance(mod_main), df = mod_main$df.residual, lower.tail = FALSE)\n\nThe two statistics, likelihood ratio test and Pearson’s chi-square tests, give similar answers (they are two ways of assessing the same hypothesis, and both have \\(\\chi^2\\) distributions under the null hypothesis of no interaction.\nGiven our large sample size, it is unsurprising to find differences. Perhaps more interesting is looking only at 5 years old versus 7 years old, as this is the age range where most changes occur. We also report effect sizes.\n\n# Take only a subset of the levels for age\ncounts_5vs7 &lt;- xtabs(\n  count ~ age + frequency, \n  data = MULTI21_D1,\n  subset = age %in% c(\"5yo\", \"7yo\"),\n  drop.unused.levels = TRUE)\ncounts_5vs7\n\n     frequency\nage   never sometimes usually\n  5yo    53        80      87\n  7yo    19        73     177\n\n# Compute chi-square test with 2x3 sub-table\nchisq.test(counts_5vs7)\n\n\n    Pearson's Chi-squared test\n\ndata:  counts_5vs7\nX-squared = 42.575, df = 2, p-value = 5.688e-10\n\n# Even stronger evidence of more verbalization\n\n# compute effect size - Cramer's V without small sample adjustment\neffectsize::cramers_v(chisq.test(counts_5vs7), adjust = FALSE)$Cramers_v\n\n[1] 0.2950689\n\n\n\n\nExample 15.3 (Racial discrimination in hiring) We consider a study from Bertrand and Mullainathan (2004), who study racial discrimination in hiring based on the consonance of applicants names; a similar example was recently featured in selection at the M.Sc. level from Ondes. The authors created curriculum vitae for four applicants and randomly allocated them a name, either one typical of a white person or a black person. The response is a count indicating how many of the applicants were called back (out of two black and two white) depending on their origin.\nIf there was no racial discrimination (null hypothesis), we would expect the average number of times a white applicant was called back (but no black applicant) to be the same as a single black applicant (but no white). Only the entries for different numbers of call-back (either 0 vs 2, 0 vs 1 or 1 vs 2 for either race) are instructive about our question of interest.\n\n\n\n\nTable 15.1: Table 2 of Bertrand and Mullainathan (2004), with counts of callbacks out of four CV (two white, two black) per combination.\n\n\n\n\n\n\n\nno white\n1 white\n2 white\n\n\n\n\nno black\n1103\n74\n19\n\n\n1 black\n33\n46\n18\n\n\n2 black\n6\n7\n17\n\n\n\n\n\n\n\n\nUnder the null hypothesis, the model would have equal off-diagonal entries in Table 15.1. We can test this null hypothesis of symmetry by creating a factor that has similar levels for off-diagonal entries.\n\ndata(BM04_T2, package = \"hecedsm\")\n# Fit the Poisson regression models\n# Saturated model - one average per cell, 9 parameters \nmod_alt &lt;- glm(count ~ white*black, \n             data = BM04_T2, \n             family = poisson) # default log-link\n# Symmetric model with 6 parameters (3 diag + 3 upper triangular)\nmod_null &lt;- glm(count ~ gnm::Symm(black, white), \n                data = BM04_T2, \n                family = poisson)\n# Compare the two nested models using a likelihood ratio test\nanova(mod_null, mod_alt,  test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: count ~ gnm::Symm(black, white)\nModel 2: count ~ white * black\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1         3     28.232                          \n2         0      0.000  3   28.232 3.246e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# The saturated model need not be fitted - use deviance and residuals directly\npval &lt;- pchisq(deviance(mod_null), df = mod_null$df.residual, lower.tail = FALSE)\n\nWe could use Pearson’s \\(\\chi^2\\) test too. In our example, the entries \\(E_{ij}\\) for off-diagonal average counts entries \\(E_{01}\\), \\(E_{12}\\), \\(E_{02}\\), etc., are obtained as the average of counts \\(E_{ij} = (O_{ij} + O_{ji})/2\\) \\((i\\neq j)\\), etc. The numeric value of the test statistic is 0, yielding a \\(p\\)-value less than \\(10^{-5}\\). There is thus strong evidence of racial discrimination.\n\n## Alternative test (Pearson)\n# Compute observed counts minus expected counts under model sum_i (O_i-E_i)^2/E_i\n# E_i is the expected count, estimated via average of non-diagonal entries 0.5*mu(0,2) + 0.5*mu(2,0)\nPearsonX2 &lt;- sum(residuals(mod_null, type = \"pearson\")^2)\npchisq(PearsonX2, df = 3, lower.tail = FALSE)\n\nWe can also compute contrasts. Because the model is multiplicative, it makes sense to report departures of symmetry by computing ratios \\(\\widehat{\\mu}_{ji}/\\widehat{\\mu}_{ij}\\) for \\(i, j = 0, 1, 2\\) and \\(i \\neq j\\). The grid specification is as usual. Wald tests (computed on the log scale) are also reported.\n\nlibrary(emmeans)\n# Compute contrasts as ratios counts01/counts10\nemmeans(mod_alt, specs = c(\"black\", \"white\")) |&gt;\n  # Compute custom contrasts - USE THIS ORDER FOR CONTRASTS\n  contrast(method = # Compute joint test of\n              list(\"0vs1\" = c(0,1,0,-1,0,0,0,0,0), \n                   #(1 white, 0 black) vs (0 white, 1 black)\n                   \"0vs2\" = c(0,0,1,0,0,0,-1,0,0),\n                   \"1vs2\" = c(0,0,0,0,0,1,0,-1,0)),\n           type = \"response\")\n\n contrast ratio     SE  df null z.ratio p.value\n 0vs1     0.446 0.0933 Inf    1  -3.858  0.0001\n 0vs2     0.316 0.1480 Inf    1  -2.461  0.0138\n 1vs2     0.389 0.1730 Inf    1  -2.120  0.0340\n\nTests are performed on the log scale \n\n\n\nIn more general regression models, we build a design matrix with explanatory variables \\(\\mathrm{X}_1, \\ldots, \\mathrm{X}_p\\). The mean model then reads \\[\\begin{align*}\n\\mu = \\exp(\\beta_0 + \\beta_1 \\mathrm{X}_{1} + \\cdots + \\beta_p \\mathrm{X}_{p}),\n\\end{align*}\\] so the mean is multiplied by \\(\\exp(\\beta_j)\\) for an increase of one unit of \\(\\mathrm{X}_{j},\\) ceteris paribus. If \\(\\beta_j &lt; 0\\), \\(\\exp(\\beta_j) &lt; 1\\) and so we have a decrease of \\(100\\cdot(1-\\exp(\\beta_j))\\)% of the mean response. Likewise, if \\(\\beta_j&gt;0\\), the mean number increases by \\(100\\cdot(\\exp(\\beta_j)-1)\\)%.\n\nExample 15.4 (Road accidents and speed limits on the motorway in Sweden) Sweden is a worldwide leader in road safety and has a long history of countermeasures to increase road traffic safety, including the Vision Zero program. Back in the 1960s, a study was conducted by the authorities to investigate the potential of speed limits on motorways to reduce the number of accidents. The sweden data contains the number of accidents on 92 matching days in both 1961 and 1962 (Svensson 1981); speed limits were in place on selected days in either year.\nTo study the impact of the restrictions we can fit a Poisson model. Let \\(Y_{i1}\\) (respectively \\(Y_{i2}\\)) denote the number of accidents in 1961 (1962) on day \\(i\\) and let \\(\\texttt{limit}_{ij}\\) denote a binary indicator equal to one if speed limits were enforced on day \\(i\\) of year \\(j\\). We set \\[\\begin{align*}\nY_{i1} &\\sim \\mathsf{Po}\\{\\exp(\\delta_i + \\alpha \\texttt{limit}_{i1})\\},\n\\\\ Y_{i2} &\\sim\\mathsf{Po}\\{\\exp(\\delta_i + \\gamma +  \\alpha \\texttt{limit}_{i2})\\}, \\qquad i=1, \\ldots, 92.\n\\end{align*}\\] The nuisance parameters \\(\\delta_1, \\ldots, \\delta_{92}\\) control for changes in background number of accidents and are of no practical interest, while \\(\\gamma\\) denotes the change from 1961 to 1962. We are interested here in assessing changes in the number of accidents due to the policy, \\(\\alpha\\); of secondary interest is \\(\\gamma\\), which determines whether there has been a change in the number of accident in 1962 relative to 1961.\n\ndata(sweden, package = \"hecedsm\")\nmodswed &lt;- glm(accidents ~ -1 + day + limit + year,\n               family = poisson(\"log\"), \n               data = sweden)\ntab &lt;- car::Anova(modswed, type = 3)\n\n\n\n\n\nTable 15.2: Analysis of deviance table (Type 3 decomposition) for the Poisson regression model fitted to the Sweden traffic restrictions data: the table gives the \\(p\\)-value for likelihood ratio tests comparing the full model including all covariates with models in which a single explanatory is removed.\n\n\n\n\n\n\nvariable\nstat\ndf\np-value\n\n\n\n\nday\n9395.22\n92\n\\(&lt;10^{-5}\\)\n\n\nlimit\n46.29\n1\n\\(&lt;10^{-5}\\)\n\n\nyear\n0.70\n1\n\\(0.401\\)\n\n\n\n\n\n\n\n\n\n\nThe residual deviance is 107.95 for 90 degrees of freedom, suggests the overall fit is good, despite the large number of nuisance parameters \\(\\delta_1, \\ldots, \\delta_{92}\\). The coefficient associated to limit is strongly significant: the estimated coefficient is \\(\\widehat{\\alpha}=-0.29\\), indicates that speed limits reduce the mean number of accidents by \\(25.3\\)% on average. In contrast, the likelihood ratio test reported in Table 15.2 shows that the change in the yearly number of accident from 1961 to 1962, \\(\\gamma\\), is not significantly different from zero.\n\n\n\n\n\nBertrand, Marianne, and Sendhil Mullainathan. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review 94 (4): 991–1013. https://doi.org/10.1257/0002828042002561.\n\n\nDuke, Kristen E., and On Amir. 2023. “The Importance of Selling Formats: When Integrating Purchase and Quantity Decisions Increases Sales.” Marketing Science 42 (1): 87–109. https://doi.org/10.1287/mksc.2022.1364.\n\n\nElliott, Emily M., Candice C. Morey, Angela M. AuBuchon, Nelson Cowan, Chris Jarrold, Eryn J. Adams, Meg Attwood, et al. 2021. “Multilab Direct Replication of Flavell, Beach, and Chinsky (1966): Spontaneous Verbal Rehearsal in a Memory Task as a Function of Age.” Advances in Methods and Practices in Psychological Science 4 (2): 1–20. https://doi.org/10.1177/25152459211018187.\n\n\nFlavell, John H., David R. Beach, and Jack M. Chinsky. 1966. “Spontaneous Verbal Rehearsal in a Memory Task as a Function of Age.” Child Development 37 (2): 283–99.\n\n\nSvensson, Ake. 1981. “On a Goodness-of-Fit Test for Multiplicative Poisson Models.” Ann. Statist. 9 (4): 697–704. https://doi.org/10.1214/aos/1176345512.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Count data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "16  References",
    "section": "",
    "text": "Achilles, C. M., Helen Pate Bain, Fred Bellott, Jayne Boyd-Zaharias,\nJeremy Finn, John Folger, John Johnston, and Elizabeth Word. 2008.\n“Tennessee’s Student Teacher Achievement Ratio (STAR)\nProject.” Harvard Dataverse. https://doi.org/10.7910/DVN/SIWH9F.\n\n\nAguinis, Herman, Isabel Villamor, and Ravi S. Ramani. 2021.\n“MTurk Research: Review and Recommendations.”\nJournal of Management 47 (4): 823–37. https://doi.org/10.1177/0149206320969787.\n\n\nAlexander, Rohan. 2022. Telling Stories with Data. CRC Press.\nhttps://www.tellingstorieswithdata.com/.\n\n\nAnandarajan, Asokan, Chantal Viger, and Anthony P. Curatola. 2002.\n“An Experimental Investigation of Alternative Going-Concern\nReporting Formats: A Canadian Experience.”\nCanadian Accounting Perspectives 1 (2): 141–62. https://doi.org/10.1506/5947-NQTC-C3Y5-H46N.\n\n\nBarnett, Adrian Gerard, and Jonathan D Wren. 2019. “Examination of\nCIs in Health and Medical Journals from 1976 to 2019: An\nObservational Study.” BMJ Open 9 (11). https://doi.org/10.1136/bmjopen-2019-032506.\n\n\nBaron, R. M., and D. A. Kenny. 1986. “The Moderator-Mediator\nVariable Distinction in Social Psychological Research: Conceptual,\nStrategic, and Statistical Considerations.” Journal of\nPersonality and Social Psychology 51 (6): 1173–82. https://doi.org/10.1037/0022-3514.51.6.1173.\n\n\nBastian, Brock, Jolanda Jetten, and Laura J. Ferris. 2014. “Pain\nas Social Glue: Shared Pain Increases Cooperation.”\nPsychological Science 25 (11): 2079–85. https://doi.org/10.1177/0956797614545886.\n\n\nBaumann, James F., Nancy Seifert-Kessell, and Leah A. Jones. 1992.\n“Effect of Think-Aloud Instruction on Elementary Students’\nComprehension Monitoring Abilities.” Journal of Reading\nBehavior 24 (2): 143–72. https://doi.org/10.1080/10862969209547770.\n\n\nBerger, Paul, Robert Maurer, and Giovana B Celli. 2018. Experimental\nDesign with Application in Management, Engineering, and the\nSciences. 2nd ed. Springer. https://doi.org/10.1007/978-3-319-64583-4.\n\n\nBertrand, Marianne, and Sendhil Mullainathan. 2004. “Are\nEmily and Greg More Employable Than\nLakisha and Jamal? A Field Experiment on Labor\nMarket Discrimination.” American Economic Review 94 (4):\n991–1013. https://doi.org/10.1257/0002828042002561.\n\n\nBrodeur, Mathieu, Perrine Ruer, Pierre-Majorique Léger, and Sylvain\nSénécal. 2021. “Smartwatches Are More Distracting Than Mobile\nPhones While Driving: Results from an Experimental Study.”\nAccident Analysis & Prevention 149: 105846. https://doi.org/10.1016/j.aap.2020.105846.\n\n\nBrook, Robert H., Emmett B. Keeler, Kathleen N. Lohr, Joseph P.\nNewhouse, John E. Ware, William H. Rogers, Allyson Ross Davies, et al.\n2006. The Health Insurance\nExperiment: A Classic RAND Study Speaks to the\nCurrent Health Care Reform Debate. Santa Monica, CA: RAND\nCorporation.\n\n\nBrucks, Melanie S., and Jonathan Levav. 2022. “Virtual\nCommunication Curbs Creative Idea Generation.” Nature\n605 (7908): 108–12. https://doi.org/10.1038/s41586-022-04643-y.\n\n\nBullock, J. G., Green D. P., and S. E. Ha. 2010. “Yes, but What’s\nthe Mechanism? (Don’t Expect an Easy Answer).” 98 (4): 550–58. https://doi.org/10.1037/a0018933.\n\n\nCard, David, and Alan B. Krueger. 1994. “Minimum Wages and\nEmployment: A Case Study of the Fast-Food Industry in New\nJersey and Pennsylvania.” The\nAmerican Economic Review 84 (4): 772–93. http://www.jstor.org/stable/2118030.\n\n\nChandler, J. J. 2016. “Replication of Janiszewski\n& Uy (2008, Psychological\nScience, Study 4b).” https://osf.io/aaudl.\n\n\nClaerbout, Jon F., and Martin Karrenbach. 1992. “Electronic\nDocuments Give Reproducible Research a New Meaning.” SEG\nTechnical Program Expanded Abstracts. https://doi.org/https://doi.org/10.1190/1.1822162.\n\n\nCohen, J. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. New York: Routledge. https://doi.org/10.4324/9780203771587.\n\n\nCox, David R. 1958. Planning of Experiments. New York, NY:\nWiley.\n\n\nCrump, M. J. C., D. J. Navarro, and J. Suzuki. 2019. Answering\nQuestions with Data: Introductory Statistics for Psychology\nStudents. https://doi.org/10.17605/OSF.IO/JZE52.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their\nApplication. New York, NY: Cambridge University Press.\n\n\nDuke, Kristen E., and On Amir. 2023. “The Importance of Selling\nFormats: When Integrating Purchase and Quantity Decisions Increases\nSales.” Marketing Science 42 (1): 87–109. https://doi.org/10.1287/mksc.2022.1364.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the\nJackknife.” The Annals of Statistics 7 (1): 1–26. https://doi.org/10.1214/aos/1176344552.\n\n\nEfron, B., and R. J. Tibshirani. 1993. An Introduction to the\nBootstrap. Boca Raton, FL: CRC Press.\n\n\nElliott, Emily M., Candice C. Morey, Angela M. AuBuchon, Nelson Cowan,\nChris Jarrold, Eryn J. Adams, Meg Attwood, et al. 2021. “Multilab\nDirect Replication of Flavell, Beach, and\nChinsky (1966): Spontaneous Verbal Rehearsal in a Memory\nTask as a Function of Age.” Advances in Methods and Practices\nin Psychological Science 4 (2): 1–20. https://doi.org/10.1177/25152459211018187.\n\n\nEsarey, Justin, and Jane Lawrence Sumner. 2018. “Marginal Effects\nin Interaction Models: Determining and Controlling the False Positive\nRate.” Comparative Political Studies 51 (9): 1144–76. https://doi.org/10.1177/0010414017730080.\n\n\nFaul, Franz, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner. 2007.\n“G*Power 3: A Flexible Statistical Power Analysis\nProgram for the Social, Behavioral, and Biomedical Sciences.”\nBehavior Research Methods 39 (2): 175–91. https://doi.org/10.3758/BF03193146.\n\n\nFisher, Ronald A. 1926. “The Arrangement of Field\nExperiments.” Journal of the Ministry of Agriculture 33:\n503–15. https://doi.org/10.23637/rothamsted.8v61q.\n\n\nFlavell, John H., David R. Beach, and Jack M. Chinsky. 1966.\n“Spontaneous Verbal Rehearsal in a Memory Task as a Function of\nAge.” Child Development 37 (2): 283–99.\n\n\nGarcia, Donna M., Michael T. Schmitt, Nyla R. Branscombe, and Naomi\nEllemers. 2010. “Women’s Reactions to Ingroup Members Who Protest\nDiscriminatory Treatment: The Importance of Beliefs about Inequality and\nResponse Appropriateness.” European Journal of Social\nPsychology 40 (5): 733–45. https://doi.org/10.1002/ejsp.644.\n\n\nGelman, Andrew. 2005. “Analysis of Variance — Why It Is More\nImportant Than Ever.” The Annals of Statistics 33 (1):\n1–53. https://doi.org/10.1214/009053604000001048.\n\n\nGelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in\nScience.” American Scientist 102: 460–65.\n\n\nHalevy, Nir, and Yair Berson. 2022. “Thinking about the Distant\nFuture Promotes the Prospects of Peace: A Construal-Level Perspective on\nIntergroup Conflict Resolution.” Journal of Conflict\nResolution 66 (6): 1119–43. https://doi.org/10.1177/00220027221079402.\n\n\nHariton, Eduardo, and Joseph J Locascio. 2018. “Randomised\nControlled Trials – the Gold Standard for Effectiveness\nResearch.” BJOG: An International Journal of Obstetrics &\nGynaecology 125 (13): 1716–16. https://doi.org/https://doi.org/10.1111/1471-0528.15199.\n\n\nHatano, A., C. Ogulmus, H. Shigemasu, and K. Murayama. 2022.\n“Thinking about Thinking: People Underestimate How Enjoyable and\nEngaging Just Waiting Is.” Journal of Experimental\nPsychology: General 151 (12): 3213–29. https://doi.org/10.1037/xge0001255.\n\n\nHedges, Larry V. 1981. “Distribution Theory for\nGlass’s Estimator of Effect Size and Related\nEstimators.” Journal of Educational Statistics 6 (2):\n107–28. https://doi.org/10.3102/10769986006002107.\n\n\nImai, Kosuke, Luke Keele, and Dustin Tingley. 2010. “A General\nApproach to Causal Mediation Analysis.” Psychological\nMethods 15 (4): 309–34. https://doi.org/10.1037/a0020761.\n\n\nImai, Kosuke, Luke Keele, and Teppei Yamamoto. 2010.\n“Identification, Inference and Sensitivity Analysis for Causal\nMediation Effects.” Statistical Science 25 (1): 51–71.\nhttps://doi.org/10.1214/10-STS321.\n\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for\nStatistics, Social, and Biomedical Sciences: An Introduction.\nCambridge, UK: Cambridge University Press. https://doi.org/10.1017/CBO9781139025751.\n\n\nIoannidis, John P. A. 2005. “Why Most Published Research Findings\nAre False.” PLOS Medicine 2 (8). https://doi.org/10.1371/journal.pmed.0020124.\n\n\nJaniszewski, Chris, and Dan Uy. 2008. “Precision of the Anchor\nInfluences the Amount of Adjustment.” Psychological\nScience 19 (2): 121–27. https://doi.org/10.1111/j.1467-9280.2008.02057.x.\n\n\nJohnson, David J., Felix Cheung, and M. Brent Donnellan. 2014.\n“Does Cleanliness Influence Moral Judgments?” Social\nPsychology 45 (3): 209–15. https://doi.org/10.1027/1864-9335/a000186.\n\n\nJohnson, P. O., and J. Neyman. 1936. “Tests of Certain Linear\nHypotheses and Their Application to Some Educational Problems.”\nStatistical Research Memoirs 1: 57–93.\n\n\nJones, Damon, David Molitor, and Julian Reif. 2019. “What Do\nWorkplace Wellness Programs Do? Evidence from the Illinois\nWorkplace Wellness Study.” The Quarterly Journal of\nEconomics 134 (4): 1747–91. https://doi.org/10.1093/qje/qjz023.\n\n\nJudd, Charles M., and David A. Kenny. 1981. “Process Analysis:\nEstimating Mediation in Treatment Evaluations.” Evaluation\nReview 5 (5): 602–19. https://doi.org/10.1177/0193841X8100500502.\n\n\nKeppel, G., and T. D. Wickens. 2004. Design and Analysis: A\nResearcher’s Handbook. Pearson Prentice Hall.\n\n\nKohavi, Ron, and Stefan Thomke. 2017. “The Surprising Power of\nOnline Experiments.” Harvard Business Review\nSeptember–October: 74–82. https://hbr.org/2017/09/the-surprising-power-of-online-experiments.\n\n\nKowal, Agata AND Kochan-Wójcik, Marta AND Groyecka-Bernard. 2021.\n“When and How Does the Number of Children Affect Marital\nSatisfaction? An International Survey.” PLOS ONE 16 (4):\n1–14. https://doi.org/10.1371/journal.pone.0249516.\n\n\nLabonté-LeMoyne, Elise, Marc-Antoine Jutras, Pierre-Majorique Léger,\nSylvain Sénécal, Marc Fredette, Mickael Begon, and Marie-Ève Mathieu.\n2020. “Does Reducing Sedentarity with Standing Desks Hinder\nCognitive Performance?” Human Factors 62 (4): 603–12. https://doi.org/10.1177/0018720819879310.\n\n\nLakens, Daniel. 2013. “Calculating and Reporting Effect Sizes to\nFacilitate Cumulative Science: A Practical Primer for t-Tests and ANOVAs.”\nFrontiers in Psychology 4: 863. https://doi.org/10.3389/fpsyg.2013.00863.\n\n\nLäuter, J. 1978. “Sample Size Requirements for the T2 Test of\nMANOVA (Tables for One-Way Classification).”\nBiometrical Journal 20 (4): 389–406. https://doi.org/10.1002/bimj.4710200410.\n\n\nLeckfor, Christina M., Natasha R. Wood, Richard B. Slatcher, and Andrew\nH. Hales. 2023. “From Close to Ghost: Examining the Relationship\nBetween the Need for Closure, Intentions to Ghost, and Reactions to\nBeing Ghosted.” Journal of Social and Personal\nRelationships 40 (8): 2422–44. https://doi.org/10.1177/02654075221149955.\n\n\nLee, Kiljae, and Jungsil Choi. 2019. “Image-Text Inconsistency\nEffect on Product Evaluation in Online Retailing.” Journal of\nRetailing and Consumer Services 49: 279–88. https://doi.org/10.1016/j.jretconser.2019.03.015.\n\n\nLin, Jason D, Nicole You Jeung Kim, Esther Uduehi, and Anat Keinan.\n2024. “Culture for Sale: Unpacking Consumer Perceptions of\nCultural Appropriation.” Journal of Consumer Research.\nhttps://doi.org/10.1093/jcr/ucad076.\n\n\nLiu, Peggy J., SoYon Rim, Lauren Min, and Kate E. Min. 2023. “The\nSurprise of Reaching Out: Appreciated More Than We Think.”\nJournal of Personality and Social Psychology 124 (4): 754–71.\nhttps://doi.org/10.1037/pspi0000402.\n\n\nMacKinnon, David P., Chondra M. Lockwood, Jeanne M. Hoffman, Stephen G.\nWest, and Virgil Sheets. 2002. “A Comparison of Methods to Test\nMediation and Other Intervening Variable Effects.”\nPsychological Methods 7 (1): 83–104. https://doi.org/10.1037/1082-989X.7.1.83.\n\n\nMaglio, Sam J., and Evan Polman. 2014. “Spatial Orientation\nShrinks and Expands Psychological Distance.” Psychological\nScience 25 (7): 1345–52. https://doi.org/10.1177/0956797614530571.\n\n\nMagnusson, Kristoffer. 2021. “Interpreting Cohen’s\nd Effect Size: An Interactive\nVisualization.” https://rpsychologist.com/cohend/.\n\n\nMardia, Kanti V., John T. Kent, and Charles C. Taylor. 2024.\nMultivariate Analysis. Edited by Wiley. 2nd ed. Hoboken, NJ.\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian Course with\nExamples in r and Stan. 2nd ed. CRC Press. https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919.\n\n\nMcQuire, Cheryl, R. Daniel, L. Hurt, A. Kemp, and S. Paranjothy. 2020.\n“The Causal Web of Foetal Alcohol Spectrum Disorders: A Review and\nCausal Diagram.” European Child & Adolescent\nPsychiatry 29 (5): 575–94. https://doi.org/10.1007/s00787-018-1264-3.\n\n\nNosek, Brian, Johanna Cohoon, Mallory Kidwell, and Jeffrey Spies. 2015.\n“Estimating the Reproducibility of Psychological Science.”\nScience 349 (6251). https://doi.org/10.1126/science.aac4716.\n\n\nOehlert, Gary. 2000. A First Course in Design and Analysis of\nExperiments. W. H. Freeman. http://users.stat.umn.edu/~gary/Book.html.\n\n\nPearl, Judea. 2014. “Interpretation and Identification of Causal\nMediation.” Psychological Methods 19 (4): 459–81. https://doi.org/10.1037/a0036434.\n\n\nPearl, Judea, Maria Glymour, and Nicholas Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester, UK: Wiley.\n\n\nPratt, John W. 1959. “Remarks on Zeros and Ties in the\nWilcoxon Signed Rank Procedures.” Journal of the\nAmerican Statistical Association 54 (287): 655–67. https://doi.org/10.1080/01621459.1959.10501526.\n\n\nPreacher, Kristopher J., and Andrew F. Hayes. 2004.\n“SPSS and SAS Procedures for Estimating\nIndirect Effects in Simple Mediation Models.” Behavior\nResearch Methods, Instruments & Computers 36:\n717–31. https://doi.org/10.3758/BF03206553.\n\n\nRobins, James M., and Sander Greenland. 1992. “Identifiability and\nExchangeability for Direct and Indirect Effects.”\nEpidemiology 3 (2). https://journals.lww.com/epidem/fulltext/1992/03000/identifiability_and_exchangeability_for_direct_and.13.aspx.\n\n\nRosen, B., and T. H. Jerdee. 1974. “Influence of Sex Role\nStereotypes on Personnel Decisions.” Journal of Applied\nPsychology 59: 9–14.\n\n\nSharma, Eesha, Stephanie Tully, and Cynthia Cryder. 2021.\n“Psychological Ownership of (Borrowed) Money.” Journal\nof Marketing Research 58 (3): 497–514. https://doi.org/10.1177/0022243721993816.\n\n\nSokolova, Tatiana, Aradhna Krishna, and Tim Döring. 2023. “Paper\nMeets Plastic: The Perceived Environmental Friendliness of Product\nPackaging.” Journal of Consumer Research 50 (3): 468–91.\nhttps://doi.org/10.1093/jcr/ucad008.\n\n\nSong, Zirui, and Katherine Baicker. 2019. “Effect of a Workplace\nWellness Program on Employee Health and Economic Outcomes: A Randomized\nClinical Trial.” JAMA 321 (15): 1491–501. https://doi.org/10.1001/jama.2019.3307.\n\n\nSteiger, James H. 2004. “Beyond the F Test: Effect Size Confidence\nIntervals and Tests of Close Fit in the Analysis of Variance and\nContrast Analysis.” Psychological Methods 9: 164–82. https://doi.org/10.1037/1082-989X.9.2.164.\n\n\nStekelenburg, Aart van, Gabi Schaap, Harm Veling, and Moniek Buijzen.\n2021. “Boosting Understanding and Identification of Scientific\nConsensus Can Help to Correct False Beliefs.” Psychological\nScience 32 (10): 1549–65. https://doi.org/10.1177/09567976211007788.\n\n\nSvensson, Ake. 1981. “On a Goodness-of-Fit Test for Multiplicative\nPoisson Models.” Ann. Statist. 9 (4):\n697–704. https://doi.org/10.1214/aos/1176345512.\n\n\nTingley, Dustin, Teppei Yamamoto, Kentaro Hirose, Luke Keele, and Kosuke\nImai. 2014. “mediation: R Package for\nCausal Mediation Analysis.” Journal of Statistical\nSoftware 59 (5): 1–38. https://doi.org/10.18637/jss.v059.i05.\n\n\nVanderWeele, Tyler. 2015. Explanation in Causal Inference: Methods\nfor Mediation and Interaction. New York, NY: Oxford University\nPress.\n\n\nWeissgerber, Tracey L., Natasa M. Milic, Stacey J. Winham, and Vesna D.\nGarovic. 2015. “Beyond Bar and Line Graphs: Time for a New Data\nPresentation Paradigm.” PLOS Biology 13 (4): 1–10. https://doi.org/10.1371/journal.pbio.1002128.\n\n\nWeissgerber, Tracey L., Stacey J. Winham, Ethan P. Heinzen, Jelena S.\nMilin-Lazovic, Oscar Garcia-Valencia, Zoran Bukumiric, Marko D. Savic,\nVesna D. Garovic, and Natasa M. Milic. 2019. “Reveal, Don’t\nConceal.” Circulation 140 (18): 1506–18. https://doi.org/10.1161/CIRCULATIONAHA.118.037777.\n\n\nYates, F. 1964. “Sir Ronald Fisher and\nthe Design of Experiments.” Biometrics 20 (2): 307–21.\nhttp://www.jstor.org/stable/2528399.\n\n\nZhao, Xinshu, Jr. Lynch John G., and Qimei Chen. 2010.\n“Reconsidering Baron and Kenny: Myths\nand Truths about Mediation Analysis.” Journal of Consumer\nResearch 37 (2): 197–206. https://doi.org/10.1086/651257.\n\n\nZwet, Erik W. van, and Eric A. Cator. 2021. “The Significance\nFilter, the Winner’s Curse and the Need to Shrink.”\nStatistica Neerlandica. https://doi.org/https://doi.org/10.1111/stan.12241.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>References</span>"
    ]
  }
]