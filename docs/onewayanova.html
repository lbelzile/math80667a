<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>3 Completely randomized designs with one factor | Experimental Design and Statistical Methods</title>
<meta name="author" content="Léo Belzile">
<meta name="description" content="In completely randomized experiments, we are solely interested in the effect of treatment variables. This chapter describes the most simple experimental setup one can consider, which consists in...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="3 Completely randomized designs with one factor | Experimental Design and Statistical Methods">
<meta property="og:type" content="book">
<meta property="og:description" content="In completely randomized experiments, we are solely interested in the effect of treatment variables. This chapter describes the most simple experimental setup one can consider, which consists in...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3 Completely randomized designs with one factor | Experimental Design and Statistical Methods">
<meta name="twitter:description" content="In completely randomized experiments, we are solely interested in the effect of treatment variables. This chapter describes the most simple experimental setup one can consider, which consists in...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Experimental Design and Statistical Methods</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Experimental Design and Statistical Methods</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">2</span> Hypothesis testing</a></li>
<li><a class="active" href="onewayanova.html"><span class="header-section-number">3</span> Completely randomized designs with one factor</a></li>
<li><a class="" href="reproducibility-crisis.html"><span class="header-section-number">4</span> Reproducibility crisis</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/lbelzile/math80667a">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="onewayanova" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Completely randomized designs with one factor<a class="anchor" aria-label="anchor" href="#onewayanova"><i class="fas fa-link"></i></a>
</h1>
<p>In completely randomized experiments, we are solely interested in the effect of treatment variables. This chapter describes the most simple experimental setup one can consider, which consists in comparing the average of a single outcome variable with <span class="math inline">\(K\)</span> different treatments levels, each defining a sub-population differing only in the treatment they received. The global hypothesis of interest is whether the mean of all of the sub-populations are equal.</p>
<p>The basic assumption of most designs is that we can decompose the outcome into two components <span class="citation">(<a href="references.html#ref-Cox:1958" role="doc-biblioref">Cox 1958</a>)</span>
<span class="math display" id="eq:additive">\[\begin{align}
\begin{pmatrix} \text{quantity depending only } \\ 
\text{on the particular unit} 
\end{pmatrix} + 
\begin{pmatrix} \text{quantity depending} \\
 \text{on the treatment used}\end{pmatrix}
 \tag{3.1}
\end{align}\]</span>
This <strong>additive</strong> decomposition further assumes that each unit is unaffected by (i.e., independent of) the treatment of the other units and that the average effect of the treatment is constant. This notably means that usually the difference between treatments can be estimated by the difference in sample means</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 3.1  (Additivity and transformations) </strong></span>Chapter 2 of <span class="citation"><a href="references.html#ref-Cox:1958" role="doc-biblioref">Cox</a> (<a href="references.html#ref-Cox:1958" role="doc-biblioref">1958</a>)</span> discusses the assumption of additivity and provides useful examples showing when it cannot be taken for granted. One of them, Example 2.3, is a scenario in which the experimental units are participants and they are asked to provide a ranking of different kindergarden students on their capacity to interact with others in games, ranked on a scale of 0 to 100. A random group of students receives additional orthopedagogical support, while the balance is in the business-as-usual setting (control group). Since there are intrinsic differences at the student level, one could consider a <strong>paired experiment</strong> and take as outcome the difference in sociability scores at the beginning and at the end of the school year.</p>
<p>One can expect the treatment to have more impact on people with low sociability skills who were struggling to make contacts: a student who scored 50 initially might see an improvement of 20 points with support relative to 10 in the business-as-usual scenario, whereas another who is well integrated and scored high initially may see an improvement of only 5 more had (s)he been assigned to the support group. This implies that the treatment effects are not constant over the scale, a violation of the additivity assumption. One way to deal with this is via transformations: <span class="citation"><a href="references.html#ref-Cox:1958" role="doc-biblioref">Cox</a> (<a href="references.html#ref-Cox:1958" role="doc-biblioref">1958</a>)</span> discusses the transformation <span class="math inline">\(\log\{(x+0.5)/(50-x)\}\)</span> to reduce the warping due to scale.</p>
</div>
<p>Another example is in experiments where the effect of treatment is multiplicative, so that the output is of the form
<span class="math display">\[\begin{align*}
\begin{pmatrix} \text{quantity depending only } \\ 
\text{on the particular unit} 
\end{pmatrix} \times
\begin{pmatrix} \text{quantity depending} \\
 \text{on the treatment used}\end{pmatrix}
\end{align*}\]</span>
Usually, this arises for positive responses and treatments, in which case taking natural logarithms on both sides, with <span class="math inline">\(\log(xy) = \log x + \log y\)</span> yields again an additive.</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 3.2  (Inadequacy of additivity based on context) </strong></span>This example is adapted from <span class="citation"><a href="references.html#ref-Cox:1958" role="doc-biblioref">Cox</a> (<a href="references.html#ref-Cox:1958" role="doc-biblioref">1958</a>)</span>, Example 2.2. Children suffering from attention deficit hyperactivity disorder (ADHD) may receive medication to increase their attention span, measured on a scale of 0 to 100, with 0 indicating normal attention span. An experiment can be designed to assess the impact of a standardized dose in a laboratory by comparing performances of students on a series of task before and after, when to a placebo. To make a case, suppose that students with ADHD fall into two categories: low symptoms and strong symptoms. In the low symptom group, the average attention is 8 per cent with the drug and 12 per cent with the placebo, whereas for people with strong symptoms, the average is 40 per cent among treated and 60 per cent with the placebo. If these two categories are equally represented in the experiment and the population, we would estimate an average reduction of 12 percent in the score (thus higher attention span among treated). Yet, this quantity is artificial, and a better measure would be that symptoms are for the treatment are 2/3 of those of the control (the ratio of proportions).</p>
</div>
<p>Equation <a href="onewayanova.html#eq:additive">(3.1)</a> also implies that the effect of the treatment is constant for all individuals. This often isn’t the case: in an experimental study on the impact of teaching delivery type (online, hybrid, in person), it may be that the response to the choice of delivery mode depends on the different preferences of learning types (auditory, visual, kinestetic, etc.) Thus, recording additional measurements that are susceptible to interact may be useful; likewise, treatment allotment must factor in this variability should we wish to make it detectable.</p>
<div id="hypothesis-tests-for-the-one-way-analysis-of-variance" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Hypothesis tests for the one-way analysis of variance<a class="anchor" aria-label="anchor" href="#hypothesis-tests-for-the-one-way-analysis-of-variance"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>one-way analysis of variance</strong> compares the sample averages of each treatment group <span class="math inline">\(T_1, \ldots, T_K\)</span> to try and determine if the population averages could be the same. Since we have <span class="math inline">\(K\)</span> groups, there will be <span class="math inline">\(K\)</span> averages (one per group) to estimate.</p>
<p>Let <span class="math inline">\(\mu_1, \ldots, \mu_K\)</span> denote the expectation (theoretical mean) of each of the <span class="math inline">\(K\)</span> sub-populations defined by the different treatments. Lack of difference between treatments is equivalent to equality of means, which translates into the hypotheses
<span class="math display">\[\begin{align*}
\mathscr{H}_0: &amp; \mu_1 = \cdots = \mu_K \\
\mathscr{H}_a: &amp; \text{at least two treatments have different averages, }
\end{align*}\]</span>
The null hypothesis is, as usual, a single numerical value, <span class="math inline">\(\mu\)</span>. The alternative consists of all potential scenarios for which not all expectations are equal. Going from <span class="math inline">\(K\)</span> averages to one requires imposing <span class="math inline">\(K-1\)</span> restrictions (the number of equality signs), as the value of the global mean <span class="math inline">\(\mu\)</span> is left unspecified.</p>
<div id="parametrizations" class="section level3" number="3.1.1">
<h3>
<span class="header-section-number">3.1.1</span> Parametrizations<a class="anchor" aria-label="anchor" href="#parametrizations"><i class="fas fa-link"></i></a>
</h3>
<p>The most natural parametrization is in terms of group averages: the (theoretical unknown) average for treatment <span class="math inline">\(T_j\)</span> is <span class="math inline">\(\mu_j\)</span>, so we obtain <span class="math inline">\(K\)</span> parameters <span class="math inline">\(\mu_1, \ldots, \mu_K\)</span> whose estimates are the sample averages <span class="math inline">\(\widehat{\mu}_1, \ldots, \widehat{\mu}_K\)</span>. One slight complication arising from the above is that the values of the population average are unknown, so this formulation is ill-suited for hypothesis testing because none of the <span class="math inline">\(\mu_i\)</span> values are known in practice and we need to make comparisons in terms of a known numerical value.</p>
<p>The most common parametrization is in terms of <strong>constrasts</strong> (or mean differences) relative to a reference group (say <span class="math inline">\(T_1\)</span>). The theoretical average of each group is written as <span class="math inline">\(\mu_1 + a_i\)</span> for treatment <span class="math inline">\(T_i\)</span>, where <span class="math inline">\(a_1=0\)</span> for <span class="math inline">\(T_1\)</span> and <span class="math inline">\(a_i = \mu_i-\mu_1\)</span> otherwise. The parameters are <span class="math inline">\(\mu_1, a_2, \ldots, a_K\)</span>.</p>
<p>An equivalent formulation writes for each treatment group the average of subpopulation <span class="math inline">\(j\)</span> as <span class="math inline">\(\mu_j = \mu + \delta_j\)</span>, where <span class="math inline">\(\delta_j\)</span> is the difference between the treatment average <span class="math inline">\(\mu_j\)</span> and the global average of all groups. Imposing the constraint <span class="math inline">\(\delta_1 + \cdots + \delta_K=0\)</span> ensures that the average of effects equals <span class="math inline">\(\mu\)</span>. Thus, if we know any <span class="math inline">\(K-1\)</span> of <span class="math inline">\(\{\delta_1, \ldots, \delta_K\}\)</span>, we automatically can deduce the last one is automatically known.</p>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 3.3  (Impact of encouragement on teaching) </strong></span>In <strong>R</strong>, the <code>lm</code> function fits a linear model based on a formula of the form <code>response ~ explanatory</code>. If the explanatory is categorical (i.e., a factor), the parameters of this model are the intercept, which is the sample average of the baseline group and the other parameters are simply contrasts, i.e., the <span class="math inline">\(a_i\)</span>’s.</p>
<p>In the sum-to-zero parametrization, obtained with <code>contrasts = list(... = contr.sum)</code>, where the ellipsis is replaced by the name of the categorical variable. In this parametrization, the intercept is the average of each treatment average, <span class="math inline">\((\widehat{\mu}_1 + \cdots + \widehat{\mu}_5)/5\)</span>; this need not coincide with the (overall) mean of the response <span class="math inline">\(\widehat{\mu} = \overline{y}\)</span> unless the sample is balanced, meaning that the number of observations in each group is the same. The other coefficients of the sum-to-zero parametrization are the differences between this intercept and the group means. Since the group means is zero, we can deduce that of the last group from the sum of the others.</p>
<p>We show the function call to fit a one-way ANOVA in the different parametrizations along with the sample average of each arithmetic group (the two controls who were taught separately and the groups that were praised, reproved and ignored in the third class). Note that the omitted category changes depending on the parametrization.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">mod_contrast</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">group</span>, 
                   data <span class="op">=</span> <span class="va">arithmetic</span><span class="op">)</span>
<span class="va">mod_sum2zero</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">score</span> <span class="op">~</span> <span class="va">group</span>, 
                   data <span class="op">=</span> <span class="va">arithmetic</span>,
                   contrasts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>group <span class="op">=</span> <span class="va">contr.sum</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:tableanovaparam">Table 3.1: </span>Coefficients of the analysis of variance model for the arithmetic scores using different parametrizations.
</caption>
<thead><tr>
<th style="text-align:left;">
group
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
contrasts
</th>
<th style="text-align:right;">
sum-to-zero
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
intercept
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
19.67
</td>
<td style="text-align:right;">
21.00
</td>
</tr>
<tr>
<td style="text-align:left;">
control 1
</td>
<td style="text-align:right;">
19.7
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
-1.33
</td>
</tr>
<tr>
<td style="text-align:left;">
control 2
</td>
<td style="text-align:right;">
18.3
</td>
<td style="text-align:right;">
-1.33
</td>
<td style="text-align:right;">
-2.67
</td>
</tr>
<tr>
<td style="text-align:left;">
praised
</td>
<td style="text-align:right;">
27.4
</td>
<td style="text-align:right;">
7.78
</td>
<td style="text-align:right;">
6.44
</td>
</tr>
<tr>
<td style="text-align:left;">
reproved
</td>
<td style="text-align:right;">
23.4
</td>
<td style="text-align:right;">
3.78
</td>
<td style="text-align:right;">
2.44
</td>
</tr>
<tr>
<td style="text-align:left;">
ignored
</td>
<td style="text-align:right;">
16.1
</td>
<td style="text-align:right;">
-3.56
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table></div>
</div>
<p>We can still assess the hypothesis by comparing the sample means in each group, which are noisy estimates of the expectation: their inherent variability will limit our ability to detect differences in mean if the signal-to-noise ratio is small.</p>
</div>
</div>
<div id="sum-of-square-decomposition" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Sum of square decomposition<a class="anchor" aria-label="anchor" href="#sum-of-square-decomposition"><i class="fas fa-link"></i></a>
</h2>
<p>The following section tries to shed some light into how the <span class="math inline">\(F\)</span>-test statistic works as a summary of evidence: it isn’t straightforward in the way it appears.</p>
<div id="mathematical-decomposition-of-sum-of-squares" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Mathematical decomposition of sum of squares<a class="anchor" aria-label="anchor" href="#mathematical-decomposition-of-sum-of-squares"><i class="fas fa-link"></i></a>
</h3>
<p>Proceed if you want to see the mathematical intuition behind
The usual notation for this decomposition is the following: suppose <span class="math inline">\(y_{ik}\)</span> represents the <span class="math inline">\(i\)</span>th person in the <span class="math inline">\(k\)</span>th treatment group (<span class="math inline">\(k=1, \ldots, K\)</span>) and the sample size <span class="math inline">\(n\)</span> can be split between groups as <span class="math inline">\(n_1, \ldots, n_K\)</span>; in the case of a balanced sample, <span class="math inline">\(n_1=\cdots=n_K = n/K\)</span>. We denote by <span class="math inline">\(\widehat{\mu}_k\)</span> the sample average in group <span class="math inline">\(k\)</span> and <span class="math inline">\(\widehat{\mu}\)</span> the overall average <span class="math inline">\((y_{11} + \cdots + y_{n_KK})/n = \sum_k \sum_i y_{ik}\)</span>, where <span class="math inline">\(\sum_i\)</span> denotes the sum over all individuals in the group.
Under the null model, all groups have the same mean, so the natural estimator is the sample average <span class="math inline">\(\widehat{\mu}\)</span> and likewise the group averages <span class="math inline">\(\widehat{\mu}_1, \ldots, \widehat{\mu}_K\)</span> are the correct estimators if each group has a (potentially) different mean. The more complex, which has more parameters, will always fit better because it has more possibility to accommodate differences observed in a group, even if these are spurious.
The sum of squares measures the (squared) distance between the observation and the fitted values, with the terminology total, within and between sum of squares linked to the decomposition
<span class="math display">\[\begin{align*}
\underset{\text{total sum of squares}}{{\sum_{i}\sum_{k} (y_{ik} - \widehat{\mu})^2}} &amp;= \underset{\text{within sum of squares}}{\sum_i \sum_k (y_{ik} - \widehat{\mu}_k)^2} +  \underset{\text{between sum of squares}}{\sum_k n_i (\widehat{\mu}_k - \widehat{\mu})^2}.
\end{align*}\]</span>
The term on the left is a measure of the variability for the null model (<span class="math inline">\(\mu_1 = \cdots = \mu_K\)</span>) under which all observations are predicted by the overall average <span class="math inline">\(\widehat{\mu}\)</span>. The within sum of squares measures likewise the distance between the two. We can measure how much worst we do with the alternative model (different average per group) relative to the null by calculating the between sum of square. This quantity in itself varies with the sample size (the more observations, the larger it is) so we must standardize as usual this quantity so that we have a suitable benchmark. In large samples, the <span class="math inline">\(F\)</span> statistic</p>
<p>The <span class="math inline">\(F\)</span>-statistic is
<span class="math display" id="eq:Fstatheuristic">\[\begin{align}
F = \frac{\text{between-group variability}}{\text{within-group variability}} = \frac{\text{between sum of squares}/(K-1)}{\text{within sum of squares}/(n-K)}
\tag{3.2}
\end{align}\]</span>
If there is no difference in mean, the <em>F</em>-statistic follows in large sample a <em>F</em>-distribution, whose shape is governed by two parameters named degrees of freedom which appear in eq.<a href="onewayanova.html#eq:Fstatheuristic">(3.2)</a> as scaling factors to ensure proper standardization. The first is the number of restrictions imposed by the null hypothesis (<span class="math inline">\(K-1\)</span>, the number of groups minus one for the one-way analysis of variance), and the second is the number of observations minus the number of <em>parameters estimates</em> for the mean (<span class="math inline">\(n-K\)</span>, where <span class="math inline">\(n\)</span> is the overall sample size and <span class="math inline">\(K\)</span> is the number of groups).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;There are only &lt;span class="math inline"&gt;\(K\)&lt;/span&gt; parameter estimates for the mean, since the overall mean is full determined by the other averages with &lt;span class="math inline"&gt;\(n\widehat{\mu} =n_1\widehat{\mu}_1 + \cdots + n_K \widehat{\mu}_K\)&lt;/span&gt;.&lt;/p&gt;'><sup>5</sup></a></p>
<p>Figure <a href="onewayanova.html#fig:squareddistanova">3.1</a> shows how the difference between these distances can encompass information that the null is wrong. The sum of squares is obtained by computing the squared length of these vectors and adding them up. The left panel shows strong signal-to-noise ratio, so that, on average, the black segments are much longer than the colored ones. This indicates that the model obtained by letting each group have its own mean is much better than the other. The picture in the right panel is not as clear: on average, the colored arrows are shorter, but the difference in length is much smaller relative to the colored arrows.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:squareddistanova"></span>
<img src="03-completely_randomized_trials_files/figure-html/squareddistanova-1.png" alt="Observations drawn from three groups from a model with a strong (left) and weak (right) signal-to-noise ratio, along with their sample mean (colored horizontal segments) and the overall average (horizontal line). Arrows indicate the magnitude of the difference between the observation and the (group/average) mean." width="85%"><p class="caption">
Figure 3.1: Observations drawn from three groups from a model with a strong (left) and weak (right) signal-to-noise ratio, along with their sample mean (colored horizontal segments) and the overall average (horizontal line). Arrows indicate the magnitude of the difference between the observation and the (group/average) mean.
</p>
</div>
<p>If there is no mean difference (null), the numerator is an estimator of the population variance, and so is the denominator of <a href="onewayanova.html#eq:Fstatheuristic">(3.2)</a>. If there are many observations (and relatively fewer groups), the ratio is approximately one on average.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:compareFnullalternative"></span>
<img src="03-completely_randomized_trials_files/figure-html/compareFnullalternative-1.png" alt="Null (left) and alternative (right) distributions for the one-way analysis of variance: if some of the group means are different, the curve gets shifted to the right. The shaded area is the type I error (null hypothesis) and the type II error (alternative hypothesis)." width="85%"><p class="caption">
Figure 3.2: Null (left) and alternative (right) distributions for the one-way analysis of variance: if some of the group means are different, the curve gets shifted to the right. The shaded area is the type I error (null hypothesis) and the type II error (alternative hypothesis).
</p>
</div>
</div>
</div>
<div id="graphical-representation" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Graphical representation<a class="anchor" aria-label="anchor" href="#graphical-representation"><i class="fas fa-link"></i></a>
</h2>
<p>If we have a lot of data, it sometimes help to focus only on selected summary statistics. A box-and-whiskers plot (or boxplot) represents five numbers</p>
<ul>
<li><p>The box gives the quartiles, say <span class="math inline">\(q_1\)</span>, <span class="math inline">\(q_2\)</span> (median) and <span class="math inline">\(q_3\)</span> of the distribution: 50% of the observations are smaller or larger than <span class="math inline">\(q_2\)</span>, 25% are smaller than <span class="math inline">\(q_1\)</span> and 75% are smaller than <span class="math inline">\(q_3\)</span> for the sample.</p></li>
<li><p>The length of the whiskers is up to <span class="math inline">\(1.5\)</span> times the interquartiles range <span class="math inline">\(q_3-q_1\)</span> (the whiskers extend until the latest point in the interval, so the largest observation that is smaller than <span class="math inline">\(q_3+1.5(q_3-q_1)\)</span>, etc.)</p></li>
<li><p>Observations beyond the whiskers are represented by dots or circles, sometimes termed outliers. However, beware of this terminology: the larger the sample size, the more values will fall outside the whiskers (about 0.7% for normal data). This is a drawback of boxplots, which were conceived at a time where big data didn’t exist.</p></li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:boxplot"></span>
<img src="figures/01-intro-boxplot.png" alt="Box-and-whiskers plot" width="85%"><p class="caption">
Figure 3.3: Box-and-whiskers plot
</p>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="hypothesis-testing.html"><span class="header-section-number">2</span> Hypothesis testing</a></div>
<div class="next"><a href="reproducibility-crisis.html"><span class="header-section-number">4</span> Reproducibility crisis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#onewayanova"><span class="header-section-number">3</span> Completely randomized designs with one factor</a></li>
<li>
<a class="nav-link" href="#hypothesis-tests-for-the-one-way-analysis-of-variance"><span class="header-section-number">3.1</span> Hypothesis tests for the one-way analysis of variance</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#parametrizations"><span class="header-section-number">3.1.1</span> Parametrizations</a></li></ul>
</li>
<li>
<a class="nav-link" href="#sum-of-square-decomposition"><span class="header-section-number">3.2</span> Sum of square decomposition</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#mathematical-decomposition-of-sum-of-squares"><span class="header-section-number">3.2.1</span> Mathematical decomposition of sum of squares</a></li></ul>
</li>
<li><a class="nav-link" href="#graphical-representation"><span class="header-section-number">3.3</span> Graphical representation</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/lbelzile/math80667a/blob/master/03-completely_randomized_trials.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/lbelzile/math80667a/edit/master/03-completely_randomized_trials.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Experimental Design and Statistical Methods</strong>" was written by Léo Belzile. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
